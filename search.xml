<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Introduction to Video Action Understanding</title>
      <link href="2020/10/19/Introduction-to-Video-Action-Understanding/"/>
      <url>2020/10/19/Introduction-to-Video-Action-Understanding/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文主要是对 <a href="https://arxiv.org/pdf/2010.06647.pdf">Video Action Understanding: A Tutorial</a> 一文进行翻译和整理, 这是一篇很好的综述文章, MIT 大佬写的, 推荐去看.</p><p>首先说明一下几个概念, 在文献中 <code>instance</code> 通常是指一个完整的视频, <code>segment</code> 是指从一个视频中裁剪出的一个片段.</p><p>作者把视频行为理解分为了下图中的几个部分:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/action_understanding.png"></p><h1 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h1><h2 id="Action-Recognition"><a href="#Action-Recognition" class="headerlink" title="Action Recognition"></a>Action Recognition</h2><p>定义为对一个完整的视频进行分类的过程, 如果动作在整个视频中都存在, 那么被称为 trimmed action recognition, 否则被称为 untrimmed action recognition, 通常而言 untrimmed action recognition 会更难, 因为此时模型需要在完成分类任务的同时忽略非动作的背景片段.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/action_recognition.png"></p><h2 id="Action-Prediction"><a href="#Action-Prediction" class="headerlink" title="Action Prediction"></a>Action Prediction</h2><p>定义为对一个不完整的视频进行分类的过程, 即只知道视频的一部分, 用这一部分信息来对整个视频进行分类.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/action_prediction.png"></p><h2 id="Temporal-Action-Proposal"><a href="#Temporal-Action-Proposal" class="headerlink" title="Temporal Action Proposal"></a>Temporal Action Proposal</h2><p>通过求得每个行为实例的开始时间和结束时间来把输入的视频分为若干个动作和非动作的片段(片段是一系列连续的帧).不对每个行为实例进行具体分类.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/temporal_action_proposal.png"></p><h2 id="Temporal-Action-Localization-Detection"><a href="#Temporal-Action-Localization-Detection" class="headerlink" title="Temporal Action Localization/Detection"></a>Temporal Action Localization/Detection</h2><p>在 Temporal Action Proposal 的基础上对每个 Proposal(行为实例) 进行分类.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/temporal_action_detection.png"></p><h2 id="Spatiotemporal-Action-Proposal"><a href="#Spatiotemporal-Action-Proposal" class="headerlink" title="Spatiotemporal Action Proposal"></a>Spatiotemporal Action Proposal</h2><p>除了给出每个动作在时间上的开始和结束时间外(找到开始帧和结束帧), 还需要在空间上给出 Bounding Box 来框出每个正在进行某个动作的物体. 如果这里使用了某种连接策略来对每个帧的 Bounding Box 进行关联, 那么这个动作区域(时间和空间维度)被称为 tubes 或者 tubelets.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/spatiotemporal_action_proposal.png"></p><h2 id="Spatiotemporal-Action-Localization-Detection"><a href="#Spatiotemporal-Action-Localization-Detection" class="headerlink" title="Spatiotemporal Action Localization/Detection"></a>Spatiotemporal Action Localization/Detection</h2><p>在 Spatiotemporal Action Proposal 基础上对每个 Proposal(行为实例) 进行分类.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/spatiotemporal_action_detection.png"></p>]]></content>
      
      
      <categories>
          
          <category> Video </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Video </tag>
            
            <tag> Action </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deformable Convolutional Networks</title>
      <link href="2020/10/12/Deformable-CNNs/"/>
      <url>2020/10/12/Deformable-CNNs/</url>
      
        <content type="html"><![CDATA[<h1 id="Deformable-CNN-v1"><a href="#Deformable-CNN-v1" class="headerlink" title="Deformable CNN v1"></a><a href="https://openaccess.thecvf.com/content_iccv_2017/html/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper.html">Deformable CNN v1</a></h1><p><a href="https://jifengdai.org/">Jifeng Dai</a> 大佬在 MSRA 的工作, 在检测和分割领域应用很多, 思想也是非常值得学习.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>对于视觉任务, 我们需要保持特征对于图片的旋转, 平移以及尺度的不变性, 举例就是对于图像分类任务, 对于同样的目标在不同图像中的偏移，旋转，尺度，要输出同样的结果。一些手工设计的特征(SIFT, SURF等)有一定的能力, 但显然无法利用 CNN 来实现 end-to-end 的设计.</p><p>CNN 通过池化层来实现平移不变性, 网络越深其效果越好, 但这种方式仍然受到卷积核以及感受野大小的限制, 但对于尺度不变性和旋转不变性几乎是没有的, 对于目标检测, 语义分割等任务可以使用特征金字塔(用来近似代替图像金字塔)来得到一定的尺度不变性, 旋转不变性就只能通过数据增强来实现.</p><p>一句话来说就是 <strong>网络模型对于物体几何形变的适应能力几乎完全来自于数据本身所具有的多样性。</strong></p><p>这篇文章想解决的问题就是如何对 CNN 进行变化, 使其具有对物体的几何形变有适应能力. 要解决问题首先需要明白为什么会有这个问题, 卷积本身就是一种非常固定的几何结构:</p><ol><li>卷积单元在固定位置对输入特征图进行采样</li><li>池化层以固定比例降低空间分辨率</li><li>RoI Pooling 将 RoI 分成 size 固定的特征</li></ol><p>可以看到不管是在卷积, 池化还是其他操作上, CNN 采取的都是固定的操作, 这种固定的操作是不具备处理几何形变的能力的. 所以为了解决这个问题, 提出了 Deformable CNN.</p><h2 id="Deformable-CNN"><a href="#Deformable-CNN" class="headerlink" title="Deformable CNN"></a>Deformable CNN</h2><p>先以一个 $3 \times 3$ 卷积为例来说明可变形卷积和普通卷积的区别. 下图中的 (a) 就是一个标准的卷积, (b)是可变形卷积, (c) 和 (d) 是 (b) 的一种特点形式.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Deformable-CNNs/deformable_cnn_3x3.png"></p><p>再来看下可变形卷积有什么好处, 先看下面左边的图, 标准的卷积，感受野必然是一个方方正正的区域。顶图有一个中心像素，它的感受野是 $3 \times 3$，到了中间的图，周围四个角点又可以进一步扩展感受野，直到底部的图。</p><p>所以对于顶部目标的中心像素，经历了两次 $3 \times 3$ 卷积，它的感受野是固定的 $5 \times 5$，与动物本身的形状并不匹配。而同样的两个 $3 \times 3$ 的卷积，右边的可变形卷积，则由于灵活的感受野，所覆盖的区域更大，也更匹配了目标本身的形状。这是一个非常通用的问题，<strong>标准卷积对目标的形状感受野不够灵活，卷积的效率自然也就下降。而可变形卷积则利用了不规则可变化的形状，改善了这两个问题。</strong></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Deformable-CNNs/deformable_cnn_vs_standard_cnn.png"></p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>实现起来也是很简单的, 只需要增加一个 offsets 即可, 通道数 2N 表示需要学习 x 和 y 方向上的 offset.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Deformable-CNNs/deformable_cnn_implement.png"></p><p>对于目标检测任务, 作者还提出了 Deformable RoI Pooling, 思路也是一样的.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Deformable-CNNs/deformable_roi_pooling.png"></p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>可以看到使用可变形卷积所得到的采样位置能更好地匹配物体的形状.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Deformable-CNNs/deformable_cnn_sampling_locations.png"></p><p>VOC 数据集上的结果, 可以看到 deformable layer 越多提升越明显</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Deformable-CNNs/deformable_cnn_result_voc.png"></p><p>COCO 数据集上的结果</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Deformable-CNNs/deformable_cnn_result_coco.png"></p>]]></content>
      
      
      <categories>
          
          <category> Backbone </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Action Recognition</title>
      <link href="2020/10/02/Action-Recognition/"/>
      <url>2020/10/02/Action-Recognition/</url>
      
        <content type="html"><![CDATA[<p>基本知识大家去看 <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a> 开的 <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/">EECS 498-007 / 598-005: Deep Learning for Computer Vision</a> 这门公开课就好啦</p><p>这里就直接上近 2 年的顶会论文了</p><h1 id="ECCV’20"><a href="#ECCV’20" class="headerlink" title="ECCV’20"></a>ECCV’20</h1><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>AR-Net: Adaptive Frame Resolution for Eﬃcient Action Recognition</span></div>    <div class="hide-content"><p>想法是为输入中的每个帧即时选择最佳的分辨率, 以此来有效识别长的未裁剪的视频. 具体的, 对于给定给定视频帧, 用一个 policy network 替分类网络决定使用多大的分辨率, 这个 policy network 是和分类网络一起联合使用反向传播来训练的.</p><p>Pipeline 如下:</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/ARNet_structure.png"></p><p>估计作者和我一样, 缺卡, 所以没跑整个 Kinetics 数据集.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/arnet_result_activity_and_fcvid.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/arnet_result_map_and_flops.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>DDGCN: A Dynamic Directed Graph Convolutional Network for Action Recognition</span></div>    <div class="hide-content"><p>利用图卷积和骨架信息来进行动作识别, 涉及到骨架的方法等用到骨架的时候再看吧, 先跳过.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition</span></div>    <div class="hide-content"><p>同上, 也是利用图卷积和骨架信息来进行动作识别, 涉及到骨架的方法等用到骨架的时候再看吧, 先跳过.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Directional Temporal Modeling for Action Recognition</span></div>    <div class="hide-content"><p>许多现在的行为识别模型使用 3D 卷积, 但 3D 卷积无法对 clip-level 的有序时空信息进行建模.</p><p>作者提出了一种 channel independent directional convolution (CIDC) 操作, 这种卷积操作能学到局部特征之间的时序信息, 通过使用 CIDC 来对 cross multiple spatial scales 的 clip-level 时序信息进行建模.</p><p>作者还对 CIDC 网络的 activation map 进行了可视化, 表明这种操作能够使模型关注于更有意义, 和动作相关的帧.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/CIDC_unit.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/CIDC_pipeline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/CIDC_result_ucf_and_hmdb.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/CIDC_result_kinetics_and_sth_sth.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Empowering Relational Network by Self-Attention Augmented Conditional Random Fields for Group Activity Recognition</span></div>    <div class="hide-content"><p>group activity recognition 是指识别一群人正在干啥. </p><p>本文的核心是通过新设计的时间和空间 self attention 来学习视频中每个 actor 的时间演变和空间关系背景，从而增加条件随机场（CRF），以学习相关观测值的相互依赖性。</p><p>这种组合方式利用 self attention 的全局感受野来构造一个时空拓扑图, 以解决 actor 的时间依赖性和非局部的关系.</p><p>首先使用 temporal self-attention 和 spatial self-attention 来对 CRF 的 pairwise energy 进行建模, 接着为了适应每个视频的独特性, 还提出了一种具有动态停止功能的 mean-field inference 算法, 最后结合了前向和后向时间上下文信息的通用 transformer encoder (UTE) 用于聚合关系上下文和场景信息, 以进行 group action recognition.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/self_attention_augmented_crf_pipeline.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Few-shot Action Recognition with Permutation-invariant Attention</span></div>    <div class="hide-content"><p>Philip H. S. Torr 组的文章, 主要是对视频分类做少样本学习.</p><p>本文用一个 C3D 的 encoder 来对时空视频块捕获 short-range 的 action parttern, 然后把这些 encoder blocks 利用 permutation-invariant pooling 给聚合到一起, 这种方法能使模型对不同的  action lengths 和 long-range 时序信息更加鲁棒.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/arn_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Human Interaction Learning on 3D Skeleton Point Clouds for Video Violence Recognition</span></div>    <div class="hide-content"><p>3D 的骨骼信息, 老规矩, 跳过.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Multi-view Action Recognition using Cross-view Video Prediction</span></div>    <div class="hide-content"><p>挖坑??? 提出了在多视角环境下的动作识别问题…问题的定义是把不同视角和不同时间的几个 short clips 作为输入, 以此来学习一个整体的 internal representation, 这个 representation 用于从未知的视角和时间预测 video clip</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>RAFT: Recurrent All-Pairs Field Transforms for Optical Flow</span></div>    <div class="hide-content"><p>Jia Deng 大佬组的工作, 用深度学习来生成光流.</p><p>核心想法是提取每一个像素点的特征, 为所有的像素对建立多尺度的 4D correlation volumes, 并通过一个 recurrent unit 来迭代地更新一个 flow field</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/raft_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/raft_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition</span></div>    <div class="hide-content"><p>Fei-Fei Li 组的工作…</p><p>由于需要对空间上下文和时序上下文信息进行建模, 标准的方法依赖于 2D 或者 3D 的卷积来做这件事, 这导致了非常大的参数量.</p><p>最近有些高效的方法使用一个 channel-wise, shift-based 的 primitive 作为时序卷积的一种替代来降低参数量, 但这种方法仍然受限于一个 fixed-shift 方案以及在空间上的卷积操作.</p><p>为了解决这个问题作者提出了一种 <strong>可学习的 3D 时空 shift 操作</strong>, 作者分析了新的 primitive 对视频动作识别的适用性, 并探索了几种变体, 在保持高效设计的同时实现了更强的 representational flexibility.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/prior_work_for_efficient_sihft_based_video_recognition.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/rubiksshift_layer.png"></p><p>看下结果吧</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/rubiksnet_result_sth_sth_v2.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/rubiksnet_result_sth_sth_v1.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/rubiksnet_result_ucf_and_hmdb.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Social Adaptive Module for Weakly-supervised Group Activity Recognition</span></div>    <div class="hide-content"><p>针对 group activity recognition 的弱监督学习…又挖坑, 这次连训练集中都没有重要的 persons…然后自己做了数据集, 吐血…</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Temporal Distinct Representation Learning for Action Recognition</span></div>    <div class="hide-content"><p>不同帧共享相同 2D CNN 的 kernels 会导致重复和冗余的信息, 尤其在空间语义提取过程中, 所以会忽略帧和帧之间的关键变化.</p><p>作者从 2 个方面解决这个问题:</p><ol><li>设计一种顺序通道过滤机制，即渐进增强模块（PEM），以逐步激发不同帧中特征的 discriminative channels，从而避免重复提取信息</li><li>提出了一个 temporal diversity loss 来强迫 kernels 关注并捕获帧和帧之间的变化, 而不是捕捉具有相似外观的图像区域.</li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TDR_pipeline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TDR_result_sth_sth_v1_and_v2.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TDR_result_kinetics.png"></p></div></div><h1 id="CVPR’20"><a href="#CVPR’20" class="headerlink" title="CVPR’20"></a>CVPR’20</h1><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>3DV: 3D Dynamic Voxel for Action Recognition in Depth Video</span></div>    <div class="hide-content"><p>解决 3D 数据的动作识别问题…这里的数据是 3D 的动态 voxel…</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>ActBERT: Learning Global-Local Video-Text Representations</span></div>    <div class="hide-content"><p>Yi Yang 组的工作</p><p>提出用 ActBERT 来从未标记的数据中学习联合的 video-text representation 以进行自监督学习.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Action Modifiers: Learning from Adverbs in Instructional Videos</span></div>    <div class="hide-content"><p>提出了用来自 accompanying narrations 的弱监督方法从教学视频中学习副词的表示形式.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Active Vision for Early Recognition of Human Actions</span></div>    <div class="hide-content"><p>提出了一种 action 的 early recognition 方法, 充分利用了多台摄影机的优势, 本文提出的方法考虑了多个摄像头, 并在每个时间步上决定使用哪个摄像头是最可靠的, 以此来实现尽快的分类.</p><p>作者把相机选择问题看成一个连续的决策过程, 然后用强化学习来做.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Actor-Transformers for Group Activity Recognition</span></div>    <div class="hide-content"><p>提出一种 actor-transformer 模型来学习和选择性地为 group activity recognition 提取相关信息.</p><p>分别利用 2D CNN 和 3D CNN 来给 transformer 提供 actor-specific 的静态和动态的表示, 作者用了 2 种方式来对这些表示进行融合, 并展示了融合之后的效果.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/transformer_static_and_dynamic_representation.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/transformer_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/transformer_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Can Deep Learning Recognize Subtle Human Activities?</span></div>    <div class="hide-content"><p>又是挖坑, 搞了个细微动作的数据集, 提了个新任务, 用于发现人类动作的细微变化…</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Context Aware Graph Convolution for Skeleton-Based Action Recognition</span></div>    <div class="hide-content"><p>骨架信息, 先过</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition</span></div>    <div class="hide-content"><p>骨架信息, 过</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>End-to-End Learning of Visual Representations From Uncurated Instructional Videos</span></div>    <div class="hide-content"><p>提出了一种新的学习方法, 这种方法能够解决 narrated videos 中的 misalignments 问题, 解决这个问题后就能从头学习到更加强大的视频表示, 而无需任何的手工标注.</p><p>这种方法学到的表示超过了目前其他所有自监督学习.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/MIL_NCE_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Evolving Losses for Unsupervised Video Representation Learning</span></div>    <div class="hide-content"><p>作者把无监督的表示学习看成一种多模态, 多任务的学习问题, 其中利用蒸馏将表示在多个模态之间共享.</p><p>然后利用遗传算法来自动找到损失函数的最佳组合方式, 以此阐明了 loss function evolution 的概念, 基于 Zipf 定律提出了一种无监督表示进化的评估策略.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/Evolving_loss_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Few-Shot Video Classification via Temporal Alignment</span></div>    <div class="hide-content"><p>少样本学习.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Gate-Shift Networks for Video Action Recognition</span></div>    <div class="hide-content"><p>3D CNN 由于有大量的参数和计算, 如果缺乏足够大的数据集来训练的话, 效果会很不好.</p><p>作者想法是在 3D kernel 的时空分解中引入空间门控, 作者用 Gate-Shift  Module 来实现这种操作, GSM 能把 2D CNN 转为高效的时空特征提取器, 利用 GSM 来让 2D CNN 自适应地 route features 并将这些特征组合起来, 而几乎没有其他参数或者计算量.</p><p>光看摘要我怎么感觉就是 3D 的 attention 呢… 看下 GSM 的结构吧</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/GSM_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/GSM_compare_to_others.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/GSM_implementation.png"></p><p>看下最终的结果吧</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/GSM_result_sth_v1.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Listen to Look: Action Recognition by Previewing Audio</span></div>    <div class="hide-content"><p>用 audio 来作为 preview mechanism 来消除 long-term 和 short-term 的冗余.</p><p>提出一种 ImgAud2Vid 的框架, 通过从更轻的模式(单个帧和伴随的音频)中蒸馏信息来 hallucinates clip-level 的特征, 以此来减少 short-term 的时序冗余, 在 ImgAud2Vid 的基础上提出了 IMGAUD-SKIMMING 机制, 用 LSTM 来迭代地选择未裁剪网络中有用的 moments, 从而减少 long-term 的时序冗余.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/audio_result_activity.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/audio_result_ucf.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Multi-Modal Domain Adaptation for Fine-Grained Action Recognition</span></div>    <div class="hide-content"><p>无监督的 domain adaptation 在源域和目标域中使用对抗训练, 但这样无法学习每个域中视频的多模态信息, 除了 adversarial alignment 以外, 本文还将不同模态之间的对应关系用作一种自监督的 alignment 方法, 并将其用于无监督的 domain adaptation.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/multi_model_da_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/multi_modek_da_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>PREDICT & CLUSTER: Unsupervised Skeleton Based Action Recognition</span></div>    <div class="hide-content"><p>研究无监督的, 基于骨骼信息的动作识别, 所有关于骨骼的都先跳过</p></div></div><p>感觉还是有点细, 下面就只简单说下怎么做的了</p><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Progressive Relation Learning for Group Activity Recognition</span></div>    <div class="hide-content"><p>对于 group activity recognition 这个 task 而言, 通常有很多的 actor, 但只有几个关键帧的 actors 从本质上定义了这种 action, 所以有效地对 group-relevant 进行建模以及抑制不相关的 action 是 group activity recognition 的关键.</p><p>本文利用强化学习来 refine 这些 low-level 的特征以及 group activity 的 high-level 相关性.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Regularization on Spatio-Temporally Smoothed Feature for Action Recognition</span></div>    <div class="hide-content"><p>提出用一个正则化方法来防止 3D CNN 的过拟合问题, 这个正则化方法是通过随机更改特征低频分量的大小来实现的.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/RMS_results_kinetics.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/RMS_results_on_2d_cnn.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Rethinking Zero-Shot Video Classification: End-to-End Training for Realistic Applications</span></div>    <div class="hide-content"><p>视频中的 zero-shot learning 是只训练一个模型一次, 然后去寻找训练集中没有的视频类别.</p><p>作者的做法和很多传统做法不一样, 传统方法在训练时让 test task 未知, 而作者则鼓励在训练时和 test data 做 domain shift, 并禁止针对特定的测试数据集定制 zero-shot learning 模型.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/zero_shot_learning_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition</span></div>    <div class="hide-content"><p>骨架的==</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Sideways: Depth-Parallel Training of Video Models</span></div>    <div class="hide-content"><p>针对视频模型的并行计算…使用近似反向传播, 在标准的反向传播中, 模型每个计算步上的梯度和激活在时间上都是同步的</p><p>每次前向传播的 activation 都需要存储, 直到进行下一次反向传播, 以防止层间(深度)的并行化. 作者使用 smooth, 冗余的输入流来开发更加有效的培训方案.</p><p>只要有新的 activation(来自新的帧), 作者就 overwrite 网络的 activation, 来自 2 个通道的信息如果这样渐进地不断积累会破坏 gradient 和 activation 之间的对应关系, 从而导致理论上更加嘈杂的权重更新. 但作者实验发现，与标准同步反向传播相比，这种训练方式不但仍然会收敛, 而且还有更好的泛化性能.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/sideway_results.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/sideway_results_plot.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Skeleton-Based Action Recognition With Shift Graph Convolutional Network</span></div>    <div class="hide-content"><p>怎么又是骨骼信息…跳过</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>SmallBigNet: Integrating Core and Contextual Views for Video Classification</span></div>    <div class="hide-content"><p>Yu Qiao 大佬组的, 题目借鉴了 SlowFast 那篇的名字.</p><p>现在的方法都只从一个 view 去提取上下文信息, 作者认为这样是不对的, 所以需要 small view 和 big view 的结合.(感觉类似多尺度)</p><p>对于每个现在的时间步, small view branch 用来学习核心的语义信息, big view branch 利用更大的感受野来为 small view branch 提供更好的视频特征, 通过不断聚合 big view 的上下文信息, small view 可以学到更加 robust and discriminative 的特征</p><p>为了消除过拟合, 作者让 small view branch 和 big view branch 的卷积核共享参数.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/smallbig_view_motivation.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/small_big_block.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/small_big_result_kinetics.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/small_big_result_sth_sth.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Spatiotemporal Fusion in 3D CNNs: A Probabilistic View</span></div>    <div class="hide-content"><p>以前的时空融合方法都是专门设计的, 但这些方法只能在有限的融合策略上对 model-level 进行分析</p><p>本文将时空融合策略转换到概率空间来做, 这样就能对各种融合方式进行 network-level 进行评估, 而无需分别对它们进行训练</p><p>此外, 还能得到更加细粒度的信息, 并提出了一种新的融合策略, 达到了 SOTA 的效果.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/spatioTemporal_fusion_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Speech2Action: Cross-Modal Supervision for Action Recognition</span></div>    <div class="hide-content"><p>利用对话来做动作识别, 感觉还是蛮有意思, 牛津的 VGG 组做的. 不详细介绍.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/speech2actoin.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>TEA: Temporal Excitation and Aggregation for Action Recognition</span></div>    <div class="hide-content"><p>王利民老师组的工作</p><p>分别用 motion excitation (ME) module 和 multiple temporal aggregation (MTA) module 来捕获 short-range 和 long-range 的 temporal evolution.</p><p>对于 short-range 的 motion 建模, ME 模块根据时空特征来计算 feature-level 的时序差异, 然后利用这个差异来激活 motion-sensitive 的 channels.</p><p>之前文章中的 long-range 时序聚合是通过堆叠大量 local temporal convolutions 来实现, 每个 local temporal convolution 一次处理一个 local temporal window.</p><p>MTA 将 local convolution 变为一组子卷积, 从而形成分层的残差结构, 在不引入其他参数的情况下, 使用一组子卷积来提取特征, 并且每个帧都可以和相邻的帧完成时间聚合, 这样最终扩大了时间维度上的感受野, 使其能够对 distant frames 的关系进行建模.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/me_and_mta.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TEA_pipeline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TEA_result_kinetics.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TEA_result_sth_sth_v1.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Temporal Pyramid Network for Action Recognition</span></div>    <div class="hide-content"><p>周博磊和商汤那边的工作</p><p>visual tempo(视觉节奏??) 表征了动作的动态和时间尺度, 所以对 visual tempo 进行建模有助于对这些动作的识别</p><p>之前的工作通常利用对原始视频在多个比率下采样和建造一个 input-level 的 frame pyramid 来捕获 visual tempo 特征, 这种方法需要计算巨大的多分支网络来处理. 本文在 feature-level 上提出了一种通用的时间金字塔网络, 可以以即插即用的方式灵活地集成到 2D 或者 3D Backbone 中.</p><p>TPN 的 2 个基本组成部分是要素的来源和融合, 它们形成了一种特征多级结构, 以学习不同 tempo 的 action</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TPN_pipeline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TPN_result_kinetics.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TPN_result_sth_sth.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Unsupervised Learning From Video With Deep Neural Embeddings</span></div>    <div class="hide-content"><p>Stanford 和 MIT 的联合作品…</p><p>提出了一个框架用于在视频输入上训练 deep nonlinear 的 embedding, 通过学习将相似视频识别和分组在一起的 embedding dimension，同时在embedding space中将本质上不同的视频推开，VIE可以捕获视频固有的强大统计结构，而无需外部注释标签。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/VIE_result_ucf_and_hmdb.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/VIE_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Video Modeling With Correlation Networks</span></div>    <div class="hide-content"><p>这篇文章基于可学习的相关操作提出了对 3D 卷积/双流网络 的一种替代方法, 这种相关操作能在网络的不同层的特征图上建立帧到帧的匹配, 这种结构能融合时序匹配信息和传统的 2D 外观信息, 相比于 3D CNN 和双流网络都有一定的竞争力, 且能更快地训练.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/correlation_operator.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/correlationnet_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>X3D: Expanding Architectures for Efficient Video Recognition</span></div>    <div class="hide-content"><p>通过从空间, 时间, 宽度和深度维度来逐渐拓展一个 2D CNN 来实现精度和效率的 tradeoff, 可以看成是 3D 版本的 EfficientNet, 比简单地把 EfficientNet 的 2D 换成 3D 效果好了太多</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/x3d_expand_process.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/x3d_result_kinetics.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/x3d_result_charades_and_compare_to_efficient3d.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/x3d_result_ava.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/x3d_result_acc_and_flop.png"></p></div></div><h1 id="ICCV’19"><a href="#ICCV’19" class="headerlink" title="ICCV’19"></a>ICCV’19</h1><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Bayesian Hierarchical Dynamic Model for Human Action Recognition</span></div>    <div class="hide-content"><p>通过使用贝叶斯框架, 模型参数能够在不同数据序列之间变化, 这增强了模型适应类内变化(空间和时间上)的能力.</p><p>同时生成学习过程允许每个动作类别保留独特的 dynamic pattern, 通过贝叶斯推断, 我们能量化分类中的不确定性, 从而帮助进行决策.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/HDM_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition</span></div>    <div class="hide-content"><p>聚焦于用多模态来进行动作识别, 提出了一种结构来进行多模态的时序绑定, 即在一个时序 offset 范围内对模态进行绑定??? ( the combination of modalities within a range of temporal offsets)</p><p>同时使用 RGB, 光流和 audio 来训练然后融合, 这里不同模态的数据在时序聚合之前就进行了融合, 随着时间来不断共享模态和融合权重.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/epic_fusion_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Generative Multi-View Human Action Recognition</span></div>    <div class="hide-content"><p>multi-view 动作识别旨在整合来自不同 view 的信息, 以此来提高分类信息. 感觉是挖坑, 过</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Grouped Spatial-Temporal Aggregation for Efficient Action Recognition</span></div>    <div class="hide-content"><p>以前方法通过把 3D CNN 分解为空间 CNN 和时间 CNN 来降低复杂度, 本文提出把 feature channel 并行地分组为 spatial 和 temporal 这两种, 这种解耦方式能让这 2 个组专注于静态和动态信息.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/3_types_of_networks.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/gst_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/GST_result_sth_sth.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Hallucinating IDT Descriptors and I3D Optical Flow Features for Action Recognition With CNNs</span></div>    <div class="hide-content"><p>尽管使用了 RGB 和光流, 但 I3D 模型(以及其他一些模型)将其输出和 Improved Dense Trajectory (DIT) 结合在一起并通过 Bag-of-Words 和 Fisher Vector 提取 low-level 视频描述子的一个编码. 这种 CNN 和手工特征的融合是非常耗时间的.</p><p>所以作者提出了一种端到端的可训练的网络, 用网络中的每个 stream 来学习 IDT-based 的 BoW/FV 表示</p><p>每个 stream 在最后一个 1D 卷积之前获取 I3D 的特征图, 然后将这些特征图转化为 BoW/FV 的表征.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/hallucinate_IDT_pipeline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/ADL_I3D_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition</span></div>    <div class="hide-content"><p>用强化学习来更好地对视频中的 frames 进行采样.</p><p>把 frame 的采样看成是多个并行的马尔科夫决策过程, 通过不断对初始化采样进行调整来挑选 frame/clip, 然后用 multiple-agent 的 RL 来解决这个问题.</p><p>本文的 multiple-agent RL 由一个 RNN-based 的 context-aware observation network, 一个策略网络和一个分类网络组成.</p><p>观察网络能联合地对 agents 周围的上下文信息以及特定 agent 的历史状态进行建模, 策略网络能在时间步都能在预定义的 action space 中生成概率分布, 分类网络用于 reward 计算及输出最终结果.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/RL_sampling_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/RL_sampling_pipeline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/RL_sampling_results_activity.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/RL_sampling_map_vs_num_of_frames.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/RL_sampling_result_activity_1.3.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>SCSampler: Sampling Salient Clips From Video for Efficient Action Recognition</span></div>    <div class="hide-content"><p>也是针对如何进行有效地采样来做的.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/scs_sampler_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/scsample_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>SlowFast Networks for Video Recognition</span></div>    <div class="hide-content"><p>Kaiming He 的作品…一如既往地简洁有效. 有 2 个 pathway, 一个 low frame rate 的 slow pathway 用于捕获空间语义, 还有一个 high frame rate 的 fast pathway 以精细的 temporal resolution 来捕获运动信息.</p><p>其中 Fast pathway 可以通过减少其通道容量来达到更加轻量的目的.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/slow_fast_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/slow_fast_result_kinetics.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/slow_fast_result_kinetics600.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/slow_fast_ablation.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>STM: SpatioTemporal and Motion Encoding for Action Recognition</span></div>    <div class="hide-content"><p>有的论文用 3D CNN 来提取时空信息, 再加上用光流来提取动作信息, 这篇文章是想用 2D CNN 来 encode 这 2 种特征.</p><p>首先提出了一种 STM block, 用 Channel-wise SpatioTemporal Module (CSTM) 和 Channel-wise Motion Module (CMM) 来分别有效地提取时空特征和动作特征.</p><p>然后把原始的残差块用 STM block 来替代.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/STM_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/STM_pipeline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/stm_result_sth_sth.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/stm_result_kinetics.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>TSM: Temporal Shift Module for Efficient Video Understanding</span></div>    <div class="hide-content"><p>韩松大佬组的工作</p><p>提出一种 Temporal Shift Module 以 2D CNN 的计算复杂度来实现 3D CNN 的性能, TSM 在时序维度上 shift 部分 channel, 以便在相邻帧之间交换信息, TSM 能很方便地插入 2D CNN 中而无需增加额外参数. 作者也把 TSM 用于 online 环境下, 实现了实时且低能耗的 online video recognition 以及 video object detection.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TSM_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/resudial_shift.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/uni-directional_TSM.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TSM_result_sth.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TSM_result_acc_and_flops.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/TSM_result_acc_and_latency.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Video Classification With Channel-Separated Convolutional Networks</span></div>    <div class="hide-content"><p>3D 分组卷积</p><p>作者通过实验发现以下 3 点:</p><ol><li>通道交互的数量对 3D CNN 的准确性起重要作用</li><li>通过对通道交互和时空交互进行分解来分解 3D CNN 是一个好方法, 可以在降低计算成本的同时提高准确性</li><li>通道分离的 3D CNN 其实是一种正则化形式, 这使得训练精度较低但测试精度较高.</li></ol><p>根据上面的发现作者设计了一种网络结构, 并且表现 SOTA.</p><p>先回顾下什么是分组卷积:</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/2d_group_conv.png"></p><p><strong>然后是 channel-separated convolution blocks</strong></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/channel_separated_conv.png"></p><p><strong>最后看下 3D CNN 的残差结构</strong></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/resnet_simple_block.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/resnet_bottlenect.png"></p><p><strong>看下最后的结果</strong></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/csn_result_kinetics.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/csm_tradeoff_glops_acc.png"></p></div></div><h1 id="CVPR’19"><a href="#CVPR’19" class="headerlink" title="CVPR’19"></a>CVPR’19</h1><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Action Recognition From Single Timestamp Supervision in Untrimmed Videos</span></div>    <div class="hide-content"><p>还是针对数据标注的问题, 本文提出使用单个时间戳作为监督的方式来进行训练, 用这些时间戳初始化的采样分布来替换标注昂贵的动作边界, 然后用分类器的响应来迭代更新采样分布.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/single_timestamp_idea.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/timestamp_result_thumos.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Action4D: Online Action Recognition in the Crowd and Clutter</span></div>    <div class="hide-content"><p>作者试图在混乱且拥挤的环境中识别每个人的动作.</p><p>首先提出一种新的方法来用 4D 表征对人进行实时跟踪, 然后建立了一个新的网络用于识别每个被跟踪的人的动作.</p><p>为了进一步改善模型性能还设计了一个自适应的 3D 卷积核一个时间特征学习目标(目标函数???)</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/action4d_example.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/action4d_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Actional-Structural Graph Convolutional Networks for Skeleton-Based Action Recognition</span></div>    <div class="hide-content"><p>图卷积+骨架信息.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>AdaFrame: Adaptive Frame Selection for Fast Video Recognition</span></div>    <div class="hide-content"><p>也是考虑如何挑选更好的 frames 用于训练.</p><p>提出了一个增强了全局记忆的 LSTM 网络, 该记忆提供了上下文信息, 用于搜索应该使用哪些帧.</p><p>利用策略梯度来训练, AdaFrame 生成预测, 确定下一步应该观察哪一个帧, 并计算这个帧的实用性(即预期的 reward), 测试时利用预测的实用性来进行自适应的超前推理, 从而在不降低准确性的情况下降低了总的计算成本.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/adaframe_pipeline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/adaframe_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition</span></div>    <div class="hide-content"><p>骨架信息, 过.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Collaborative Spatiotemporal Feature Learning for Video Action Recognition</span></div>    <div class="hide-content"><p>现有的方法要么独立地学习时间和空间特征(C2D), 要么在参数不受限的情况下进行联合学习(C3D)</p><p>本文通过对可学习的参数添加一种权重共享的约束来协同地对时空特征进行 encode.</p><p>具体来说分别沿视频数据的三个正交视图做 2D 卷积, 分别学习空间外观信息和时序运动信息, 通过共享不同 view 的卷积核, 可以协同地学习空间和时间特征, 并且互相受益.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/three_view_of_a_video.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/cost_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/cost_residual_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/cost_result_kinetics.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Convolutional Relational Machine for Group Activity Recognition</span></div>    <div class="hide-content"><p>利用图像或者视频中每个个体之间的空间关系信息来进行 group activity recognition.它基于个人和团体活动来产生中间层次的空间表征(activity map)</p><p>然后用一个多级的 refine 模块来减少 activity map 中错误的预测, 最终用一个聚合模块使用 refine 完的信息进行最终预测</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/activity_map_example.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/CRM_pipeline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/CRM_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>DDLSTM: Dual-Domain LSTM for Cross-Dataset Action Recognition</span></div>    <div class="hide-content"><p>做 cross-dataset 的动作识别…我总感觉这些任务就是在挖坑和灌水…</p><p>目前 domain adaption 在 CNN 里面很火, 但还没人研究 RNN 中的 domain alignment, 和空间特征类似, 时序特征也可以在源域和目标域中共同学习和对齐.</p><p>本文介绍了一种 Dual-Domain LSTM 用于从 target domain 和 source domain 中学习时间依赖性, 它对 input-to-hidden 和 hidden-to-hidden 的权重进行 cross-contaminated 的 batch normalization, 并为单层和多层的 LSTM 体系结构学习 cross-contaminated 的参数.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition</span></div>    <div class="hide-content"><p>最近的工作有使用 motion vector 和 residual 来表示 motion, 因为光流计算非常耗时. 但这种方法由于 motion vector 是有噪声的, 再加上分辨率很低, 导致了学习的 motion representation 缺乏 discriminative.</p><p>作者提出了一种轻量级的生成器, 用于减少 motion vector 中的噪声并且捕捉更好的 motion details. 由于光流是更加精准的动作标注, 因此在下游分类任务上通过一个 reconstruction loss 和一个 adversarial loss 来近似光流.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/DMCNet_pipieline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/DMCNet_results.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/DMCNet_compare_and_flow_estimate.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Efficient Video Classification Using Fewer Frames</span></div>    <div class="hide-content"><p>由于模型对视频的每一帧都要处理, 所以即使内存占用量很小, 浮点运算的数量依然很大, 作者考虑用知识蒸馏的方法来解决这个问题.</p><p>具体的想法是用一个 teacher model 查看视频中的所有帧, 训练一个 student model 只查看视频中的一小部分帧, 这和通常的蒸馏有点不同, 在通常的蒸馏中, teacher model 和 student model 都查看所有的帧, 只是 student model 参数比较少.</p><p>作者在 3 种视频分类的模型上做了实验, 都表明这种方式是可行的:</p><ol><li>recurrent models</li><li>cluster-and-aggregate models</li><li>memory-efficient cluster-and-aggregate models</li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/teacher_student_network_pipeline.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/performance_of_teacher_student_model.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Large-Scale Weakly-Supervised Pre-Training for Video Action Recognition</span></div>    <div class="hide-content"><p>本文对使用大量网络视频进行预训练视频模型以进行动作识别的任务进行了深入研究。尽管有嘈杂的视频和标签, 但在这些大规模数据集下进行预训练的模型能极大改善在视频识别数据集上的表现, 所以作者研究了弱监督动作识别数据集构建过程中的 3 个问题:</p><ol><li>由于动作涉及和对象的交互, 应该如何构造 verb-object 的预训练 label space 以最大程度地使迁移学习受益?</li><li>基于 frame 的模型在动作识别方向表现出色, 预训练对良好的图像特征是否充分? 或者对时空特征进行的预训练是否对最佳的迁移学习有价值?</li><li>相对于短视频而言, 长视频中的动作通常定位较差, 在弱监督的情况下, 如何选择最佳的视频片段以获得最佳性能?</li></ol></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Learning Actor Relation Graphs for Group Activity Recognition</span></div>    <div class="hide-content"><p>用图对每个 actors 之间的交互进行建模, 再用图卷积来实现 group activity recognition.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Less Is More: Learning Highlight Detection From Video Duration</span></div>    <div class="hide-content"><p>这篇文章应该是属于 action detection 里面的…在这讲了吧…</p><p>做的是无监督的动作检测, 把整段视频的持续时间看成隐式的监督信号, 因为相比于长视频, 短视频中的视频片段更可能成为 highlight, 鉴于此作者引入了一种新的排名框架, 优先考虑较短视频中的片段, 同时适当考虑(未标记)训练数据集中的固有噪声.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/unsupervision_action_detection_result_youtube_highlight.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/unsupervision_result_tvsum.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>LSTA: Long Short-Term Attention for Egocentric Action Recognition</span></div>    <div class="hide-content"><p>提出将 Long Short-Term Attention 作为一种机制关注来自相关空间的部分, 同时 attention 在整个视频序列中被平稳的跟踪.</p><p>光看摘要不太明白这篇论文想干啥.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>MARS: Motion-Augmented RGB Stream for Action Recognition</span></div>    <div class="hide-content"><p>引入 2 种方法来训练在 RGB frames 上运行的 3D CNN, 使其模拟光流, 从而在减少光流存在的复杂计算.</p><p>首先通过最小化基于特征的损失, 重新产生了接近光流的 motion flow, 为了同时有效地使用外观和运动信息, 使用基于特征的损失和标注交叉熵损失的线性组合来进行训练, 以进行动作识别.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/mars_results.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/mars_result_kinetics.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Out-Of-Distribution Detection for Generalized Zero-Shot Action Recognition</span></div>    <div class="hide-content"><p>zero-shot learning…我还是觉得这些玩意都是挖坑和灌水, 比较有现实意义的也就全监督和弱监督, 无监督都很不可能…过</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>PA3D: Pose-Action 3D Machine for Video Recognition</span></div>    <div class="hide-content"><p>用于动作识别的 3D 模型大多建立在 RGB + 光流的基础上, 但这种方式不能充分提取 pose 动态信息 ??? 这个结论你是怎么知道的?</p><p>为了解决这个问题作者提出了一个 pose-action 3d 机制, 能用一个统一的 3D framework 有效地编码多个 pose 模态, 从而学习时空 pose 以进行动作识别.</p><p>具体的说, 提出了一种时空姿势卷积来在 frames 中聚合空间的 poses. 与经典的时间卷积不同，本文的操作可以显式地学习 pose motion 来分辨人的动作.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/pa3d_structure.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/temporal_pose_convolution.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/tpc_results.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/pa3d_result_jhmdb.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/pa3d_results_hmdb_and_charades.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Representation Flow for Action Recognition</span></div>    <div class="hide-content"><p>摘要也没太看懂, 应该是想用 CNN 来学习类似光流的 flow, 只是这个 flow 能在任何表征上得到, 而非光流一样只能在 RGB 表征上得到.</p><p>提出一种可微分的 layer 来捕获任何任何表征通道中的 flow, 以进行动作识别, 以端到端的方式和其他 CNN 模型参数一起学习, 作者还通过堆叠多个 representation flow layers 来表示 flow of flow 的概念.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/representation_flow.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/flow_layer.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/fcf_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Skeleton-Based Action Recognition With Directed Graph Neural Networks</span></div>    <div class="hide-content"><p>骨骼数据的…跳过</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Timeception for Complex Action Recognition</span></div>    <div class="hide-content"><p>本文回顾了传统的对 activity 的定义并将其限制为 “复杂动作”, 即用于特定目的的, 具有弱时间 pattern 的一组单一动作. 目前的算法使用固定 kernel size 的时空 3D 卷积, 其过于僵化以致无法在复杂的时间范围内捕获变化, 而且这种方式对于 long-range 的时间建模而言是不足够的.</p><p>本文提出了一种多尺度的 temporal 卷积, 并且降低了 3D 卷积的计算复杂度, 这种方式的结果是产生了 Timeception 的卷积层, 使得可以处理 minute-long 的时间 pattern, 比最佳的方法能处理的时间大约长了 8 倍.</p><p>作者还证明了 Timeception 学习 long-range 时间依赖性并容忍复杂动作的时间范围.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/timeception_layer.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/timeception_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</span></div>    <div class="hide-content"><p>骨骼数据的…跳过</p></div></div>]]></content>
      
      
      <categories>
          
          <category> Video Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Video </tag>
            
            <tag> Action </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Action Localization</title>
      <link href="2020/10/01/Action-Localization/"/>
      <url>2020/10/01/Action-Localization/</url>
      
        <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>首先介绍动作检测的基本知识, 后面会对这几年的顶会论文进行分析和结果展示.先看下动作检测和视频分类有什么区别吧.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/temporal_action_localization.png"></p><p>动作分类就是视频分类, 判断视频中的物体在做啥, 动作检测就是给定一段长的视频, 不但要检测出有哪些动作, 还要检测这些动作开始和结束的时间.</p><h2 id="Recent-Methods"><a href="#Recent-Methods" class="headerlink" title="Recent Methods"></a>Recent Methods</h2><p>主要分为 Anchor-Based 和 Anchor-Free</p><ol><li><p>Anchor-Based<br>又叫 top-town 网络, 主要靠的是全局的上下文信息, 需要和检测中一样, 定义多尺度的 anchor 作为 proposals, 代表的方法有 SSAD, SSTAD, CBR, TURN 等</p></li><li><p>Anchor-Free<br>又叫 bottom-up 网络, 主要靠局部的上下文信息, 首先评估类似边界概率或者动作能力, 然后利用这些来生成 proposals, 代表的方法有 TAG 和 BSN 等.</p></li></ol><h1 id="Paper-Reading"><a href="#Paper-Reading" class="headerlink" title="Paper Reading"></a>Paper Reading</h1><h2 id="CVPR’20"><a href="#CVPR’20" class="headerlink" title="CVPR’20"></a>CVPR’20</h2><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>ActionBytes: Learning From Trimmed Videos to Localize Actions</span></div>    <div class="hide-content"><p>传统的方法是在训练时使用有标注的未修剪视频, 这里在训练时使用剪短的修剪过的视频, 作者认为这种方式能从最初为动作分类设计的大规模数据集中进行受益.</p><p>作者提出一种方法来把一个视频分解为很多可解释的片段, 这些片段被称为 ActionBytes, 对 ActionBytes 进行聚类, 并将聚类结果作为伪标签来训练检测网络.</p><p>通过这样做就能在短且修剪过的数据上训练, 但对于 ActionBytes 而言都是没有修剪过的. 这样无论是单独的 ActionBytes 还是进行合并, 都可以作为有效的 proposals.</p><p>作者还展示了这种方法能够用于 zero-shot learning 和弱监督学习, 并得到了 SOTA 的结果. 下面图中的 Baseline 是指直接在 Kinetics-400 验证集上进行训练的检测模型, 没有 ActionBytes 和迭代训练, <strong>Ours</strong> 表示使用了 ActionBytes Mining 方法, <strong>Ours(+Proposals)</strong> 表示在检测时将 ActionBytes 添加到 proposals 池中</p><p>首先是在 3 个数据集上的 mAP<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/actionbytes_localization_results.png"></p><p>zero-shot 的结果<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/actionbytes_localization_zeroshot_results.png"></p><p>弱监督的结果<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/actionbytes_localization_weakly_supervised_localization_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>G-TAD: Sub-Graph Localization for Temporal Action Detection</span></div>    <div class="hide-content"><p>作者认为当前很多方法只关注时序信息而忽略了语义上下文信息(空间信息)也是一个很重要的属性.(感觉有点扯)</p><p>作者提出了一种 GCN 模型, 目的是将多级语义的上下文信息(空间信息)自适应的整合到视频特征之中, 并将动作检测问题作为子图定位问题来解决.</p><p>具体做法是将视频片段看成图的结点, 片段和片段之间的关系看成边, 与上下文相关联的动作作为目标子图.</p><p>作者设计了一个 GCN Block, 其通过聚合每个结点的上下文信息来学习每个结点的功能, 并动态更新图中的边, 为了定位每个子图, 作者还设计了一个 SGAlign Layer 来把每个子图 embedding 到欧式距离空间</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/GTAD_results_ActivityNet.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/GTAD_results_THUMOS.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Learning Temporal Co-Attention Models for Unsupervised Video Action Localization</span></div>    <div class="hide-content"><p>这篇文章主要针对弱监督和无监督的动作检测问题.</p><p>第一篇做无监督的动作检测??? 这里作者定义的无监督是指只知道视频中的动作总数(total count of unique actions), 作者把这种 task 称为 action co-localization (ACL).</p><p>为了解决这个问题作者提出了一个 two-step 的迭代过程, 聚类+检测, 聚类过程为检测过程提供有噪声的伪标签, 检测过程提供时序 co-attention model 来改善聚类性能.</p><p>作者声称主要贡献有以下 2 点:</p><ol><li>提出了 co-attention model, 不论是否针对具体类还是不针对具体类, 都能从 video-level 或者伪标签中进行学习</li><li>为 ACL 设计了新的损失函数, 包括  action-background separation loss 和 cluster-based triplet loss</li></ol><p>首先是在 THUMOS 数据集上的结果<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/coattention_THUMOS_result.png"></p><p>在 ActivityNet1.2 数据集上的结果<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/coattention_activitynet_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Learning to Discriminate Information for Online Action Detection</span></div>    <div class="hide-content"><p>这篇文章主要针对 online action detection 问题</p><p>传统方法用 RNN 来对时序信息进行建模, 但这些方法没有考虑到当前输入的视频同时包括背景, 不相关的动作以及我们需要的动作.</p><p>作者提出了一种 RNN Unit 来显示区分当前正在进行的动作和其他动作, 这种 RNN Unit 能根据与当前动作的相关性来决定是否积累输入信息, 这使得模型能够学到更具判别能力的特征, 以识别正在进行的动作.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/IDN_TVSeries.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/IDN_THUMOS14.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>METAL: Minimum Effort Temporal Activity Localization in Untrimmed Videos</span></div>    <div class="hide-content"><p>研究如何在弱监督的条件下对未知的动作进行检测. 其实…就是挖坑, 提出了这种很无语的问题, 你全监督的结果都不咋地, 更别说这个 task 了</p><p>这问题简单来说就是只给出很少的 examples, 在未经修剪过的视频中查找语义相关的视频片段, 模型的训练是只在弱监督下做的…</p><p>为了达到这个目的, 提出了一个 Similarity Pyramid Network (SPN), SPN 使用了 Relation Network 的 few-shot learning 并直接编码分层的多尺度相关性, 学习 2 个互补的损失函数来进行学习.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/METAL_activitynet_result.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/METAL_THUMOS_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Weakly-Supervised Action Localization by Generative Attention Modeling</span></div>    <div class="hide-content"><p>针对弱监督的动作检测</p><p>通常的网络都依赖于 Classification Activation, 类似于 CAM 的东西, 用 attention model 来标识并分类 frames, 但这种方法容易把接近 action frames 的 frames 也算入 action frames, 作者称为 action-context confusion issue</p><p>为了解决这个问题, 作者使用 VAE 来对 class-agnostic 的 frame-wise 概率进行建模.</p><p>作者观察到上下文信息和 representation-level 的动作有比较显著的差异, 所以在给定的 attention 下, 用一个条件 VAE 来对每个帧的可能性进行建模, 通过最大化和 attention 相关的条件概率来将 action 和 non-action 的 frames 进行分离.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/DGAM_THUMOS_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>ZSTAD: Zero-Shot Temporal Activity Detection</span></div>    <div class="hide-content"><p>又是挖坑之作, 提了个新任务叫 zero-shot temporal activity detection (ZSTAD), 是想检测训练集中没有的动作.</p><p>基于 R-C3D 来做的, 提出了一个损失函数来优化模型, 这个函数考虑了每种 action 和其超类的 embedding, 以这种方式能够学习已知和未知的 action 的语义信息.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/ZSTAD_THUMOS.png"></p></div></div><h2 id="ECCV’20"><a href="#ECCV’20" class="headerlink" title="ECCV’20"></a>ECCV’20</h2><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Action Localization through Continual Predictive Learning</span></div>    <div class="hide-content"><p>研究动作识别中的自监督学习</p><p>提出一种基于 continual learning 方法来使用 feature-level 的预测进行自监督. 就 frame-level 的 bbox 而言在训练过程中无需任何的标注. 本文想法收到 cognitive model 的启发, 该模型使用 prediction-based 方法进行事件理解.</p><p>将多个堆叠的 LSTM 和一个 CNN 编码器结合起来用, 并使用 attention 机制来对视频中的事件进行建模, 以此来预测未来帧的高级特征. 错误的预测能用于持续更新模型参数.</p><p>这种 self-supervised 方法比其他方法简单, 且对于 label 和 localization 方面能学到强大的特征. 值得注意的是该方法以流的方式输出, 所以只需要经过一次视频, 这也使得这种方式能达到实时</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/continue_learning_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Actions as Moving Points</span></div>    <div class="hide-content"><p>针对 action tubelet detection, 就是还需要检测做动作的人的位置, 这样检测的结果像一个管道.</p><p>把 CenterNet 的思想拿过来了, 把每个 action instance(做动作的人在空间中的位置) 看成一个点移动的轨迹.</p><p>提出的方法有 3 个 branches:</p><ol><li>Center Branch 用于对每个实例进行中心检测和动作识别</li><li>Movement Branch 用于在相邻帧之间进行运行估计以形成运动点的轨迹.</li><li>用于回归每个中心点的 bbox 的 Box Branch</li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/MOC_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Adaptive Video Highlight Detection by Learning from User History</span></div>    <div class="hide-content"><p>有点类似于推荐? 根据用户以前已经创造过的 highlight 来检测更适合用户的 highlight.</p><p>有 2 个子网络, 一个全卷积的时序 highlight 检测网络 H 和一个根据用户历史记录得到的历史编码网络 M.</p><p>向 H 中引入一个 temporal-adaptive instance normalization layer 来实现 2 个子网络的交互, 这个 layer 能从 M 中预测 affine parameter 并且向 H 传递用户自适应信号.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization</span></div>    <div class="hide-content"><p>为什么又是弱监督??? 全监督的效果也不咋地啊!!!</p><p>为了解决弱监督方法会把没发生动作的视频片段识别为发生动作的片段的问题.</p><p>提出了 2 个三元组, 第一个用于为每个动作类别学习可判别的特征, 第二个用于将没发生动作的特征和发生动作的特征区分开来.</p><p>利用对抗的思想来训练网络, 设置 2 个并行的 branches, 第一个用于检测视频中最可能发生动作的片段, 第二个用于从视频没被定位的地方找到其他补充的动作.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/A2CL-PT_results.png"><br>提出一种异步交互聚合的方式来增强 action detection.</p><p>主要设计了 2 个东西:</p><ol><li>设计了 Interaction Aggregation structure, 用统一的范式来建模和集成多种类型的交互.</li><li>设计了 Asynchronous Memory Update algorithm, 使我们能通过动态建模长时间的交互以获得更好的性能.</li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/AIA_AVA_result.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/AIA_results_UCF.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Bottom-Up Temporal Action Localization with Mutual Regularization</span></div>    <div class="hide-content"><p>目前的方法都有 3 个阶段: 开始时间, 持续时间和结束时间, 就扯呗… 知道开始时间和持续时间不就知道结束时间了么???</p><p>这篇文章深入研究了这种机制, 认为目前的方法将这些阶段都建模为单独的分类任务会忽略这些阶段之间潜在的一种时间约束. 所以当输入的某些帧缺少足够的判别性信息时, 会导致分类错误.</p><p>为了解决这个问题作者提出了 2 个正则化项来共同调整学习过程:</p><ol><li>the Intra-phase Consistency (IntraC) regularization 用于在每个阶段内得到验证</li><li>the Inter-phase Consistency (InterC) regularization 用于维持这些阶段之间的连续性</li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/mutual_regularization_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Boundary Content Graph Neural Network for Temporal Action Proposal Generation</span></div>    <div class="hide-content"><p>目的是为了提高 proposals 的边界的准确率以及得到置信度高的 proposal, 用图神经网络来对边界和 proposals 的动作内容之间的关系进行建模</p><p>每个 temporal proposal 的边界和内容分别作为图的结点和边, 然后提出了一种新颖的图计算操作来更新边和节点的特征。接着使用更新后的边和它连接的两个节点来预测边界的概率和内容的置信度.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/BCGNN_activitynet_result.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/BCGNN_THUMOS_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>CFAD: Coarse-to-Fine Action Detector for Spatiotemporal Action Localization</span></div>    <div class="hide-content"><p>提出了一种新的范式, 首先预测一个粗糙的 action tubes, 然后基于 key timestamps 来 refine 这些 tube 的位置.</p><p>粗糙模型中的长时序信息的参数化建模能获得比较准确的初始化的 tube, 然后 Refine 模块根据 timestamps 来选择性地调整 tube 的位置</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/CFAD_results.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/CFAD_UCF_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Context-Aware RCNN: A Baseline for Action Detection in Videos</span></div>    <div class="hide-content"><p>时空定位的论文</p><p>这篇文章指出识别精度和一个 actor 的 bbox 的 size 高度相关, 因此 actors 的分辨率越高会有更好的性能. 但由于视频需要进行密集采样才能实现准确识别, 受限于 GPU 内存, 到 backbone 的帧的分辨率必须是很低的, 这导致了进入 RoI Pooling 层的特征都很粗糙.</p><p>所以在使用 I3D 来进行特征提取之前, 作者先在 actor 周围 crop 和 resize 图像块. 此外作者还发现拓展 actor 的 bbox 能轻微提高性能, 融合上下文信息能进一步提升性能.</p><p>作者认为我们需要重新考虑分辨率在以 actor 为中心的动作识别中的作用.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/context-aware_rcnn_ava_results.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/context-aware_rcnn_result_ava_per_class.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Learning to Localize Actions from Moments</span></div>    <div class="hide-content"><p>原文写的真的不好翻译…</p><p>作者只需要以下 2 个东西就能实现对很多类别的 action detection</p><ol><li>某些特定类别的 action moments(只有动作的视频, 没有背景视频片段)</li><li>一小部分类别的视频, 这些视频是未裁剪的, 但有标注信息</li></ol><p>用一个权重转移方程来建立转换, 转换是指 action moments/foreground video 的分类和  未修剪视频/synthetic contextual moments  的 localization 之间的转换.</p><p>通过对抗机制学习每个 action moment 的上下文信息, 以区分这些从未裁剪视频的背景中所生成的特征</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/AherNet_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Localizing the Common Action Among a Few Videos</span></div>    <div class="hide-content"><p>作者提出了一个新的任务, 叫 few-shot common action localization, 即一个长的未经裁剪的视频中的动作的开始和结束时刻仅根据包含相同动作的修剪过完整视频来确定, 无需知道它们的共同类别标签.</p><p>为了解决这个任务提出了一种新的 3D 卷积结构, 这种卷积能将视频中的 representation 和相关的查询视频片段对齐.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>MINI-Net: Multiple Instance Ranking Network for Video Highlight Detection</span></div>    <div class="hide-content"><p>又是做弱监督的, 我吐了</p><p>把每个视频视为很多个细分的小视频段, 然后提出的方法能够给 positive bag 更高的 score, 给 negative bag 更低的 score. 具体是用了一个 max-max ranking loss 来获得一个最可能是 positive bag 和 一个最可能是 negative bag 之间可靠的比较.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/MININet_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>SF-Net: Single-Frame Supervision for Temporal Action Localization</span></div>    <div class="hide-content"><p>简单来说就是只标注每个动作的一个帧就行, 无需标注动作的开始和结束时间, 这样能比弱监督的方式得到更好精度.</p><p>这篇文章是研究如何在这种设定下进行 action detection</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/SFNet_result_THUMOS.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Two-Stream Consensus Network for Weakly-Supervised Temporal Action Localization</span></div>    <div class="hide-content"><p>通过迭代的 refine training method 来更新一个 frame-level 的伪标签, 用这个伪标签来增强弱监督方法中的模型训练和消除 FN 样本.</p><p>还提出了一种新的 attention normalization loss 来使预测的注意类向 binary choice 一样起作用, 并促进动作边界的检测.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/two-stream-consensusnetwork_results_thumos.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/two-stream-consensusnetwork_results_activitynet.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed Videos</span></div>    <div class="hide-content"><p>时空检测的弱监督…吐血</p><p>展示了如何使用一种新的MIL概率变体在标准的 multiple instance learning assumption（每个 bag 至少包含一个带有指定标签的 instance）的情况下应用我们的方法，并估算了每个预测的不确定性。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/UAWSAD_results_UCF.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Weakly-Supervised Action Localization with Expectation-Maximization Multi-Instance Learning</span></div>    <div class="hide-content"><p>弱监督动作检测能够在 Multiple Instance Learning (MIL) 框架下被解决, 其中一个 bag(video) 含有多个 instance (action segments), 因为只有 bag 的标签是知道的, 所以主要的挑战就在于用 bag 中的哪些 instance 来触发 bag 的标签.</p><p>许多现有的方法都采用 attention 机制, 利用 attention 来从 instance 中生成 bag 的 representation, 然后通过 bag 的分类来对这些 representation 进行训练, 这些模型隐式地违反了 MIL 的前提假设, 即 negative bag 中的 instances 应该都为 negative.</p><p>本文的方法将关键的 instance 建模为隐变量, 然后用 EM 算法来求解, 提出了 2 中伪标签生成方法来对 E 过程和 M 过程建模, 并迭代地优化似然的下界.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/EM-MIL_results_THUMOS.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/EM-MIL_results_activitynet.png"></p></div></div><h2 id="CVPR’19"><a href="#CVPR’19" class="headerlink" title="CVPR’19"></a>CVPR’19</h2><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>A Structured Model for Action Detection</span></div>    <div class="hide-content"><p>将其他 domain 的知识整合到模型结构中以简化优化, 以此来提升 action detection 的效果</p><p>具体的做法是利用跟踪模块拓展了 I3D 网络, 以此来聚合 long-term 的运动模式, 并使用一个 GCN 来对 actor 和 object 之间的交互进行推理.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/structured_model_result_ava.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Completeness Modeling and Context Separation for Weakly Supervised Temporal Action Localization</span></div>    <div class="hide-content"><p>做弱监督的</p><p>首先表明弱监督方法存在 2 个问题, 分别叫做  action completeness modeling 和 action-context separation.</p><p>为了对 action completeness 进行建模, 提出了一种多分支的结构, 每个分支都被迫使去发现独特的 action 部分. 这样完整的动作就能够通过融合来自不同 branches 的 activations 来定位.</p><p>为了将 action instance 和其周围环境分开, 作者使用固定大小的视频片段生成了 hard negative data 来训练, 这种方法的先验在于缺少动作的视频片段不太可能是行为片段</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/CMCS_THUMOS.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/CMCS_result_activitynet.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Dance with Flow: Two-in-One Stream Action Detection</span></div>    <div class="hide-content"><p>时空检测问题.</p><p>想法是把 RGB Stream 和 Flow Stream 给 embedding 到一个 stream 里面.</p><p>一个 motion conditional layer 从 flow images 中提取 motion information, 被 motion modulation layer 用来生成 transformation parameters, 这个 transform parameters 用于对 low-level 的 RGB 特征建模</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/two_in_one_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Gaussian Temporal Awareness Networks for Action Localization</span></div>    <div class="hide-content"><p>目前基于检测的方法用的都是固定尺寸的 proposal, 本文提出用高斯核来动态调整每个 action proposal 的 temporal scale.</p><p>本文通过学习一组 Gaussian Kernel 来对时序结构进行建模, 每个 kernel 对应特征图中的一个 cell.</p><p>每个 kernel 对应了 action proposal 的一个特定的间隔, 这样多个 kernels 的混合可以代表不同长度的 action proposal.</p><p>其中每个高斯曲线都能反映对一个 action proposal 进行定位的上下文信息的贡献程度.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/GTAN_results.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/gtan_result_thumos.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/gtan_result_activitynet.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Improving Action Localization by Progressive Cross-stream Cooperation</span></div>    <div class="hide-content"><p>做时空检测的</p><p>时空检测可以看成 3 个 task:</p><ol><li>空间检测</li><li>动作分类</li><li>时序分割(具体到某一帧的分类)</li></ol><p>本文的想法是利用 two-stream 中其中一个 stream 的 region proposals 和提取到的特征来帮助另一个 stream, 以此提高定位结果和生成更好的 bbox.</p><p>首先通过结合 2 个 stream 中的最新 proposals 来得到更多的 proposals, 这样就能获得更多的带有 label 的训练集, 能学到更好的动作检测模型.</p><p>然后提出了一种新的消息传递方法, 用于把信息从一个 stream 传到另一个 stream, 以学习更好的 representation.</p><p>为了进一步提升结果还提出了一种新的策略用于训练特定类别的 action detector, 以实现更好的时序分割.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/pcsc_result_ucf.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Language-driven Temporal Activity Localization: A Semantic Matchin Reinforcement Learning Model</span></div>    <div class="hide-content"><p>挖坑… 用过句子查询来定位动作, 跳过</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/SMRL_results.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos</span></div>    <div class="hide-content"><p>用动态骨骼特征来进行异常检测, 跳过…</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Long-Term Feature Banks for Detailed Video Understanding</span></div>    <div class="hide-content"><p>Kaiming He 的作品</p><p>为了理解世界, 人类需要不断地把现在和过去联系起来, 并将事件置于背景之中.</p><p>本文提出了一种 long-term feature bank, 这种方法可以在整个视频范围内提取信息, 而非只能提取 2-5 秒内的局部信息. 如下图所示:</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/3D_vs_LTFB.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/LFB_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Multi-Granularity Generator for Temporal Action Proposal</span></div>    <div class="hide-content"><p>提出一种 multi-granularity (多粒度) 生成器来从不同粒度的角度得到不同的 proposals</p><p>首先使用一个 bilinear matching model 来得到局部信息, 然后通过组合 segment proposal producer (SPP) 和 frame action producer (FAP) 以 2 种不同的粒度来得到 proposals.</p><p>SPP 以特征金字塔的形式考虑整个视频, 并从一个粗略的角度生成分段的 proposals, 而 FAP 对每个视频进行更精细的 proposals 评估.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/mgg_result_thumos.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Peeking Into the Future: Predicting Future Person Activities and Locations in Videos</span></div>    <div class="hide-content"><p>Feifei Li 的作品, 也算是挖坑, 预测行人未来的轨迹和动作…</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Spatio-temporal Video Re-localization by Warp LSTM</span></div>    <div class="hide-content"><p>又是挖坑…提了个任务, 给定一个 query video 和一个 reference video,  Spatio-temporal Video Re-localization 的目的是在 reference video 中定位和 query video 有相同语义内容的 tubelets.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>STEP: Spatio-Temporal Progressive Learning for Video Action Detection</span></div>    <div class="hide-content"><p>做时空检测的</p><p>逐步 refine 得到的 tubelets, 在每个 step 中都会适当地拓展 proposals 的 size, 使其能够纳入更多相关的时序上下文信息.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/step_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Video Action Transformer Network</span></div>    <div class="hide-content"><p>设计了一种类似 Transformer 的结构, 将时空上下文信息聚合到 actor 周围, 通过使用高分辨率的, 以人为中心的, 类别无关的查询, 模型能够自发地学习跟踪单个人并从他人的行为中获取语义上下文信息.</p><p>这种注意力机制结果会更倾向于人的手和脸, 这通常是区分动作的关键, 除了 bbox 和类别标签外没有其他的监督.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/transformer_result_txhead.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection</span></div>    <div class="hide-content"><p>作者认为当前方法效果不好的原因在于有些不是 action 的视频段的 state 和真实 action 视频段的 state 相似, 所以导致了分类错误, 这种 state 被作者称为 “transitional states”</p><p>作者提出了 2 个东西来解决这个问题, temporal context detector 和 transition-aware classifier.</p><p>temporal context detector 通过构建一个 RNN 来提取 long-term 的上下文信息, transition-aware classifier 通过同时对 action 和 transitional state 进行分类来区分 transitional states.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/TACNet_result_ucf.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/TACNet_results.png"></p></div></div><h2 id="ICCV’19"><a href="#ICCV’19" class="headerlink" title="ICCV’19"></a>ICCV’19</h2><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization</span></div>    <div class="hide-content"><p>介绍了一种新颖的公式来学习 discriminative 的 action 特征, 公式主要有 3 个项:</p><ol><li>一个用于确保学到的特征具有可区分性的分类项</li><li>一个自适应的 center loss 项来增强特征的可判别性</li><li>一个计数损失项来描述相邻的动作序列, 从而改善定位.</li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/3cnet_result_thumos.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/3cnet_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>BMN: Boundary-Matching Network for Temporal Action Proposal Generation</span></div>    <div class="hide-content"><p>目前的方法能够产生精准的边界, 但无法产生很高的置信度, 所以作者提出了一种 MB 机制来评估密集分布的 proposals 的置信度得分.</p><p>具体做法是将一个 proposal 看成一组匹配的开始点和结束点, 并将所有密集分布的 BM 对组合到 BM 置信图中.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/BMN_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Graph Convolutional Networks for Temporal Action Localization</span></div>    <div class="hide-content"><p>用 GCN 来做, 每个 proposal 是一个节点, proposal 和 proposal 之间的关系是边, 这里使用了 2 种类型的关系, 一种用于为每个 proposal 捕捉上下文信息, 另一种用来表征不同动作之间的相关性.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/PGCN_result_thumos.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/PGCN_result_activitynet.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Learning Motion in Feature Space: Locally-Consistent Deformable Convolution Networks for Fine-Grained Action Detection</span></div>    <div class="hide-content"><p>搞细粒度的动作检测,</p><p>提出了一种局部连续的可变形卷积, 它利用感受野的变化和局部相干约束来有效地捕捉动作信息.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Learning Temporal Action Proposals With Fewer Labels</span></div>    <div class="hide-content"><p>做半监督的, 直接上结果吧</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/semi_supervised_action_proposal_result_thumos.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>StartNet: Online Detection of Action Start in Untrimmed Videos</span></div>    <div class="hide-content"><p>把 Online Detection of Action Start (ODAS) 看成一个 two-stage 的问题: 使用 ClsNet 来进行动作识别, 使用 LocNet 来进行 start point 的检测.</p><p>ClsNet 对每一帧都打标签, 并且在线预测动作的 score 分布, 然后 LocNet 使用策略梯度来优化 long-term 的定位, 以此实现与类别无关的检测.</p><p>不是很能看明白这个结果, 还是放上来吧<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/startnet_result.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Temporal Recurrent Networks for Online Action Detection</span></div>    <div class="hide-content"><p>看名字就很直观了, 用 RNN 来做 online action detection,具体地, 在每个时间步, 我们都利用累计的历史信息和预测的未来信息来更好地识别当前正在发生的行为, 并将两者整合为一个统一的架构.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/TRN_result_thumos.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Temporal Structure Mining for Weakly Supervised Action Detection</span></div>    <div class="hide-content"><p>弱监督…</p><p>现在的弱监督方法基本都是通过给每个视频段打分来检测动作段, 但这些方法都很难对视频段之间的时序关系进行建模, 且不能表征具有潜在时间结构的 action instance.</p><p>为了解决这个问题作者提出了 temporal structure mining (TSM) 的方法, 在 TSM 中每个 instance 都被建模为一个多阶段的过程.</p><p>在本文的方法中, phase filters 被用来计算每个细分动作阶段的置信度得分, 因为弱监督没有 frame-level 的标注, 所以无法直接训练 phase filters. 作者把每个分段的 phase 看成隐变量, 使用每个片段的 confidence score 来构造一个表, 然后利用最大循环路径来确定隐藏的隐变量, 也就是每个 segment 的 phase.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Weakly Supervised Temporal Action Localization Through Contrast Based Evaluation Networks</span></div>    <div class="hide-content"><p>弱监督…</p><p>弱监督 action detection 只有 video-level 的标注, 这不可避免地会导致一些错误, 尤其是在为裁剪的视频中. 为了解决这个问题本文提出了一种新的 action proposal evaluator, 通过在 snippet-level 的动作分类上使用时序限制来提供伪监督.</p><p>从本质上讲，新的 action proposal evaluator 会强制一个额外的时序约束, 因此置信度高的 proposals 会更有可能和真实的 action instance 吻合.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/cleannet_result_thumos.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/CleanNet_result_acativitynet.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Weakly-Supervised Action Localization With Background Modeling</span></div>    <div class="hide-content"><p>弱监督…</p><p>使用一个 attention 模型来同时对前景帧和背景帧进行特征提取, 之前的方法忽略了背景, 但本文发现通过对背景也进行建模, 我们的系统可以学习到更加丰富的 action 和时序的信息.</p><p>本文将自下而上的, 与类别无关的注意力模块与自上而下的, 类别特定的 activation map 结合, 让后者作为前者的自监督形式, 这样可以让模型在没有明确的时间监督下学习更准确的 attention 模型.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/WSALB_result_thumos.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/WSALB_result_activitynet.png"></p></div></div>]]></content>
      
      
      <categories>
          
          <category> Video Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Detection </tag>
            
            <tag> Video </tag>
            
            <tag> Action </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to Visual Tracking</title>
      <link href="2020/09/29/Introduction-To-Visual-Tracking/"/>
      <url>2020/09/29/Introduction-To-Visual-Tracking/</url>
      
        <content type="html"><![CDATA[<h1 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a>Basic Knowledge</h1><p>模型主要分为2大类: 生成和判别, 目前最火的是判别方法,  即 tracking-by-detection.</p><ol><li><p>生成类方法<br>在当前帧对目标区域建模，下一帧寻找与模型最相似的区域就是预测位置，比较著名的有卡尔曼滤波，粒子滤波，mean-shift 等。举个例子，从当前帧知道了目标区域80%是红色，20%是绿色，然后在下一帧，搜索算法就像无头苍蝇，到处去找最符合这个颜色比例的区域</p></li><li><p>判别类方法<br>CV中的经典套路图像特征+机器学习， 当前帧以目标区域为正样本，背景区域为负样本，机器学习方法训练分类器，下一帧用训练好的分类器找最优区域</p></li></ol><p>与生成类方法最大的区别，是分类器训练过程中用到了背景信息，这样分类器就能专注区分前景和背景，所以判别类方法普遍都比生成类好。</p><p>以下内容主要是对文章 <a href="https://arxiv.org/abs/1912.00535">Deep Learning for Visual Tracking: A Comprehensive Survey</a> 进行翻译和自行整理.</p><h1 id="发现"><a href="#发现" class="headerlink" title="发现"></a>发现</h1><ol><li>SNN-based 方法最受欢迎, 因为能很好满足速度和精度上的 trade-off</li><li>最近的方法尝试用 RL 和 GAN 来 refine 决策以及缓解数据不足的问题</li><li>offline 的端到端学习适当地适应了预训练的特征</li><li>虽然 online 训练增加了计算复杂度导致大多数方法无法实时, 但这也帮助跟踪器得到了很好的 appearance 信息, 剔除了部分视觉干扰并提升了模型的精度和鲁棒性</li><li>同时使用 online training 和 offline training 能得到更鲁棒的跟踪器</li><li>使用更宽或者更深的 backbone 可以提高跟踪器区分目标和背景的能力</li><li>目前最好的跟踪器同时使用分类和回归的目标函数, 不但预测 target proposals 也预测 tightest bounding box</li><li>多种特征的融合能提高模型鲁棒性</li><li>一些互补特征的融合能提点+提高鲁棒性</li><li>目前最大的挑战还是来自遮挡, 消失及快速移动, 如果有相似语义的目标可能导致漂移的问题</li></ol><h1 id="模型分类"><a href="#模型分类" class="headerlink" title="模型分类"></a>模型分类</h1><h2 id="按网络结构分类"><a href="#按网络结构分类" class="headerlink" title="按网络结构分类"></a>按网络结构分类</h2><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><h4 id="为了得到更好的特征"><a href="#为了得到更好的特征" class="headerlink" title="为了得到更好的特征"></a>为了得到更好的特征</h4><ol><li>在大数据集上做 offline training</li><li>设计更加特殊的网络结构, 而不是使用预训练的模型</li><li>构造多目标模型来捕获目标外观的多样性</li><li>合并时序信息和空间信息来增强泛化能力</li><li>用特征融合增强空间和语义信息</li><li>学习不同的目标模型(relative model/part-based model)来处理部分遮挡和变形</li><li>使用 Two-Stream 网络来防止过拟合以及学习旋转信息</li></ol><h4 id="平衡训练数据"><a href="#平衡训练数据" class="headerlink" title="平衡训练数据"></a>平衡训练数据</h4><p>对于单目标跟踪来说只有一个 positive sample, 也就是第一帧中被标注的对象, 这有可能导致过拟合, 尽管可以将任意的背景信息视为每个帧中的背景作为 negative information, 但不完美的目标估计也可能导致不可靠的训练样本, 这些问题极大影响了跟踪的精度.</p><ol><li>domain adaption</li><li>various update mechanisms(e.g., periodic, stochastic, short-term, and long-term updates)</li><li>convolutional Fisher discriminative analysis (FDA) for positive and negative sample mining</li><li>efficient sampling strategies to increase the number of training samples</li></ol><h4 id="计算复杂度问题"><a href="#计算复杂度问题" class="headerlink" title="计算复杂度问题"></a>计算复杂度问题</h4><ol><li>把大的 CNN 分解为几个小的网络</li><li>对训练样本空间进行压缩或者剪枝</li><li>使用 RoI 来进行特征计算, 或者使用 oblique random frost 来获取更好的数据</li><li>corrective domain adaption method</li><li>轻量级结构</li><li>高效的优化过程</li><li>充分利用相关滤波</li><li>particle sampling strategy</li><li>使用 attention</li></ol><h3 id="SNN"><a href="#SNN" class="headerlink" title="SNN"></a>SNN</h3><p>给定 target 和 search regions, 孪生网络计算 same function 来得到一个 similarity map, SNN-based 模型的共同目标是为了克服预训练 CNN 的限制以及充分利用 end-to-end learning 来进行实时跟踪.</p><h4 id="为了得到判别性的特征"><a href="#为了得到判别性的特征" class="headerlink" title="为了得到判别性的特征"></a>为了得到判别性的特征</h4><ol><li>学习 distractor-aware 或者 target-aware 的特征</li><li>多级特征融合或者结合 confidence map</li><li>在 siamese function 中使用不同的 loss function 以得到更有效的 filters</li><li>使用不同的特征, 例如上下文信息或者时序信息</li><li>开拓 low-level 的空间特征</li><li>考虑角度估计来防止显著性的背景物体</li><li>使用 multi-stage refine 来目标特征</li><li>使用更深/宽的 backbone</li></ol><h4 id="适应目标外观变换"><a href="#适应目标外观变换" class="headerlink" title="适应目标外观变换"></a>适应目标外观变换</h4><ol><li>online update strategy</li><li>background suppression</li><li>把跟踪看成一个 one-shot 的 local detection 问题</li><li>给重要的 feature channel 或者 score maps 更高的权重</li></ol><p>或者, DaSiamRPN 和 MMLT 使用 local-to-global 的区域搜索策略来处理全遮挡, 消失问题, 使用 memory exploitation 来增强 local search strategy</p><h4 id="平衡训练数据-1"><a href="#平衡训练数据-1" class="headerlink" title="平衡训练数据"></a>平衡训练数据</h4><ol><li>使用 multi-stage Siamese framework 来模拟 hard negative sampling</li><li>使用多级采样, 例如固定背景的比例, 使用随机采样, 或者 flow-based 采样</li><li>充分利用相关滤波</li></ol><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>RNN是用来考虑时序信息的, 除此以外, 还有一个作用是用于避免使用预训练的 CNN 以防止过拟合(你难道不会 train from scratch 吗?) 这类方法的主要目的如下:</p><ol><li>捕获时序信息, 融合空间和时序信息</li><li>利用上下文信息来处理复杂的背景</li><li>使用 multi-level 的视觉 attention 来增强 target 及抑制背景</li><li>使用 LSTM 来得到 long-term 的信息</li><li>对目标的自身结构进行编码, 以降低与相似干扰物有关的跟踪灵敏度</li></ol><h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><p>主要是为了增强训练样本以及目标模型, 这些网络能够在特征空间中增加 positive samples 来解决训练样本的分布不平衡问题, 此外 GAN 也能学习到更加 general 外观分布来解决跟踪中的 self-learning 问题.</p><h3 id="Custom-Networks"><a href="#Custom-Networks" class="headerlink" title="Custom Networks"></a>Custom Networks</h3><p>就是把不同的网络结构给结合起来, 包括 AE, CNN, RNN, SNN 和 RL. 这种网络的主要的 Motivation 和贡献有</p><h4 id="计算复杂度问题-1"><a href="#计算复杂度问题-1" class="headerlink" title="计算复杂度问题"></a>计算复杂度问题</h4><p>TRACA 和 AEPCF 使用 AEs 来压缩原始的特征, EAST 自适应地将浅层特征用于简单帧的跟踪, 将 expensive feature 用于困难帧的跟踪, TRACA , CFSRL 和 AEPCF 还利用了 DCF 的计算有效性.</p><h4 id="模型更新"><a href="#模型更新" class="headerlink" title="模型更新"></a>模型更新</h4><p>这里的更新是指更新 template, 因为目标的形状或者其他条件变了之后需要更新目标的特征, 这样才能更好的进行跟踪.</p><p>为了在跟踪国产中保持 target model 的稳定性, 提出了不同的更新策略.  CFSRL 并行地更新多个模型, DRRL 使用 LSTM 处理长时间依赖, AEPCF 同时使用 long-term 和 shot-term 更新机制来加快跟踪速度.</p><p>DRT 把跟踪的公式修改为下一次定位的最佳 object template 的一个连续决策过程, 此外也可以使用 RL 来进行有效的决策.</p><h4 id="受限的训练数据"><a href="#受限的训练数据" class="headerlink" title="受限的训练数据"></a>受限的训练数据</h4><p>在遮挡, 模糊或者形变过大的情况下, soft 且没有代表性的训练样本会干扰视觉跟踪.</p><p>AEPCF 提出使用一种密集的循环采样方式来防止由于受限数据导致的过拟合, SINT++使用 positive sample generation network (PSGN) 和 hard positive transformation network (HPTN) 来生成 positive samples 以及 hard training samples.</p><p>部分被标记的 training samples 被用来训练 action-driven 的跟踪器</p><h4 id="搜索策略"><a href="#搜索策略" class="headerlink" title="搜索策略"></a>搜索策略</h4><p>搜索策略的定义就是如何预测需要被跟踪的目标在下一帧的状态, 目前最佳的搜索区域取决于迭代搜索策略, 但这种策略通常和视频内容无关, 而且还需要蛮力搜索并且手工设计</p><p>最近有使用 RL 来学习 data-driven 的搜索策略, 为了全面地得到 RoI 和最佳的 Candidate, 这种机制考虑了目标的上下文变换和目标的运动, 此外 ACT 和 DRRL 提出了基于 coarse-to-fine verification 和 dynamic search process 的实时 RL-based 搜索策略</p><h4 id="捕获额外信息"><a href="#捕获额外信息" class="headerlink" title="捕获额外信息"></a>捕获额外信息</h4><p>DCTN 使用 Two-Stream 网络, SRT 使用 multi-directional RNN 来学习跟踪过程中目标的进一步依赖性, DRL-IS 提出一种 Actor-Critic 网络来估计目标运动参数</p><h4 id="决策"><a href="#决策" class="headerlink" title="决策"></a>决策</h4><p>Online 决策对 DL-based 的跟踪方法的性能有主要影响,  P-Track 使用 data-driven 的技术来决定跟踪, 重新初始化或者更新过程. DRL-IS 根据目标状态来选择最佳的行为, 也有工作提出一个动作预测网络来调整跟踪器的连续动作, 以此决定最佳的策略.</p><h2 id="按网络用途分"><a href="#按网络用途分" class="headerlink" title="按网络用途分"></a>按网络用途分</h2><p>大致分为2个:</p><ol><li>重用在相关数据集上预训练的模型</li><li>为跟踪任务重新训练模型</li></ol><h3 id="使用预训练模型"><a href="#使用预训练模型" class="headerlink" title="使用预训练模型"></a>使用预训练模型</h3><p>在 ImageNet 上或者在检测数据集上进行预训练, 然后对不同分辨率的特征进行融合, 总的来说就2个主要的点, 一个是提取动作信息, 另一个是选择适合跟踪任务的特征.</p><h3 id="使用现有的特征"><a href="#使用现有的特征" class="headerlink" title="使用现有的特征"></a>使用现有的特征</h3><p>使用现有特征会限制跟踪器的性能, 因为 offline trained 的模型可能捕捉不到目标变换, 而且会对初始的 target templates 过拟合.</p><p>目前的方法通常都会用 offline training 或者 online training 或者都使用来预训练 backbone</p><h2 id="按训练方式分类"><a href="#按训练方式分类" class="headerlink" title="按训练方式分类"></a>按训练方式分类</h2><h3 id="offline-training"><a href="#offline-training" class="headerlink" title="offline training"></a>offline training</h3><p>传统的那一套, 在相关的其他数据集上进行预训练</p><h3 id="online-training"><a href="#online-training" class="headerlink" title="online training"></a>online training</h3><p>online training 是为了针对那些外观信息变化较大的目标, 通常使用整个 DNN 或者固定一部分, 对另一部分进行 online training 来调整网络参数, 使其能够捕获一些外观信息改变的物体.</p><p>但考虑到在大规模数据集上进行 offline training 比较耗时间并且学到的特征不是很适合于其他任务, 所以基本还是 offline 对模型进行训练, 只是在 inference 的时候再 online 更新模型参数.(use directly training of DNNs and inference process alternatively online.)</p><h3 id="both-online-training-and-offline-training"><a href="#both-online-training-and-offline-training" class="headerlink" title="both online training and offline training"></a>both online training and offline training</h3><p>offline training 和 online training 得到的特征被证明是可以共享, 且是和领域相关的, offline feature 主要将目标与前景信息分开, online training 的目的是将类内干扰因素区分开</p><h2 id="按照目标函数分"><a href="#按照目标函数分" class="headerlink" title="按照目标函数分"></a>按照目标函数分</h2><ol><li>都是分类</li><li>都是回归</li><li>分类+回归<br>前面都是在扯淡, 就这个靠谱, 和目标检测一样, 分类用于对bbox分类, 回归用于调整 bbox 坐标.</li></ol><h2 id="按模型输出分"><a href="#按模型输出分" class="headerlink" title="按模型输出分"></a>按模型输出分</h2><ol><li>confidence map (also includes score map, response map, and voting map)</li><li>Bounding Box (also includes rotated Bounding Box)</li><li>object score (also includes probability of object proposal, verification score, similarity score, and layerwise score)</li><li>action</li><li>feature maps</li><li>segmentation mask</li></ol><h2 id="相关滤波"><a href="#相关滤波" class="headerlink" title="相关滤波"></a>相关滤波</h2><p>基于 DCF (Discriminative Correlation Filter) 的方法旨在学习一组判别滤波器，这些判别滤波器将它们与频域中的一组训练样本进行逐元素相乘即可确定空间目标位置.</p><p>使用相关滤波的方法, 主要根据如何使用相关滤波来进行分类, 主要有利用相关滤波来高效地进行计算, 使用 correlation layer 或者相关目标函数等.</p><h1 id="Benchmark-数据集和评价指标"><a href="#Benchmark-数据集和评价指标" class="headerlink" title="Benchmark 数据集和评价指标"></a>Benchmark 数据集和评价指标</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>直接上图吧<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/datasets.png"></p><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><ol><li><p>Center Location Error(CLE)<br>GT 坐标和预测坐标之间的平均欧式距离, CLE是最古老的指标，它不仅对数据集标注敏感，不会考虑跟踪失败，而且会忽略目标 BB 并导致重大错误。</p></li><li><p>Accuracy<br>在给定一个确定阈值的情况下, overlap score 决定了跟踪器在一帧中是否跟踪成功, 最终的 accuracy 由 average overlap score 计算得到, overlap score 其实就是 IoU:<br>$$<br>S=\frac{\left|b_{t} \cap b_{g}\right|}{\left|b_{t} \cup b_{g}\right|}<br>$$</p></li><li><p>Robustness/failure score<br>就是跟踪器在跟踪过程中跟丢的数目, 即目标需要被重新初始化的次数, 跟丢的定义为 IoU 降到 0</p></li><li><p>Expected average overlap (EAO)<br>可以看成是 Accuracy 和 Robustness 的结合, 对于给定的 $N_s$ 个 frames, EAO scores 为:<br>$$<br>\widehat{\Phi}<em>{N</em>{s}}=\left\langle\frac{1}{N_{s}} \sum_{i=1}^{N_{s}} \Phi_{i}\right\rangle<br>$$<br>其中 $\Phi_i$ 是跟踪器在每个帧的平均 IoU</p></li><li><p>Area under curve (AUC)<br>其定义为平均成功率(0 到 1)</p></li></ol><h2 id="性能图"><a href="#性能图" class="headerlink" title="性能图"></a>性能图</h2><ol><li><p>Precision plot<br>预测符合条件帧占所有帧的百分比, 符合条件是指预测的位置和真实位置之间有特定的阈值??? ( Given the CLEs per different thresholds, the precision plot shows the percentage of video frames in which the estimated locations have at most the specific threshold with the ground-truth locations)</p></li><li><p>Success plot<br>评估了那些预测的 bbox 和真实 bbox 之间的 IoU 大于确定阈值的帧占所有帧的百分比.</p></li><li><p>EAO Curve<br>对于单个长度的视频序列，预期的平均重叠曲线是由特定间隔 $[N_{lo}, N_{hi }]$ 得到:<br>$$<br>\widehat{\Phi}=\frac{1}{N_{h i}-N_{l o}} \sum_{N_{s}=N_{l o}}^{N_{h i}} \widehat{\Phi}<em>{N</em>{s}}<br>$$</p></li><li><p>One-pass evaluation with restart (OPER):<br>评估对目标重新进行初始化的准确率, 一次重新初始化是指目标跟踪失败一次.</p></li></ol><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>结果太多了, 可以去 <a href="https://github.com/MMarvasti/Deep-Learning-for-Visual-Tracking-Survey">Github</a> 看完整的实验结果, 这里贴最后结果出来看下.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/results_vot2018.png"></p><h1 id="总结和讨论"><a href="#总结和讨论" class="headerlink" title="总结和讨论"></a>总结和讨论</h1><p>最近的方法都是从以下几个方向来做的:</p><ol><li>利用数据增强, 对抗训练的生成网络来消除训练数据的不平衡分布</li><li>重新定义分类/回归的公式来进行更加高效的训练并且得到更加适合跟踪的特征</li><li>使用更宽, 更深的 backbone</li><li>通过使用其他信息(上例如下文, 时序信息)来提取互补特征</li></ol><p>具体来说:</p><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>消除训练数据中正负样本的不平衡分布</span></div>    <div class="hide-content"><ol><li>VITAL, DaSiamRPN, UPDT 都试图消除训练数据中正负样本的不平衡分布</li><li>VITAL 使用对抗训练来增强正样本, 减少负样本, 而且提供更加有判别性和鲁棒性的特征</li><li>DaSiamRPN 同时使用数据增强和和负语义样本 (negative semantic samples) 来考虑视觉干扰因素并提高视觉跟踪的鲁棒性</li><li>UPDT 使用标准的数据增强和一种质量评估(用于预测状态)来有效地融合浅层和深层特征</li></ol></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>提高 DL-based 方法的学习过程</span></div>    <div class="hide-content"><ol><li>UPDT, DeepSTRCF, DRT, LSART, and ASRCF 都修改了 DCF 的传统岭回归公式.</li><li>DaSiamRPN 使用 distractor-aware 的目标函数</li><li>VITAL 提出了一种 cost-sensitive loss 重新定义了 GAN 的目标函数</li><li>SiamDW, SiamRPN++, SiamMask 使用现有的 SOTA 网络来作为 backbone</li><li>SiamDW 提出新的残差模块和结构来防止显著地增加感受野并同时提高特征可判别性和 localization 精度</li><li>SiamRPN++的结构包括了不同的 layer-wise 和 depth-wise 特征聚合, 以此来填补 SNN-based 和 CNN-based 方法之间的性能差距.</li></ol></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>考虑时序信息</span></div>    <div class="hide-content"><ol><li>DAT 使用 reciprocative learning</li><li>DeepSTRCF 使用 online passive-aggressive (PA) learning</li><li>主要有 4 种学习方法:</li></ol><ul><li>similarity learning(i.e., SiamDW, SiamRPN++, C-RPN, StructSiam, SiamMask, DaSiamRPN)</li><li>multi-domain learning (i.e., MDNet, DAT)</li><li>adversarial learning (i.e., VITAL)</li><li>spatial-aware regressions learning (i.e., LSART)<br>再加上最后的 DCF learning</li></ul></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>使用简单的 Backbone</span></div>    <div class="hide-content"><p>个人感觉这是为了速度问题, 但这种简单的 backbone 不能得到 discriminative  的特征, 所以为了解决在形变严重及存在视觉干扰因素下的鲁棒性问题, 不同的方法提出了不同的解决方案:</p><ol><li>C-RPN 在 Siamese Network 中级联多个 RPN, 以此来解决 hard negative samples 问题</li><li>为了减少 SNN-based 方法对非刚性物体外观信息改变敏感及 POC 属性, StructSiam 检测 local pattern 的上下文及相互之间的关系, 且利用一个 Siamese Network 来进行实时匹配</li><li>DaSiamRPN 采用 local-to-global 的搜索策略和 NMS 来重新检测目标并减少可能的干扰因素, DaSiamRPN 能有效处理 FOC, OV, POC 和 BC 问题</li></ol></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>使用较大的 Backbone</span></div>    <div class="hide-content"><p>SiamMask, SiamDW-SiamRPN 和 SiamRPN++ 都使用 ResNet, 使用大的 Backbone 的好处在于能得到比较丰富的特征, 所以可以用于多个任务.</p><ol><li>SiamMask 用 three-branches 来预测旋转的 bbox, 一个用于预测 bbox, 一个用于预测角度, 还有一个用于预测 binary mask(盲猜是遮挡, 具体还是看论文吧)</li><li>SiamMask 无法处理的场景在于 MB 和 OV, 因为他们会产生错误的目标 mask</li><li>SiamDW-SiamRPN 和 SiamRPN++ 充分利用了大 backbone 的好处, 以此来减少模型对 MB 和 OV 的敏感性.</li></ol></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>MDNet 和基于它的方法们</span></div>    <div class="hide-content"><p>由于在大规模数据集上对这些网络进行了专门的 offline training 和 online training, 因此这些方法能够处理各种挑战, 几乎不会错过视觉目标, 所以有着比较令人满意的性能.</p><p>但这些方法的问题在于:</p><ol><li>计算复杂度高</li><li>有相同语义信息的目标之间存在类内问题</li><li>进行 scale estimate 时的空间是离散的</li></ol><p>VITAL 能够在 DEF, IPR 和 OPR 下表现良好是因为利用 cost-sensitive loss 集中对 hard negative samples 的处理, 但由于利用一个生成网络产生了固定尺寸的 mask, 所以无法很好地处理比较明显的 SV 情况.</p><p>LSART 利用改进的 Kernelized ridge 回归（KRR），通过逐块相似性的加权组合来专注于目标的可靠区域。考虑到旋转角度信息和 CNN 的在线适应能力, 该方法能有效作用于 DEF 和 IPR 情况.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>基于 DCF 的方法</span></div>    <div class="hide-content"><p>DeepSTRCF, ASRCF, DRT 和 UPDT 不仅可以利用现有的深层特征(就 CNN 学的特征), 还可以将它们与浅层(这里是指手工设计的特征, 例如 HOG, SIFT 这些)功能融合, 以提高视觉跟踪的鲁棒性.</p><ol><li>DeepSTRCF 提出一种时序正则化机制, 并将其添加到空间正则化的 DCF 公式中, 以减少 OOC 和 OV 的不利影响</li><li>使用 object-aware 的空间正则化项和可靠项, ASRCF 和 DRT 尝试优化模型以有效地学习更多自适应的相关滤波.</li><li>UPDT 主要用于增强目标跟踪的鲁棒性, 利用独立训练基于浅层特征的 DCF 和基于深度特征的 DCF 的自适应性融合来实现.</li></ol><p>尽管这些基于 DCF 方法与更复杂的跟踪器相比, 有差不多的性能, 但这些方法仍然存在对预训练模型的局限性(例如计算复杂度), 纵横比变换, 模型降级和外观信息变化巨大等问题.</p></div></div><p>下面是对近2年论文摘要的一个总结翻译, 最后会有个小总结, 目的是为了搞明白近2年在做什么, 在研究什么问题, 如何解决这些问题.</p><h1 id="CVPR’20"><a href="#CVPR’20" class="headerlink" title="CVPR’20"></a>CVPR’20</h1><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>A Unified Object Motion and Affinity Model for Online Multi-Object Tracking</span></div>    <div class="hide-content"><p>这篇文章主要解决的是速度问题, 把目标动作和相似性模型统一成一个框架, 主要想法是通过多任务学习来将单个目标跟踪和度量学习集成到一个统一的三元组网络.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/UMA_MOT16.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/UMA_MOT17.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Correlation-Guided Attention for Corner Detection Based Visual Tracking</span></div>    <div class="hide-content"><p>分析了基于角点的检测在跟踪中效果不好的原因, 并提出了一个 two-stage 的 correlation-guided attentional corner detection. <strong>直观理解就是在空间和通道维度上加了 attention 来让角点检测效果更好.</strong></p><p>第一个阶段使用 Siamese 网络得到的 RoI 来把目标从背景中区分开来<br>第二个阶段添加了 pixel-wise correlation-guided spatial attention module 和一个 channel-wise correlation-guided channel attention module 来为角点检测增强 RoI 的特征.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/CGACD_VOT18.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Deformable Siamese Attention Networks for Visual Object Tracking</span></div>    <div class="hide-content"><p>引入一种新的 Siamese attention 机制, 这种 attention 机制能够计算可变形的 self-attention 和 cross-attention.</p><p>作者认为 self-attention 能够通过空间注意力来学到很强的上下文信息, 并且利用 channel attention 来增强 channel-wise 特征.</p><p>cross-attention 能够聚合 template 和 search region 的上下文依赖关系, 从而隐式地自适应地更新目标 template.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/deformable_SAN_VOT.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>High-Performance Long-Term Tracking with Meta-Updater Best Paper Mention</span></div>    <div class="hide-content"><p>聚焦 long-term 的单目标跟踪</p><p>目前的 long-term 跟踪方法基本采用 offline training 的 Siamese Network, 所以不能从使用 online training 的 short-term 跟踪过程中受益, 但又不能用 online training 来解决 long-term 的问题, 因为 long-term 跟踪有很大的不确定性.</p><p>所以作者提出了一个 offline-trained 的更新器 <strong>Meta-Updater</strong>, 用这个东西来判断是否在当前帧更新 target template.</p><p>这个 Meta-Updater 可以按顺序有效集成几何信息, 判别信息和外观信息, 并输出一个二值输出, 判断是否更新, 然后用级联的 LSTM 来处理这些信息序列</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/LTMU_VOT18.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/LTMU_VOT19.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>How to Train Your Deep Multi-Object Tracker</span></div>    <div class="hide-content"><p>聚焦 short-term 的多目标跟踪</p><p>把评价指标放到损失函数里面的一个工作</p><p>目前的多目标跟踪方法的评价指标都是不可微的, 例如 MOTA 和 MOTP, 作者提出了一种可微分的 MOTA 和 MOTP 的代理来为 end-to-end 的跟踪得到一个合适的目标函数</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/deepmot_mot.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Learning a Neural Solver for Multiple Object Tracking</span></div>    <div class="hide-content"><p>聚焦 short-term 的多目标跟踪</p><p>用图来做多目标跟踪, 把图论中经典的网络流公式用于定义一个全部可微分的框架, 通过直接在图域上操作，可以在整个检测范围内进行全局推理并预测最佳解决方案.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/graph_solver_mot.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Probabilistic Regression for Visual Tracking</span></div>    <div class="hide-content"><p>聚焦 short-term 的单目标跟踪</p><p>目前的算法都需要预测一个独立状态的 confidence map, 但这个值缺乏一个明确的概率解释.</p><p>本文提出了一种概率回归方程并将其用于跟踪中, 对于给定的一张图像, 预测目标状态条件概率密度.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/PrDiMP_vot18.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking</span></div>    <div class="hide-content"><p>聚焦 short-term 的单目标跟踪</p><p>online training 能够在存在背景干扰的情况下提供比较高的辨别力, 但也有 2 个不足, 即无法保证良好的收敛性以及由于内存限制进行过多更新而导致的过拟合问题, 这篇文章主要解决的是这个问题.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/RLS-RTMDNet_VOT.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>RetinaTrack: Online Single Stage Joint Detection and Tracking</span></div>    <div class="hide-content"><p>想法是把检测和跟踪用 RetinaNet 合起来做??? 不太懂</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/retina_track_vot17.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>ROAM: Recurrently Optimizing Tracking Model</span></div>    <div class="hide-content"><p>聚焦 short-term 的单目标跟踪</p><p>简单地把 CenterNet 那套拿来了, 用一个 heat map 来预测物体的位置, 然后回归 bbox, 不同的是 offline train 了一个 recurrent neural optimizer 来在 meta-learning 的情况下更新模型</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/ROAM_VOT.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Siam R-CNN: Visual Tracking by Re-Detection</span></div>    <div class="hide-content"><p>利用重新检测的方式来解决遮挡问题</p><p>同时使用了第一帧的 template 和之前帧的预测来进行重新检测, 这样能够同时对背景和要被跟踪的目标进行建模. 这种方法能保证在长时间的遮挡之后仍然能被检测到.</p><p>还提出了一种 hard example mining 的方法提高对相似物体的鲁棒性</p><p>short-term 的结果<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/Siam_RCNN.png"></p><p>long-term 的结果<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/SiamRCNN_VOT18_longterm.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking</span></div>    <div class="hide-content"><p>把目标跟踪分成 2 个任务, 对每个像素进行分类以及对每个像素的 bbox 进行回归, 整体结构就 2 个子网络, 一个用于特征提取, 一个用于 bbox 预测</p><p>这个网络同时实现了 anchor free 和 proposal free.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/SiamCAR.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Siamese Box Adaptive Network for Visual Tracking</span></div>    <div class="hide-content"><p>short-term 的单目标跟踪</p><p>又是 CenterNet 那一套, 服了…直接用全卷积网络来分类和回归.直接下一篇.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/siamesebox_vot.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>SQE: a Self Quality Evaluation Metric for Parameters Optimization in Multi-Object Tracking</span></div>    <div class="hide-content"><p>提了个新的评价标准, 过.</p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Tracking by Instance Detection: A Meta-Learning Approach</span></div>    <div class="hide-content"><p>short-term 的单目标跟踪</p><p>通过适当的初始化，可以通过从单个图像中学习新 instance 来将检测器快速转换为跟踪器。</p><p>作者发现与模型无关的元学习提供了一种初始化, 使其能满足我们需求.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/MAML_VOT2018.png"></p></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>TubeTK: Adopting Tubes to Track Multi-Object in a One-Step Training Model</span></div>    <div class="hide-content"><p>short-term 的多目标跟踪</p><p>目前的 Tracking by Detection 其实是一个二阶段的东西, 先检测再跟踪, 但这种方法有一些不足, 所以作者提出使用 “bounding tube” 来表示物体的时空位置, 这样仅需仅需一次训练即可, 无需先训练检测再训练跟踪.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Visual-Tracking/tubetk_vot.png"></p></div></div>]]></content>
      
      
      <categories>
          
          <category> Tracking </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
            <tag> Tracking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to Adversarial Examples</title>
      <link href="2020/09/15/Introduction-To-Adversarial-Examples/"/>
      <url>2020/09/15/Introduction-To-Adversarial-Examples/</url>
      
        <content type="html"><![CDATA[<h1 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a>Basic knowledge</h1><h2 id="Different-types-of-adversarial-example-attacks"><a href="#Different-types-of-adversarial-example-attacks" class="headerlink" title="Different types of adversarial example attacks"></a>Different types of adversarial example attacks</h2><h3 id="Classified-by-attack-mode"><a href="#Classified-by-attack-mode" class="headerlink" title="Classified by attack mode"></a>Classified by attack mode</h3><ol><li>White-box<br>The attacker <strong>can get the algorithm used by machine learning and the parameters used by the algorithm</strong>. The attacker can interact with the machine learning system in the process of generating adversarial attack data.</li><li>Black-box<br>The attacker <strong>does not know the algorithms and parameters used in machine learning</strong>, but the attacker can still interact with the machine learning system. For example, he can pass in any input to observe the output and judge the output.</li></ol><p>In practical applications, the difference between the two is reflected in: Model A is used to generate adversarial examples, and then model B is attacked. <strong>When model A and model B are the same model, it is a white box attack; when model A and model B are not the same model, it is a black box attack.</strong></p><h3 id="Classified-by-attack-target"><a href="#Classified-by-attack-target" class="headerlink" title="Classified by attack target"></a>Classified by attack target</h3><ol><li><p>Non-Target<br>As long as the attack is successful, the generated adversarial examples <strong>can be predicted to any class.</strong></p></li><li><p>Target<br>Not only the attack is required to be successful, but also the generated <strong>adversarial examples belong to a specific class.</strong></p></li></ol><h3 id="Classified-by-attack-method"><a href="#Classified-by-attack-method" class="headerlink" title="Classified by attack method"></a>Classified by attack method</h3><ol><li>Individual<br>Generate adversarial examples by adding <code>different</code> perturbation for each input image.</li><li>Universal<br>Generate adversarial examples by adding <code>same</code> perturbation for each input image.</li></ol><h3 id="Classified-by-the-way-the-adversarial-examples-generated"><a href="#Classified-by-the-way-the-adversarial-examples-generated" class="headerlink" title="Classified by the way the adversarial examples generated"></a>Classified by the way the adversarial examples generated</h3><ol><li>Generation-based</li><li>Gradient-based</li><li>Optimization-based</li></ol><p>The distinction among these generation methods will be update in a week (￣▽￣)~* (maybe later…)</p><h2 id="Different-types-of-defense"><a href="#Different-types-of-defense" class="headerlink" title="Different types of defense"></a>Different types of defense</h2><ol><li>Adversarial training: Add adversarial examples to training dataset.</li><li>Gradient mask: Mask the gradient of model, thus attacker cannot get information about gradient.</li><li>Randomization: Apply random layer or random variables to original model, making the original model have randomness to better tolerate noise.</li><li>Denoising: Before the input model is judged, the current adversarial samples are denoised first, and the information that causes disturbances is eliminated.</li><li>Model compression: Can be regarded as a kind of Denoising method</li></ol><h2 id="Domains-in-Defense"><a href="#Domains-in-Defense" class="headerlink" title="Domains in Defense"></a>Domains in Defense</h2><ol><li>Verification(measure how robust a model is)</li></ol><ul><li>The smallest ball where no adversarial example exists</li><li>The radius of the ball</li></ul><ol start="2"><li>Detection</li></ol><ul><li>Binary classification</li></ul><ol start="3"><li>Robust defense</li></ol><ul><li>Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. <strong>ICML, 2018  Best Paper</strong></li><li>Adversarial training</li></ul><h1 id="Some-Attack-Methods"><a href="#Some-Attack-Methods" class="headerlink" title="Some Attack Methods"></a>Some Attack Methods</h1><h2 id="FGSD-Fast-gradient-sign-method"><a href="#FGSD-Fast-gradient-sign-method" class="headerlink" title="FGSD(Fast gradient sign method)"></a><a href="https://arxiv.org/pdf/1412.6572.pdf">FGSD(Fast gradient sign method)</a></h2><p>This is a gradient-based algorithm to generate adversarial examples, we need to maximize the loss function $J(x^*, y)$ to get adversarial example $x^*$, $J$ is the loss function, which be used to measure the classification error.(usually cross-entropy loss).</p><p>We want to maximize the value of $J$ because we want the adversarial example won’t be class $y$ any more. During this whole optimization process, we need to constrain $L_{\infty}$ with $\left|x^{<em>}-x\right|_{\infty} \leq \epsilon$, which means the error between input image and adversarial image must be within a certain and small range. So we can generate adversarial example $x^</em>$ by:<br>$$<br>\boldsymbol{x}^{*}=\boldsymbol{x}+\epsilon \cdot \operatorname{sign}\left(\nabla_{\boldsymbol{x}} J(\boldsymbol{x}, y)\right)<br>$$</p><h2 id="FGM-fast-gradient-method"><a href="#FGM-fast-gradient-method" class="headerlink" title="FGM(fast gradient method)"></a>FGM(fast gradient method)</h2><p>Improve FGSD to make sure that can satisfy the $L_2 \space norm$:  $L_{\infty}$ with $\left|x^{<em>}-x\right|_{2} \leq \epsilon$<br>$$\boldsymbol{x}^{</em>}=\boldsymbol{x}+\epsilon \cdot \frac{\nabla_{\boldsymbol{x}} J(\boldsymbol{x}, y)}{\left|\nabla_{\boldsymbol{x}} J(\boldsymbol{x}, y)\right|_{2}}$$</p><h2 id="IFGSD-Iterative-gradient-sign-Method"><a href="#IFGSD-Iterative-gradient-sign-Method" class="headerlink" title="IFGSD(Iterative gradient sign Method)"></a><a href="https://arxiv.org/pdf/1607.02533.pdf">IFGSD(Iterative gradient sign Method)</a></h2><p>Also be regarded as an extension of FGSD, specifically, it utilizes a small value $\alpha$ to update $x^*$ step by step, usually $\alpha = \epsilon/T$, $T$ is the iteration number.<br>$$\boldsymbol{x}<em>{0}^{*}=\boldsymbol{x}, \quad \boldsymbol{x}</em>{t+1}^{<em>}=\boldsymbol{x}_{t}^{</em>}+\alpha \cdot \operatorname{sign}\left(\nabla_{\boldsymbol{x}} J\left(\boldsymbol{x}_{t}^{*}, y\right)\right)$$</p><p>The results show that IFGSD is better than FGSD in white-box attacks.</p><h2 id="DeepFool"><a href="#DeepFool" class="headerlink" title="DeepFool"></a><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.html">DeepFool</a></h2><p>Consider a binary classification situation on the hyperplane(超平面)<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Adversarial-Examples/deepfool_binary_classification.png"></p><p>for an input $x_0$, if we want to change its label $f(x)&gt;0$ to the wrong label $f(x) &lt; 0$, the smallest perturbation we need is “move” $x_0$ to the decision border on the hyperplane.<br>$$\begin{aligned}<br>\boldsymbol{r}<em>{*}\left(\boldsymbol{x}</em>{0}\right) &amp;:=\arg \min |\boldsymbol{r}|<em>{2} \<br>&amp; \text { subject to } \operatorname{sign}\left(f\left(\boldsymbol{x}</em>{0}+\boldsymbol{r}\right)\right) \neq \operatorname{sign}\left(f\left(\boldsymbol{x}<em>{0}\right)\right) \<br>&amp;=-\frac{f\left(\boldsymbol{x}</em>{0}\right)}{|\boldsymbol{w}|_{2}^{2}} \boldsymbol{w}<br>\end{aligned}$$</p><p>Multi-classification problems are similar solutions:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Adversarial-Examples/deepfool_multi_cls.png"></p><h2 id="C-amp-W-Carlini-amp-Wagner"><a href="#C-amp-W-Carlini-amp-Wagner" class="headerlink" title="C&amp;W(Carlini &amp; Wagner)"></a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7958570">C&amp;W(Carlini &amp; Wagner)</a></h2><p>C&amp;W is similar to IFGSD, they are the same iterative attack algorithm, searching the smallest perturbation $r_n$ with the help of a variable $w_n$:<br>$$r_{n}=\frac{1}{2}\left(\tanh \left(\omega_{n}+1\right)\right)-X_{n}$$</p><p>The loss function based $w_n$ can be shown as:<br>$$\min <em>{\omega</em>{n}}\left|\frac{1}{2}\left(\tanh \left(\omega_{n}\right)+1\right)-X_{n}\right|+c \cdot f\left(\frac{1}{2}\left(\tanh \left(\omega_{n}\right)+1\right)\right)$$</p><p>where<br>$$f\left(x^{\prime}\right)=\max \left(\max \left{Z\left(x^{\prime}\right)<em>{i}: i \neq t\right}-Z\left(x^{\prime}\right)</em>{t},-\kappa\right)$$</p><p>$Z(x)_i$ is the Rockyster output of class $i$, we can control the confidence with which the misclassification occurs by adjusting $k$, the parameter $k$ encourages the solver to find an adversarial example $x’$ that will be classified as class $t$(not the true class) with high confidence.(通过调整 $k$ 来控制分类错误发生的置信度, $k$ 越大说明将对抗样本 $x’$ 分为错误类别 $t$ 的可能性越大).</p><h2 id="Optimization-based-method"><a href="#Optimization-based-method" class="headerlink" title="Optimization based method"></a><a href="https://arxiv.org/pdf/1312.6199.pdf">Optimization based method</a></h2><p>This kind of methods is aimed to minimize the distance between adversarial examples and input images on the premise that the attack can be achieved (make the classifier misclassify the adversarial sample)<br>$$\underset{\boldsymbol{x}^{<em>}}{\arg \min } \lambda \cdot\left|\boldsymbol{x}^{</em>}-\boldsymbol{x}\right|_{p}-J\left(\boldsymbol{x}^{*}, y\right)$$</p><h1 id="Some-Defend-Methods"><a href="#Some-Defend-Methods" class="headerlink" title="Some Defend Methods"></a>Some Defend Methods</h1><h2 id="Adversarial-Training"><a href="#Adversarial-Training" class="headerlink" title="Adversarial Training"></a>Adversarial Training</h2><ol><li>Turning adversarial examples into training dataset to train a robuster model.</li><li>Regularize model with the prior knowledge of attack.</li></ol><p>As an example for method 2, if we know that attacker would use FGSM to attack model, we can add a regularization to the loss function to achieve adversarial training:<br>$$\tilde{J}(\boldsymbol{\theta}, \boldsymbol{x}, y)=\alpha J(\boldsymbol{\theta}, \boldsymbol{x}, y)+(1-\alpha) J\left(\boldsymbol{\theta}, \boldsymbol{x}+\epsilon \operatorname{sign}\left(\nabla_{\boldsymbol{x}} J(\boldsymbol{\theta}, \boldsymbol{x}, y)\right)\right.$$</p><h2 id="Defensive-Knowledge-Distillation"><a href="#Defensive-Knowledge-Distillation" class="headerlink" title="Defensive Knowledge Distillation"></a>Defensive Knowledge Distillation</h2><p>There is a teacher model and a student model in knowledge distillation. We want to transfer the teacher model’s(big model) knowledge to the student model(small model).</p><p>In defensive knowledge distillation, first train a model with input images $x$ and labels $y$, thus we can get a probability distribution $F(x)$, then replace $y$ with $F(x)$ to train a new model but have same architecture and same temperature. Then classify samples with the whole model.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Adversarial-Examples/defensive_kd.png"></p><h2 id="Denoise"><a href="#Denoise" class="headerlink" title="Denoise"></a>Denoise</h2><p>The idea of denoising is to make the adversarial example s closer to the original sample, which is equivalent to removing the noise added to generate the adversarial sample and restoring it to the original sample as much as possible, so that the adversarial sample can be accurately classified.</p><p>There are two different architectures are designed for denoising, Denoising AutoEncoder(DAE) and Denoising Additive U-Net(DUNET)<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Adversarial-Examples/dae_and_dunet.png"></p><p>Firstly propose a kind of denoiser called Pixel Guided Denoiser(PSG), the loss function of PGD is: $L=|x-\hat{x}|$, $\hat{x}$ is the denoised image. However, the denoised images purified by PGD shows lower accuracy compare to the raw adversarial examples.</p><p>Due to the PGD cannot competely eliminate the perturbations, the remained perturbations will be enlarged with the increase of layers, finally forcing network make the wrong decision.</p><p>So another denoiser named High-level representation Guided Denoiser(HGD) is proposed, the key idea of HGD is replace  the difference between pixels with the difference between high-level representation in a pre-trained CNN. Thus the loss function is changed into: $L=\left|f_{l}(\hat{x})-f_{l}(x)\right|$, $f_l(\hat{x})$ means the $l_{th}$ feature in the pre-trained model.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Adversarial-Examples/3_different_training_mathods_for_HGD.png"></p>]]></content>
      
      
      <categories>
          
          <category> Adversarial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Adversarial </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper List About Adversarial Examples</title>
      <link href="2020/09/15/Adversarial-Examples/"/>
      <url>2020/09/15/Adversarial-Examples/</url>
      
        <content type="html"><![CDATA[<h1 id="Paper-List-and-Abstract"><a href="#Paper-List-and-Abstract" class="headerlink" title="Paper List and Abstract"></a>Paper List and Abstract</h1><p>You can click on each paper title to access the detailed information of the paper.</p><h2 id="CVPR"><a href="#CVPR" class="headerlink" title="CVPR"></a>CVPR</h2><h3 id="2019-26-papers"><a href="#2019-26-papers" class="headerlink" title="2019(26 papers)"></a>2019(26 papers)</h3><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Taghanaki_A_Kernelized_Manifold_Mapping_to_Diminish_the_Effect_of_Adversarial_CVPR_2019_paper.html"><strong><code>A Kernelized Manifold Mapping to Diminish the Effect of Adversarial Perturbations</code></strong></a></p><p>Propose a non-linear radial basis convolutional feature mapping by <strong>learning a Mahalanobis-like distance function</strong>, then maps the convolutional features onto a linearly well-separated manifold, which prevents small adversarial perturbations from forcing a sample to cross the decision boundary.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_Adversarial_Attacks_Beyond_the_Image_Space_CVPR_2019_paper.html"><strong><code>Adversarial Attacks Beyond the Image Space</code></strong></a></p><p>In the contexts of classification and visual question answering, we augment CNNs receive 2D input images with a rendering module (either differentiable or not) in front, so that  a 3D scene (in physical space) is rendered to a 2D image (in the image space), and then mapped to a prediction (in the output space).</p><p><strong>This paper demonstrates that the adversarial perturbations can now go beyond the image space and have clear meanings in the 3D physical world</strong>. Though the image-space adversaries can be interpreted as per-pixel albedo change, we verify that they cannot be well explained along these physically meaningful dimensions, which often have a non-local effect. But it is still possible to successfully attack beyond the image space on the physical space, thought it is more difficult than image-space attack, reflected in lower success rate and heavier perturbations required.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Adversarial_Defense_by_Stratified_Convolutional_Sparse_Coding_CVPR_2019_paper.html"><strong><code>Adversarial Defense by Stratified Convolutional Sparse Coding</code></strong></a></p><p>Based on convolutional sparse coding, this paper <strong>constructs a stratified low-dimensional quasi-natural image space</strong>, the quasi-natural space can not only faithfully approximates the nature image space but also removing adversarial perturbations. The method shows SOTA performance compared to other  attack-agnostic adversarial defense methods.</p><p>Introduce a novel Sparse Transformation Layer (STL) between the input image and the first layer of the neural network to efficiently project images into our quasi-natural image space.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Qiu_Adversarial_Defense_Through_Network_Profiling_Based_Path_Extraction_CVPR_2019_paper.html"><strong><code>Adversarial Defense Through Network Profiling Based Path Extraction</code></strong></a></p><p>Recent work has shown the effectiveness of decomposing DNN models to decomposed functional blocks for defending adversarial attacks.</p><p>This work <strong>proposes a profiling-based method to decompose the DNN models to different functional blocks</strong>, which lead to the effective path as a new approach to explore DNNs’ internal organizations.</p><p>The per-image effective path can be aggregated to the class-level effective path, through which we observe that adversarial images activate effective path different from normal images. So proposing an effective path similarity-based method to detect adversarial images with an interpretable model, which achieve better accuracy and broader applicability.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Ho_Catastrophic_Childs_Play_Easy_to_Perform_Hard_to_Defend_Adversarial_CVPR_2019_paper.html"><strong><code>Catastrophic Child’s Play: Easy to Perform, Hard to Defend Adversarial Attacks</code></strong></a></p><p><strong>This paper discusses the perceptibility of attack perturbations and confirm the hypothesis that none of the existing defenses is found effective against the attacks.</strong></p><p>It is argued that perceptibility depends on context, and a distinction is made between imperceptible and semantically imperceptible perturbations. While the former survives in image comparisons, the latter are perceptible but have no impact on human object recognition.</p><p>A procedure is proposed to determine the perceptibility of perturbations using Turk experiments, and a dataset of both perturbation classes which enables replicable studies of object manipulation attacks, is assembled.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Jia_ComDefend_An_Efficient_Image_Compression_Model_to_Defend_Adversarial_Examples_CVPR_2019_paper.html"><strong><code>ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples</code></strong></a></p><p>This paper <strong>proposes an end-to-end image compression model to defend adversarial examples</strong>. <strong>The proposed model consists of a compression module (ComCNN) and a reconstruction module (RecCNN)</strong>. The ComCNN is used to maintain the structure information of the original image and purity adversarial perturbations. The RecCNN is used to reconstruct the original image with high quality.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Curls__Whey_Boosting_Black-Box_Adversarial_Attacks_CVPR_2019_paper.html"><strong><code>Curls &amp; Whey: Boosting Black-Box Adversarial Attacks</code></strong></a></p><p>Existing black-box iterative attacks have two defects:</p><ol><li>These attacks add noises monotonically along the direction of gradient ascent, resulting in a lack of diversity and adaptability of the generated iterative trajectories.</li><li>It is trivial to perform adversarial attack by adding excessive noises, but currently there is no refinement mechanism to squeeze redundant noises.</li></ol><p>So this paper <strong>proposes Curl &amp; Whey black-box attack to fix the above two defects.</strong></p><p>During Curl iteration, by combining gradient descent and ascent, we ‘curl’ up iterative trajectories to integrate more diversity and transferability into adversarial examples. Curls iteration also alleviates the diminishing marginal effect in existing iterative attacks.</p><p>The Whey optimization further squeezes the ‘whey’ of noises by exploiting the robustness of adversarial perturbation</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Rony_Decoupling_Direction_and_Norm_for_Efficient_Gradient-Based_L2_Adversarial_Attacks_CVPR_2019_paper.html"><strong><code>Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses</code></strong></a></p><p>Considering L2 norm distortions, the Carlini and Wagner attack is presently the most effective white-box attack in the literature. However, this method is slow since it performs a line-search for one of the optimization terms, and often requires thousands of iterations.</p><p>In this paper, <strong>an <code>efficient</code> approach is proposed to generate gradient-based attacks that induce misclassifications with low L2 norm</strong>, by decoupling the direction and the norm of the adversarial perturbation that is added to the image.Models trained with our attack achieve state-of-the-art robustness against white box gradient-based L2 attacks.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Taran_Defending_Against_Adversarial_Attacks_by_Randomized_Diversification_CVPR_2019_paper.html"><strong><code>Defending against adversarial attacks by randomized diversification</code></strong></a></p><p>We propose a randomized diversification as a defense strategy. <em>This strategy is used to gray-box scenario(situation), which assumes that the architecture of the classifier and the training data set are known to the attacker. What’s more, attacker have access to the internal system and to a secret key at the test time.</em>  <code>That&#39;s totally bullshit! If attacker can get all of this, you should call police right now rather than training a robust model......</code></p><p><strong>The defender processes an input in multiple channels.Each channel introduces its own randomization in a special transform domain based on a secret key shared between the training and testing stages.</strong></p><p>Such a transform based randomization with a shared key preserves the gradients in key-defined sub-spaces for the defender but it prevents gradient back propagation and the creation of various bypass systems for the attacker. An additional benefit of multi-channel randomization is the aggregation that fuses soft-outputs from all channels, thus increasing the reliability of the final score. The sharing of a secret key creates an information advantage to the defender.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Dubey_Defense_Against_Adversarial_Images_Using_Web-Scale_Nearest-Neighbor_Search_CVPR_2019_paper.html"><strong><code>Defense Against Adversarial Images using Web-Scale Nearest-Neighbor Search</code></strong></a></p><p>We hypothesize that adversarial perturbations fool the DNNs by moving the image away from the image manifold, because there exist no physical process that could have produced the adversarial image. This hypothesis suggest that a good defense mechanism should aim to project the adversarial images back onto the image manifold.</p><p>This paper <strong>study such defense mechanism, which approximate the projection onto the unknown image manifold by a nearest neighbor search against a web-scale image database containing tens of billions of images</strong>.(通过对含数百亿个图像的数据库进行最邻近搜索来预测未知图像的 image manifold)</p><p>It is very effective in attack settings in which the adversary does not have access to the image database. This paper propose two attack methods to break nearest neighbor defenses, and demonstrate conditions under which nearest-neighbor defense fails. With a series of experiments, there a tradeoff between robustness and accuracy in our defense.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Detection_Based_Defense_Against_Adversarial_Examples_From_the_Steganalysis_Point_CVPR_2019_paper.html"><strong><code>Detection based Defense against Adversarial Examples from the Steganalysis Point of View</code></strong></a></p><p><strong>This paper point out that steganalysis  can be applied to adversarial examples detection, and propose a method to enhance steganalysis features by estimating the probability of modifications</strong> caused by adversity attacks. Due to this method is not based on  DNN but based on high-dimensional manual-crafted feature, secondary attacks are hard to be directly performed to the model.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Stutz_Disentangling_Adversarial_Robustness_and_Generalization_CVPR_2019_paper.html"><strong><code>Disentangling Adversarial Robustness and Generalization</code></strong></a></p><p><strong>Study the relationship between model’s robustness and generalization on an underline, low-dimensional data manifold</strong>, results shows that:</p><ol><li>regular adversarial examples leave the manifold</li><li>adversarial examples constrained to the manifold (i.e. on-manifold adversarial examples) exist</li><li>on–manifold adversarial examples are generalization errors, and on-manifold adversarial training boosts generalization</li><li>regular robustness and generalization are not necessarily contradicting goals</li></ol><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Efficient_Decision-Based_Black-Box_Adversarial_Attacks_on_Face_Recognition_CVPR_2019_paper.html"><strong><code>Efficient Decision-based Black-box Adversarial Attacks on Face Recognition --Jun Zhu</code></strong></a></p><p>Evaluate the robustness of SOTA face recognition models in the decision-based black-box attack setting. (the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model).</p><p>To improve the efficiency of previous methods, <strong>we propose an evolutionary attack algorithm, which can model the local geometry of the search directions and reduce the dimension of the search space.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Evading_Defenses_to_Transferable_Adversarial_Examples_by_Translation-Invariant_Attacks_CVPR_2019_paper.html"><strong><code>Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks --Jun Zhu</code></strong></a></p><p>We <strong>propose a translation-invariant attack method to generate more transferable adversarial examples</strong> against the defense models.</p><p>By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability.</p><p>To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Exact_Adversarial_Attack_to_Image_Captioning_via_Structured_Output_Learning_CVPR_2019_paper.html"><strong><code>Exact Adversarial Attack to Image Captioning via Structured Output Learning with Latent Variables</code></strong></a></p><p><strong>Study the robustness of a CNN+RNN based image captioning system</strong>, specifically, propose to fool an image captioning system to generate some targeted partial captions for an adversarial example.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Feature_Denoising_for_Improving_Adversarial_Robustness_CVPR_2019_paper.html"><strong><code>Feature Denoising for Improving Adversarial Robustness --Kaiming He</code></strong></a></p><p>Why model cannot correctly recognize the image is that adversarial perturbations lead to noise in the feature constructed by the networks, so denoising the noise in feature is crucial to increase adversarial robustness.</p><p><strong>This paper propose new architectures to denoising the noises in feature.</strong> Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end.</p><p>The proposed feature denoising networks can improve the SOTA robustness in both white-box and black-box attack settings combined with adversarial training.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Feature_Distillation_DNN-Oriented_JPEG_Compression_Against_Adversarial_Examples_CVPR_2019_paper.html"><strong><code>Feature Distillation DNN-Oriented JPEG Compression Against Adversarial Examples</code></strong></a></p><p>Prior compression-based works mainly rely on directly tuning parameters like compression rate, to blindly reduce image features, thereby lacking guarantee on both efficiency and accuracy of benign images after applying defense methods.</p><p><strong>We propose a JPEG-based defensive compression framework to effectively rectify adversarial examples without impacting classification accuracy on benign data.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.html"><strong><code>Feature Space Perturbations Yield More Transferable Adversarial Examples</code></strong></a></p><p>This work <strong>describes a transfer-based black-box targeted attack of deep deep feature space representations</strong>, the attack also provides insights into cross-model class representations of deep CNNs.</p><p>The attack is explicitly designed for transferability, driving feature space representation of a source image at layer $L$ towards the representation of a target image at layer $L$.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Improving_Transferability_of_Adversarial_Examples_With_Input_Diversity_CVPR_2019_paper.html"><strong><code>Improving Transferability of Adversarial Examples with Input Diversity</code></strong></a></p><p>Most of the existing attacks only achieve low success rates under the black-box setting. To this end, we <strong>propose to improve the transferability of adversarial examples by creating diverse input patterns.</strong></p><p>Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Xiao_MeshAdv_Adversarial_Meshes_for_Visual_Recognition_CVPR_2019_paper.html"><strong><code>MeshAdv: Adversarial Meshes for Visual Recognition --Jia Deng</code></strong></a></p><p>Currently, the majority of these studies have focused on perturbation added to image pixels, while such manipulation is not physically realistic. So this work propose to <strong>add adversarial shape/texture on 3D object to achieve attack.</strong></p><p><strong>This work proposes meshAdv to generate “adversarial 3D meshes” from objects that have rich shape features but minimal textural variation.</strong></p><p>To manipulate the shape or texture of the objects, we make use of a differentiable renderer to compute accurate shading on the shape and propagate the gradient.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/He_Parametric_Noise_Injection_Trainable_Randomness_to_Improve_Deep_Neural_Network_CVPR_2019_paper.html"><strong><code>Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial Attack</code></strong></a></p><p>Training the network with Gaussian noise is an effective technique to perform model regularization, thus improving model robustness against input variation. Inspired by this idea, we explore to <strong>utilize the regularization characteristic of noise injection to improve DNN’s  robustness against adversarial attack.</strong></p><p>we propose ParametricNoise-Injection (PNI) which involves trainable Gaussian noise injection at each layer on either activation or weights through solving the Min-Max optimization problem, embedded with adversarial training. These parameters are trained explicitly to achieve improved robustness.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Retrieval-Augmented_Convolutional_Neural_Networks_Against_Adversarial_Examples_CVPR_2019_paper.html"><strong><code>Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples</code></strong></a></p><p><strong>Propose a retrieval-augmented convolutional network (RaCNN) and propose to train it with local mixup, using them to deal with <code>off-manifold</code> and <code>on-manifold</code> adversarial examples respectively</strong>, local mixup is a novel variant of the recently proposed mixup algorithm.</p><p>The proposed hybrid architecture combining a convolutional network and an off-the-shelf retrieval engine was designed to mitigate the adverse effect of <code>off-manifold</code> adversarial examples.</p><p>The proposed local mixup addresses <code>on-manifold</code> ones by explicitly encouraging the classifier to locally behave linearly on the data manifold.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Wicker_Robustness_of_3D_Deep_Learning_in_an_Adversarial_Setting_CVPR_2019_paper.html"><strong><code>Robustness of 3D Deep Learning in an Adversarial Setting</code></strong></a></p><p>Another work about the robustness of 3D deep learning…(3rd in CVPR 2019)</p><p><strong>we develop an algorithm for analysis of point-wise robustness of neural networks that operate on 3D data.</strong> We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness.</p><p>Then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Theagarajan_ShieldNets_Defending_Against_Adversarial_Attacks_Using_Probabilistic_Adversarial_Robustness_CVPR_2019_paper.html"><strong><code>ShieldNets: Defending Against Adversarial Attacks Using Probabilistic Adversarial Robustness</code></strong></a></p><p>Probabilistic adversarial robustness (PAR), as a theoretical framework, is introduced to neutralize adversarial attacks by concentrating sample probability to adversarial-free zones.</p><p><strong>Unlike the most of the existing defense mechanisms that require modifying the architecture/training of the target classifier.PAR is designed in the first place to provide proactive protection to an existing fixed model.</strong></p><p>ShieldNet is implemented as a demonstration of PAR in this work by using PixelCNN.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Yao_Trust_Region_Based_Adversarial_Attack_on_Neural_Networks_CVPR_2019_paper.html"><strong><code>Trust Region Based Adversarial Attack on Neural Networks</code></strong></a></p><p>Current state-of-the-art adversarial attack methods typically require very time consuming hyper-parameter tuning, or require many iterations to solve an optimization based adversarial attack.</p><p>To address this problem, <strong>we present a new family of trust region based adversarial attacks, with the goal of computing adversarial perturbations efficiently. We propose several attacks based on variants of the trust region optimization method.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Corneanu_What_Does_It_Mean_to_Learn_in_Deep_Networks_And_CVPR_2019_paper.html"><strong><code>What does it mean to learn in deep networks? And, how does one detect adversarial attacks?</code></strong></a></p><p><strong>we derive a novel approach to define what it means to learn in deep networks, and how to use this knowledge to detect adversarial attacks.</strong></p><p>Study how to define the ability of a network to generalize to unseen testing samples and, most importantly, why this is the case.</p><h3 id="2020-34-papers"><a href="#2020-34-papers" class="headerlink" title="2020(34 papers)"></a>2020(34 papers)</h3><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Naseer_A_Self-supervised_Approach_for_Adversarial_Robustness_CVPR_2020_paper.html"><strong><code>A Self-supervised Approach for Adversarial Robustness</code></strong></a></p><p>Transferability of adversarial examples demand generalizable defenses that can provide cross-task protection, but adversarial training lack such generalization because adversarial training enhances robustness by only modifying target model’s parameters. On the of hand, different input processing based defenses cannot defend continuously evolving attacks.</p><p><strong>We take the first step to combine the benefits of both approaches and propose a self-supervised adversarial training mechanism in the input space.</strong> our defense is a generalizable approach and provides significant robustness against the unseen adversarial attacks.</p><p>The results show that  our defense is a generalizable approach and provides significant robustness against the unseen adversarial attacks.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Gowal_Achieving_Robustness_in_the_Wild_via_Adversarial_Mixing_With_Disentangled_CVPR_2020_paper.html"><strong><code>Achieving Robustness in the Wild via Adversarial Mixing with Disentangled Representations --DeepMind</code></strong></a></p><p>Adversarial training’s application has been limited to enforcing invariance to analytically defined transformations like $ℓ_p$ p-norm bounded perturbations. Such perturbations do not necessarily cover plausible real-world variations that preserve the semantics of the input.(对抗训练的应用场景仅限于对定义好的变换实施不变性, 但这样的扰动变换不一定必须包括真实世界中能保留语义的变换, 例如化妆或者改变光照条件之后的图片不应该被视为对抗样本.)</p><p><strong>In this paper, we propose a novel approach to express and formalize robustness to these kinds of real-world transformations of the input.</strong>(我们提出了一种方法用于表达和形式化这些真实世界中存在的变换的鲁棒性)</p><p>The two key ideas underlying our formulation are (1) leveraging disentangled representations of the input to define different factors of variations(利用输入的解耦表示来定义变换的不同因素), and (2) generating new input images by adversarially composing the representations of different images(通过对抗地组合不同图像的表示来生成新的输入图像)</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Duan_Adversarial_Camouflage_Hiding_Physical-World_Attacks_With_Natural_Styles_CVPR_2020_paper.html"><strong><code>Adversarial Camouflage: Hiding Physical-World Attacks with Natural Styles</code></strong></a></p><p>Existing works have mostly focused on either digital adversarial examples created via small and imperceptible perturbations, or physical-world adversarial examples created with large and less realistic distortions that are easily identified by human observers.</p><p><strong>We propose a novel approach to craft and camouflage physical world adversarial examples into natural styles that appear legitimate to human observers. AdvCam transfers large adversarial perturbations into customized styles, which are then “hidden” on-target object or off-target background.</strong></p><p>The results show that in both digital and physical-world scenarios, adversarial examples crafted by AdvCam are well camouflaged and highly stealthy, while remaining effective in fooling state-of-the-art DNN image classifiers.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Xie_Adversarial_Examples_Improve_Image_Recognition_CVPR_2020_paper.html"><strong><code>Adversarial Examples Improve Image Recognition</code></strong></a></p><p>Here we present an opposite perspective: <strong>adversarial examples can be used to improve image recognition models if harnessed in the right manner.</strong></p><p>We propose AdvProp, an enhanced adversarial training scheme which <strong>treats adversarial examples as additional examples, to prevent overfitting to improve performance in image recognition.</strong></p><p><strong>Key to our method is the usage of a separate auxiliary(辅助的) batch norm for adversarial examples, as they have different underlying distributions to normal examples.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Adversarial_Robustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_paper.html"><strong><code>Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning</code></strong></a></p><p>Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy, but <strong>gaining robustness from pretraining is left unexplored.</strong></p><p><strong>We introduce adversarial training into selfsupervision, to provide general-purpose robust pretrained models for the first time.</strong></p><p>We find these robust pretrained models can benefit the subsequent(随后的) fine-tuning in two ways: (1) boosting final model robustness. (2) saving the computation cost, if proceeding towards adversarial fine-tuning.</p><p><strong>We find that different self-supervised pretrained models have diverse adversarial vulnerability.</strong> It inspires us to ensemble several pretraining tasks, which boosts robustness more.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Lee_Adversarial_Vertex_Mixup_Toward_Better_Adversarially_Robust_Generalization_CVPR_2020_paper.html"><strong><code>Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization</code></strong></a></p><p>There is a large gap exists between test accuracy and training accuracy in adversarial training. <strong>we identify Adversarial Feature Overfitting (AFO), which may cause poor adversarially robust generalization</strong>, and <strong>we show that adversarial training can overshoot(超过) the optimal point in terms of robust generalization, leading to AFO in our simple Gaussian model.</strong></p><p><strong>We present soft labeling as a solution to the AFO problem.</strong> Furthermore, <strong>we propose Adversarial Vertex mixup (AVmixup), a soft-labeled data augmentation approach for improving adversarially robust generalization.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Dong_Benchmarking_Adversarial_Robustness_on_Image_Classification_CVPR_2020_paper.html"><strong><code>Benchmarking Adversarial Robustness on Image Classification --Jun Zhu</code></strong></a></p><p>In this paper, <strong>we establish a comprehensive, rigorous, and coherent <code>benchmark</code> to evaluate adversarial robustness on image classification tasks.</strong></p><p>We perform large-scale experiments with two robustness curves as the fair-minded evaluation criteria to fully understand the performance ofthese methods(我们以两条稳健性曲线作为公正的评估标准进行大规模实验，以充分了解这些方法的性能)</p><p>we draw several important findings that can provide insights for future research, including:</p><ol><li><strong>The relative robustness between models can change across different attack configurations, thus it is encouraged to adopt the robustness curves to evaluate adversarial robustness.</strong></li><li><strong>As one of the most effective defense techniques, adversarial training can generalize across different threat models.</strong></li><li><strong>Randomization-based defenses are more robust to query-based black-box attacks.</strong></li></ol><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Wu_Boosting_the_Transferability_of_Adversarial_Samples_via_Attention_CVPR_2020_paper.html"><strong><code>Boosting the Transferability of Adversarial Samples via Attention</code></strong></a></p><p>This paper aims to improve the transferability of adversarial examples. <strong>The synthesized adversarial samples often achieve limited success due to overfitting to the local model employed.</strong></p><p><strong>We propose a novel mechanism to alleviate(缓和) the overfitting issue. It computes model attention over extracted features to regularize the search of adversarial examples(通过提取到的特征来计算模型的attention, 以此来调整对抗样本的生成), which prioritizes the corruption of critical features that are likely to be adopted by diverse architectures(这种做法保证了关键特征中的对抗信息能够被不同的网络结构所适应.)</strong></p><p>It can promote the transferability of resultant adversarial instances, its superiority to state-of-the-art benchmarks in both white-box and black-box settings.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Shamsabadi_ColorFool_Semantic_Adversarial_Colorization_CVPR_2020_paper.html"><strong><code>ColorFool: Semantic Adversarial Colorization</code></strong></a></p><p>Adversarial attacks that generate small Lp-norm perturbations to mislead classifiers have limited success in black-box settings and with unseen classifiers. Instead, adversarial attacks that generate unrestricted perturbations are more robust to defenses, are generally more successful in black-box settings and are more transferable to unseen classifiers. However, unrestricted perturbations may be noticeable to humans.</p><p><strong>we propose a content-based black-box adversarial attack that generates unrestricted perturbations by exploiting image semantics to selectively modify colors within chosen ranges that are perceived as natural by humans.</strong>(可以理解为生成一个人类看起来没有语义问题, 但实际上不受限制的扰动, 这里的限制指类似 $l_2$ 范数这种)</p><p>The proposed ColorFool, outperforms in terms of success rate, robustness to defense frameworks and transferability, five state-of-the-art adversarial attacks.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_DaST_Data-Free_Substitute_Training_for_Adversarial_Attacks_CVPR_2020_paper.html"><strong><code>DaST: Data-Free Substitute Training for Adversarial Attacks</code></strong></a></p><p>For the black-box setting, current substitute attacks need pre-trained (classification) models to generate adversarial examples. <strong>This paper propose a data-free substitute training method (DaST) to obtain substitute models</strong> for adversarial black-box attacks <strong>without the requirement of any real data.</strong></p><p>DaST utilizes specially designed generative adversarial networks (GANs) to train the substitute models. In particular, <strong>we design a multi-branch architecture and label-control loss for the generative model to deal with the uneven distribution of synthetic samples. The substitute model is then trained by the synthetic samples generated by the generative model, which are labeled by the attacked model subsequently</strong></p><p><strong>We are the first to train a substitute model for adversarial attacks without any real data.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/He_Defending_and_Harnessing_the_Bit-Flip_Based_Adversarial_Weight_Attack_CVPR_2020_paper.html"><strong><code>Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack</code></strong></a></p><p>Bit-Flip Attack (BFA). BFA has shown extraordinary attacking ability, where the adversary can malfunction a quantized Deep Neural Network (DNN) as a random guess, through malicious bit-flips on a small set of vulnerable weight bits.However, <strong>there are no effective defensive methods to enhance the fault-tolerance capability of DNN against such BFA.</strong></p><p><strong>We conduct comprehensive investigations on BFA and propose to leverage binarizationaware training and its relaxation – piece-wise clustering as simple and effective countermeasures to BFA.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Cohen_Detecting_Adversarial_Samples_Using_Influence_Functions_and_Nearest_Neighbors_CVPR_2020_paper.html"><strong><code>Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors</code></strong></a></p><p><strong>We present a method for detecting adversarial attacks, which is suitable for any pre-trained neural network classifier</strong></p><p>We <strong>use influence functions to measure the impact of every training sample on the validation set data</strong>. From the influence scores, <strong>we find the most supportive training samples for any given validation example.</strong></p><p><strong>A k-nearest neighbor (k-NN) model fitted on the DNN’s activation layers is  employed to search for the ranking of these supporting training samples.</strong></p><p>We observe that <strong>these samples are highly correlated with the nearest neighbors of the normal inputs, while this correlation is much weaker for adversarial inputs.</strong></p><p>We train an adversarial detector using the k-NN ranks and distances and show that it successfully distinguishes adversarial examples, getting state-of-the-art results.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zheng_Efficient_Adversarial_Training_With_Transferable_Adversarial_Examples_CVPR_2020_paper.html"><strong><code>Efficient Adversarial Training with Transferable Adversarial Examples</code></strong></a></p><p>This paper shows that  <strong>there is high transferability between models from neighboring epochs in the same training process.</strong>(i.e. adversarial examples from one epoch continue to be adversarial in subsequent epochs)</p><p>we propose a novel method, Adversarial Training with Transferable Adversarial Examples (ATTA), that <strong>can enhance the robustness of trained models and greatly improve the training efficiency <code>by accumulating adversarial perturbations through epochs</code>.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Enhancing_Intrinsic_Adversarial_Robustness_via_Feature_Pyramid_Decoder_CVPR_2020_paper.html"><strong><code>Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder</code></strong></a></p><p>In this paper, we propose an attack-agnostic defence framework to enhance the intrinsic robustness of neural networks, without jeopardizing(危害) the ability of generalizing clean samples.</p><p>This feature pyramid decoder(FPD) framework <strong>applies to all block-based CNNs.</strong></p><p><strong>It implants denoising and image restoration modules into a targeted CNN, and it also constraints the Lipschitz constant of the classification layer</strong></p><p>Moreover, we propose a two-phase strategy to train the FPD-enhanced CNN, utilizing $\epsilon$ neighborhood noisy images with multi-task and self-supervised learning.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Yuan_Ensemble_Generative_Cleaning_With_Feedback_Loops_for_Defending_Adversarial_Attacks_CVPR_2020_paper.html"><strong><code>Ensemble Generative Cleaning With Feedback Loops for Defending Adversarial Attacks</code></strong></a></p><p>We develop a new method called ensemble generative cleaning with feedback loops (EGC-FL) <strong>for effective defense</strong> of deep neural networks.</p><p>The proposed EGC-FL method is based on two central ideas:</p><ol><li>We introduce a transformed deadzone layer into the defense network, which consists of an orthonormal transform and a deadzone-based activation function, to destroy the sophisticated noise pattern of adversarial attacks.</li><li>By constructing a generative cleaning network with a feedback loop, we are able to generate an ensemble of diverse estimations of the original clean image.</li></ol><p>Then learn a network to fuse this set of diverse estimations together to restore the original image.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.html"><strong><code>Exploiting Joint Robustness to Adversarial Perturbations</code></strong></a></p><p>This paper <strong>exploits first-order interactions within ensembles to formalize a reliable and practical defense.</strong></p><p><strong>We introduce a scenario of interactions that certifiably improves the robustness</strong> according to the size of the ensemble, the diversity of the gradient directions, and the balance of the member’s contribution to the robustness.</p><p>We <strong>present a joint gradient phase and magnitude regularization (GPMR) as a vigorous approach to impose the desired scenario of interactions among members of the ensemble.</strong></p><p>we demonstrate that GPMR is orthogonal to other defense strategies developed for single classifiers and their combination can further improve the robustness of ensembles.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Rahmati_GeoDA_A_Geometric_Framework_for_Black-Box_Adversarial_Attacks_CVPR_2020_paper.html"><strong><code>GeoDA: a geometric framework for black-box adversarial attacks</code></strong></a></p><p><strong>We also obtain the optimal distribution of the queries over the iterations of the algorithm</strong>.(the adversary can only generate a small number of queries, each of them returning the top-1 label of the classifier)</p><p><strong>Our framework is based on the observation that the decision boundary of deep networks usually has a small mean curvature in the vicinity of data samples</strong></p><p><strong>We propose an effective iterative algorithm to generate query-efficient black-box perturbations with small $ℓ_p$ norms for p ≥ 1</strong>, which is confirmed via experimental evaluations on state-of-the-art natural image classifiers</p><p>Moreover, <strong>for p = 2, we theoretically show that our algorithm actually converges to the minimal $ℓ_2$ perturbation when the curvature of the decision boundary is bounded.</strong></p><p>We also obtain the optimal distribution of the queries over the iterations of the algorithm.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Haque_ILFO_Adversarial_Attack_on_Adaptive_Neural_Networks_CVPR_2020_paper.html"><strong><code>ILFO: Adversarial Attack on Adaptive Neural Networks</code></strong></a></p><p><strong>The first attempt to attack the energy consumption of an AdNN.</strong>(继攻击模型之后开始攻击模型的能耗…真能灌水)</p><p>We investigate the robustness of neural networks against energy-oriented attacks.(研究了模型对针对能耗进行攻击的鲁棒性)</p><p>Specifically, we propose ILFO (Intermediate Output-Based Loss Function Optimization) attack against a common type of energy-saving neural networks, Adaptive Neural Networks <strong>AdNNs save energy consumption by dynamically deactivating part of its model based on the need of the inputs.</strong></p><p>ILFO leverages intermediate output as a proxy to infer the relation between input and its corresponding energy consumption.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Jeddi_Learn2Perturb_An_End-to-End_Feature_Perturbation_Learning_to_Improve_Adversarial_Robustness_CVPR_2020_paper.html"><strong><code>Learn2Perturb: an End-to-end Feature Perturbation Learning to Improve Adversarial Robustness</code></strong></a></p><p>Adversarial training methods leverage fixed, pre-defined perturbations and require significant hyperparameter tuning that makes them very difficult to leverage in a general fashion.</p><p><strong>We introduce Learn2Perturb, an end-to-end feature perturbation learning approach for improving the adversarial robustness of deep neural networks.</strong></p><p>We introduce novel perturbation-injection modules that are incorporated at each layer to perturb the feature space and increase uncertainty in the network. <strong>This feature perturbation is performed at both the training and the inference stages.</strong>(就是在训练和 inference 的时候为每一层都添加一个扰动模块, 用这个模块来产生网络进行对抗训练所需要的不确定性)</p><p>Furthermore, inspired by the Expectation-Maximization, <strong>an alternating back-propagation training algorithm is introduced to train the network and noise parameters consecutively.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Kim_Modeling_Biological_Immunity_to_Adversarial_Examples_CVPR_2020_paper.html"><strong><code>Modeling Biological Immunity to Adversarial Examples</code></strong></a></p><p>The attack takes advantage of statistical irregularities(统计不规则性) within the training data, where the added perturbations can “move” the image across deep learning decision boundaries.</p><p><strong>In a general sense, adversarial attack through perturbations is not a machine learning vulnerability</strong>, because human can be fooled by various methods too.</p><p>There is a gap between human perception and machine perception, which means in order to change biological perception, we need to add greater amount and magnitude perturbations to the image than changing machine perception.</p><p><strong>This paper explored this gap through the lens of biology and neuroscience in order to understand the robustness exhibited in human perception.</strong>(这篇文章通过生物学和神经科学的视角探索了为什么对于模型只需要一点扰动就能改变其决策认知而需要对生物进行更大数量和幅度的扰动)</p><p>Our experiments show that by leveraging sparsity and modeling the biological mechanisms at a cellular level, we are able to mitigate the effect of adversarial alterations to the signal that have no perceptible meaning. Furthermore, we present and illustrate the effects of top-down functional processes that contribute to the inherent immunity in human perception in the context of exploiting these properties to make a more robust machine vision system.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_On_Isometry_Robustness_of_Deep_3D_Point_Cloud_Models_Under_CVPR_2020_paper.html"><strong><code>On Isometry Robustness of Deep 3D Point Cloud Models under Adversarial Attacks</code></strong></a></p><p>Regarding the 3D adversarial samples, most existing works focus on manipulation of local points, which may fail to invoke the global geometry properties, like robustness under linear projection that preserves the Euclidean distance, i.e., isometry.</p><p><strong>We show that existing state-of-the-art deep 3D models are extremely vulnerable to isometry transformations</strong>(等轴测图转换).</p><p>Armed with the Thompson Sampling, we develop a black-box attack with success rate over 95% on ModelNet40 dataset Incorporating with the Restricted Isometry Property, we <strong>propose a novel framework of white-box attack</strong> on top of spectral norm based perturbation.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Xiao_One_Mans_Trash_Is_Another_Mans_Treasure_Resisting_Adversarial_Examples_CVPR_2020_paper.html"><strong><code>Resisting Adversarial Examples by Adversarial Examples</code></strong></a></p><p><strong>To defend against adversarial examples, a plausible(合理的) idea is to obfuscate(模糊/扰乱) the network’s gradient with respect to the input image.(比如之前的模型压缩, 添加扰动模块, 打乱模型的信息等都是这一类方法)</strong> Yet, almost all of them have proven vulnerable.</p><p>We revisit this seemingly flawed idea from a radically different perspective: <strong>Using adversarial examples to resist adversarial examples, specifically,  turn the harmful attacking process into a useful defense mechanism</strong></p><p><strong>Our defense method is conceptually simple: before feeding an input image for classification, transform it by finding an adversarial example on a pre-trained external model.</strong></p><p>Particularly, in comparison to adversarial training, our method offers lower training cost as well as stronger robustness.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_One-Shot_Adversarial_Attacks_on_Visual_Tracking_With_Dual_Attention_CVPR_2020_paper.html"><strong><code>One-Shot Adversarial Attacks on Visual Tracking With Dual Attention</code></strong></a></p><p>Almost all adversarial attacks in computer vision are aimed at pre-known object categories, but for visual object tracking, the tracked target categories are normally unknown in advance. Further more, there is a problem that the attack on tracking has the free-model tracked target.(跟踪的目标是和模型无关的)</p><p><strong>We propose a method to attack tracking algorithms by adding slight perturbations on the target patch in the initial frame and cause the SOTA trackers to lose the target in subsequent frames.</strong></p><p>The optimization objective of the proposed attack consists of two components and leverages the dual attention mechanisms:</p><ol><li>The first component adopts a targeted attack strategy by optimizing the batch confidence loss with confidence attention.</li><li>The second one applies a general perturbation strategy by optimizing the feature loss with channel attention.</li></ol><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Kong_PhysGAN_Generating_Physical-World-Resilient_Adversarial_Examples_for_Autonomous_Driving_CVPR_2020_paper.html"><strong><code>PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving</code></strong></a></p><p>Most existing algorithms are used to digital-world adversarial scenarios, it is unclear now how they perform in physical world, and more importantly, the generated perturbations would cover a whole scene including these fixed background(i.e. sky), but obviously that sky could not change in physical world.</p><p><strong>We present PhysGAN, which generates physical-world-resilient adversarial examples for misleading autonomous driving systems in a continuous manner.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Tu_Physically_Realizable_Adversarial_Examples_for_LiDAR_Object_Detection_CVPR_2020_paper.html"><strong><code>Physically Realizable Adversarial Examples for LiDAR Object Detection</code></strong></a></p><p>This paper concerns the security question in automatic driving and <strong>present a method to generate universal 3D adversarial objects to fool LiDAR detectors.</strong></p><p>We demonstrate that placing an adversarial object on the rooftop of any target vehicle to hide the vehicle entirely from LiDAR detectors.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Shi_Polishing_Decision-Based_Adversarial_Noise_With_a_Customized_Sampling_CVPR_2020_paper.html"><strong><code>Polishing Decision-based Adversarial Noise with a Customized Sampling</code></strong></a></p><p>Decision-based methods polish adversarial noise by querying the target model, among them,  boundary attack is widely applied due to its powerful noise compression capability, especially when combined with transfer-based methods.(这里的 adversarial noise 应该是指 $l_p \space norm$ 的约束, 即原图和对抗样本之间不能差距太大)</p><p><strong>Boundary attack splits the noise compression into several independent sampling processes, repeating each query with a constant sampling setting.</strong></p><p><strong>We demonstrate the advantage of using current noise and historical queries to customize the variance and mean of sampling in boundary attack to polish adversarial noise. Further more, we reveal the relationship between the initial noise and the compressed noise in boundary attack.</strong></p><p>We propose Customized Adversarial Boundary (CAB) attack that uses the current noise to model the sensitivity of each pixel and polish adversarial noise of each image with a customized sampling setting.</p><p>On the one hand, CAB uses current noise as a prior belief to customize the multivariate normal distribution. On the other hand, CAB keeps the new samplings away from historical failed queries to avoid similar mistakes.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Rahnama_Robust_Design_of_Deep_Neural_Networks_Against_Adversarial_Attacks_Based_CVPR_2020_paper.html"><strong><code>Robust Design of Deep Neural Networks against Adversarial Attacks based on Lyapunov Theory</code></strong></a></p><p>In this paper <strong>we take a control theoretic approach(Lyapunov theory) to the problem of robustness in DNNs.</strong></p><p>We treat each individual layer of the DNN as a nonlinear system and use Lyapunov theory to prove stability and robustness locally, then proceed to prove stability and robustness globally for the entire DNN.</p><p>We develop empirically tight bounds on the response of the output layer, or any hidden layer, to adversarial perturbations added to the input, or to any preceding hidden layer.</p><p>We show how the spectral norm of the weight matrix for an individual layer relates to Lyapunov properties of that layer, and consequently to the local and global stability and robustness of the DNN.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Dong_Robust_Superpixel-Guided_Attentional_Adversarial_Attack_CVPR_2020_paper.html"><strong><code>Robust Superpixel-Guided Attentional Adversarial Attack</code></strong></a></p><p><strong>Most of these methods add perturbations in a “pixel-wise” and “global” way, but these two methods both have problems.</strong></p><p>Because of the contradiction between the local smoothness of normal images and the noisy property of these adversarial perturbations, this “pixel-wise” way makes these methods not robust to image processing-based defense methods and steganalysis-based methods.</p><p>we find that adding perturbations to the background is less useful than to the salient object, thus the “global” way is not optimal.</p><p><strong>We propose the first robust superpixel-guided attentional adversarial attack method. The adversarial perturbations are only added to the salient regions and guaranteed to be the same within each superpixel</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/B.S._Single-Step_Adversarial_Training_With_Dropout_Scheduling_CVPR_2020_paper.html"><strong><code>Single-Step Adversarial Training With Dropout Scheduling</code></strong></a></p><p>In adversarial training regime, models are trained with mini-batches augmented with adversarial samples. Fast and simple methods (e.g., single-step gradient ascent) are used for generating adversarial samples, in order to reduce computational complexity.</p><p>It is shown that <strong>models trained using single-step adversarial training method</strong> (adversarial samples are generated using non-iterative method) <strong>are pseudo robust</strong>(伪鲁棒的).</p><p>Further, <strong>this pseudo robustness of models is attributed to the gradient masking effect</strong>. However, <strong>existing works fail to explain when and why gradient masking effect occurs during single-step adversarial training.</strong></p><p>In this work, we (1) show that <strong>models trained using single-step adversarial training method learn to prevent the generation of single-step adversaries</strong>, and this is <strong>due to over-fitting of the model during the initial stages of training.</strong> (2) <strong>to mitigate this effect, we propose a single step adversarial training method with dropout scheduling.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Addepalli_Towards_Achieving_Adversarial_Robustness_by_Enforcing_Feature_Consistency_Across_Bit_CVPR_2020_paper.html"><strong><code>Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes</code></strong></a></p><p><strong>We attempt to address adversarial attack by training networks to form coarse impressions based on the information in higher bit planes, and use the lower bit planes only to refine their prediction.</strong></p><p>By imposing consistency on the representations learned across differently quantized images, the adversarial robustness of networks improves significantly when compared to a normally trained model.</p><p>Though by now adversarial training is the best way to defense adversarial attacks, this work paves the way towards achieving robustness without having to explicitly train on adversarial samples, which means our method is faster, and also closer to the natural learning process in humans.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_Towards_Large_Yet_Imperceptible_Adversarial_Image_Perturbations_With_Perceptual_Color_CVPR_2020_paper.html"><strong><code>Towards Large Yet Imperceptible Adversarial Image Perturbations With Perceptual Color Distance</code></strong></a></p><p>The conventional <strong>assumption</strong> on imperceptibility is that <strong>perturbations should strive for tight $L_p-norm$ bounds in RGB space.</strong></p><p><strong>We drop this assumption by pursuing an approach that exploits human color perception,  more specifically, minimizing perturbation size with respect to perceptual color distance.</strong></p><p>The first proposed approach called Perceptual Color distance C&amp;W (PerC-C&amp;W), extends the widely-used C&amp;W approach and produces larger RGB perturbations.</p><p>The second proposed approach  Perceptual Color distance Alternating Loss (PerC-AL), achieves the same outcome, but does so more efficiently by alternating between the classification loss and perceptual color difference when updating perturbations.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Transferable_Controllable_and_Inconspicuous_Adversarial_Attacks_on_Person_Re-identification_With_CVPR_2020_paper.html"><strong><code>Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking</code></strong></a></p><p>In this work, <strong>we examine the insecurity of current best performing ReID models</strong> by proposing a learning-to-mis-rank formulation to perturb the ranking of the system output.</p><p>As the cross-dataset transferability is crucial in the ReID domain, we also perform a back-box attack by developing a novel multi-stage network architecture that pyramids the features of different levels to extract general and transferable features for the adversarial perturbations.</p><p>Our method can control the number of malicious pixels by using differentiable multi-shot sampling. To guarantee the inconspicuousness of the attack, we also propose a new perception loss to achieve better visual quality.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Understanding_Adversarial_Examples_From_the_Mutual_Influence_of_Images_and_CVPR_2020_paper.html"><strong><code>Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations</code></strong></a></p><p>We propose to treat the DNN logits as a vector for feature representation, and <strong>utilize this vector representation to understand adversarial examples and adversarial perturbations, and analyze the influence on each other, based on the Pearson correlation coefficient (PCC).</strong></p><p>Our results suggest a new perspective towards the relationship between images and universal perturbations:  <strong>Universal perturbations contain dominant features, and images behave like noise to them.</strong></p><p>This feature perspective leads to a new method for generating targeted universal adversarial perturbations using random source images. <strong>We are the first to achieve the challenging task of a targeted universal attack without utilizing original training data.</strong></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Guo_When_NAS_Meets_Robustness_In_Search_of_Robust_Architectures_Against_CVPR_2020_paper.html"><strong><code>When NAS Meets Robustness: In Search of Robust Architectures Against Adversarial Attacks --Dahua Lin</code></strong></a></p><p><strong>We take an architectural perspective and investigate the patterns of network architectures that are resilient to adversarial attacks.</strong></p><p>Training a large network for once (one-shot NAS) and then fine-tuning the sub-networks sampled therefrom.</p><p>The result shows some valuable observations:</p><ol><li>Densely connected patterns result in improved robustness.</li><li>Under computational budget, adding convolution operations to direct connection edge is effective.</li><li>Flow of solution procedure (FSP) matrix is a good indicator of network robustness.</li></ol><h2 id="ICCV"><a href="#ICCV" class="headerlink" title="ICCV"></a>ICCV</h2><h3 id="2019-24-papers"><a href="#2019-24-papers" class="headerlink" title="2019(24 papers)"></a>2019(24 papers)</h3><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Mustafa_Adversarial_Defense_by_Restricting_the_Hidden_Space_of_Deep_Neural_ICCV_2019_paper.html"><strong><code>Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks</code></strong></a></p><p>The robustness of existing defenses suffers greatly under the white-box attack, which means attacker has full knowledge about the network. <strong>The main reason for the existence of such perturbations is the close proximity of different class samples in feature space.</strong></p><p><strong>we propose to class-wise disentangle the intermediate representations of DNNs. Specifically, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes.</strong></p><p>In this manner, the network is forced to learn distinct and distant decision regions for each class, which means impove the model’s  robustness.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.html"><strong><code>Adversarial Defense via Learning to Generate Diverse Attacks</code></strong></a></p><p><strong>Propose to utilize the generator to learn how to create adversarial examples. Unlike the existing approaches that create a one-shot perturbations by a deterministic generator, we propose a recursive and stochastic generator that produces much stronger and diverse perturbations that comprehensively reveal the vulnerability of the target classifier.</strong></p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Zhong_Adversarial_Learning_With_Margin-Based_Triplet_Embedding_Regularization_ICCV_2019_paper.html"><strong><code>Adversarial Learning with Margin-based Triplet Embedding Regularization</code></strong></a></p><p><strong>Propose to improve the smoothness of the representation space, by integrating a margin-based triplet embedding regularization term into the classification object, so that the obtained model learns to resist adversarial examples.</strong></p><p>The regularization term consists of two steps optimizations which find potential perturbations and punish them by a large margin in an iterative way.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Adversarial_Robustness_vs._Model_Compression_or_Both_ICCV_2019_paper.html"><strong><code>Adversarial Robustness vs. Model Compression, or Both?</code></strong></a></p><p>Adversarial robustness requires a significantly larger capacity of the network than that for the natural training with only benign examples, this paper <strong>propose a framework of concurrent adversarial training and weight pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the dilemma of adversarial training.</strong></p><p>Furthermore, this work studies two hypotheses about weight pruning in the conventional setting and finds that:</p><ol><li>weight pruning is essential for reducing the model size in the adversarial setting.</li><li>training a small model from scratch even with inherited initialization from the large model cannot achieve neither adversarial robust nor high standard accuracy.</li></ol><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.html"><strong><code>Bilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks</code></strong></a></p><p>Study the fast training of adversarially robust models, from the analysis of the SOTA defense methods, i.e. multi-step adversarial training, we <strong>hypothesize that the gradient magnitude links to the model robust. Motivated by this, we propose to perturb both the image and the label during training, which called Bilateral Adversarial Training(BAT).</strong></p><p>To generate the adversarial image, using one-step targeted attack with the target label being the most confusing class. To generate the adversarial label, we derive an closed-form heuristic solution.</p><p>The paper first show that random start and the most confusing target attack effectively prevent the label leaking and gradient masking problem. Then coupled with the adversarial label part , our model significantly improves the SOTA results.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Gupta_CIIDefence_Defeating_Adversarial_Attacks_by_Fusing_Class-Specific_Image_Inpainting_and_ICCV_2019_paper.html"><strong><code>CIIDefence: Defeating Adversarial Attacks by Fusing Class-specific Image Inpainting and Image Denoising</code></strong></a></p><p>The proposed defence mechanism is inspired by the <strong>recent works mitigating(缓和) the adversarial disturbances(扰动) by the means of image reconstruction and denoising.</strong></p><p>Unlike previous work, <strong>this paper apply reconstruction only for small and carefully selected image area that are most influential to the current classification outcome.</strong> The selection process is guided by the CAM responses obtained for multiple top-ranking class label. The same regions are also the most prominent for the adversarial perturbations and hence most important to purify. The resulting inpainting task is substantially(本质上) more tractable(容易) than the full image reconstruction, while still being able to prevent the adversarial attacks.</p><p>We combine the selective image inpainting with wavelet(小波) based image denoising to produce a non-differentiable layer that prevents attacker from using gradient backpropagation. The proposed nonlinearity cannot be easily approximated with simple differentiable alternative as demonstrated in the experiments with Backward Pass Differentiable Approximation (BPDA) attack.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Mummadi_Defending_Against_Universal_Perturbations_With_Shared_Adversarial_Training_ICCV_2019_paper.html"><strong><code>Defending Against Universal Perturbations With Shared Adversarial Training</code></strong></a></p><p><strong>We show that adversarial training is more effective in preventing universal perturbations and investigate the trade-off between robustness against universal perturbations and performance on unperturbed data. Moreover, propose an extension of adversarial training that handles this trade-off more gracefully.</strong></p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Enhancing_Adversarial_Example_Transferability_With_an_Intermediate_Level_Attack_ICCV_2019_paper.html"><strong><code>Enhancing Adversarial Example Transferability With an Intermediate Level Attack</code></strong></a></p><p><strong>Adversarial examples are typically overfit to exploit the particular architecture and feature representation ofa source model, resulting in sub-optimal black-box transfer attacks to other target models.</strong></p><p>This paper introduce the Intermediate Level Attack (ILA), which <strong>attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model.</strong></p><p>The result show that we can select a layer of the source model to perturb without any knowledge of the target models while achieving high transferability, what’s more, we provide some explanatory insights regarding our method and the effect of optimizing for adversarial examples using intermediate feature maps.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Evaluating_Robustness_of_Deep_Image_Super-Resolution_Against_Adversarial_Attacks_ICCV_2019_paper.html"><strong><code>Evaluating Robustness of Deep Image Super-Resolution Against Adversarial Attacks</code></strong></a></p><p><strong>This paper investigates(调查) the robustness of deep learning-based super-resolution methods against adversarial attacks.</strong> These attacks can significantly deteriorate the superresolved images without noticeable distortion in the attacked low-resolution images.</p><p>The results show that stateof-the-art deep super-resolution methods are highly vulnerable to adversarial attacks. We also present analysis on transferability ofattacks, and feasibility oftargeted attacks and universal attacks.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Brunner_Guessing_Smart_Biased_Sampling_for_Efficient_Black-Box_Adversarial_Attacks_ICCV_2019_paper.html"><strong><code>Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial Attacks</code></strong></a></p><p><strong>The paper proposes a efficient attack named Boundary Attack, can be reinterpreted as a biased sampling framework that gains efficient from domain knowledge. With combining there biases, named <code>image frequency</code>, <code>region masks</code> and <code>surrogate(替代) gradient</code>, outperforms the SOTA attack approaches.</strong></p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Bai_Hilbert-Based_Generative_Defense_for_Adversarial_Examples_ICCV_2019_paper.html"><strong><code>Hilbert-based Generative Defense for Adversarial Examples</code></strong></a></p><p><strong>This paper aims to <code>improve the developed PixelDefend</code>, which purifies a perturbed image on PixelCNN in a raster(扫描线) scan order (row/col by row/col).</strong> This scan mode inefficiently exploits the correlations between pixels, which further limit its robustness performance.</p><p>We propose a more advanced Hilbert curve scan order to model the pixel dependencies in this paper. It could well preserve local consistency(一致性) when mapping from 2-D image to 1-D vector, thus the local features in neighboring pixels can be more effectively modeled, the defensive power can be further improved via ensembles ofHilbert curve with different orientations(方向).</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Improving_Adversarial_Robustness_via_Guided_Complement_Entropy_ICCV_2019_paper.html"><strong><code>Improving Adversarial Robustness via Guided Complement Entropy</code></strong></a></p><p>Many recent methods have proposed to improve adversarial robustness by utilizing adversarial training or model distillation, which adds additional procedures to model training.</p><p><strong>we propose a new training paradigm calledGuidedComplementEntropy (GCE) that is capable of achieving“adversarial defense for free,”which involves no additional procedures in the process of improving adversarial robustness.</strong></p><p>In addition to maximizing model probabilities on the ground-truth class like cross entropy, we neutralize its probabilities on the incorrect classes along with a “guided” term to balance between these two terms.</p><p>We also show that our method can be used orthogonal to adversarial training across wellknown methods with noticeable robustness gain.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Miss_Detection_vs._False_Alarm_Adversarial_Learning_for_Small_Object_ICCV_2019_paper.html"><strong><code>Miss Detection vs. False Alarm: Adversarial Learning for Small Object Segmentation in Infrared Images</code></strong></a></p><p><strong>Using a adversarial learning to improve the situation that the miss detection and false alarm in infrared small object segmentation.</strong></p><p>To balance miss detection (MD) and false alarm (FA), usually need “opposite” strategies to suppress the two terms, this problem is unsolved yet.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_On_the_Design_of_Black-Box_Adversarial_Examples_by_Leveraging_Gradient-Free_ICCV_2019_paper.html"><strong><code>On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method</code></strong></a></p><p>Existing studies on black-box adversarial attacks are still restricted to very specific settings of threat models,  and/or suffer from prohibitively high query complexity.</p><p><strong>we introduce a general framework based on an operator splitting method, the alternating direction method of multipliers (ADMM) to devise efficient, robust black-box attack</strong>s that work with various distortion metrics and feedback settings without incurring high query complexity.</p><p>Due to the black-box nature of the threat model, the proposed ADMM solution framework is integrated with zeroth-order (ZO) optimization and Bayesian optimization (BO), and thus is applicable to the gradient-free regime, resulting in  two new black-box adversarial attack generation methods, ZOADMM and BO-ADMM.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Han_Once_a_MAN_Towards_Multi-Target_Attack_via_Learning_Multi-Target_Adversarial_ICCV_2019_paper.html"><strong><code>Once a MAN: Towards Multi-Target Attack via Learning Multi-Target Adversarial Network Once --Xiaogang Wang</code></strong></a></p><p>Based on the first optimization-based attacking method, many following methods are proposed to improve the attacking performance and speed.</p><p>Recently, <strong>generation-based methods have received much attention since they directly use feed-forward networks to generate the adversarial examples, which avoid the time-consuming iterative attacking procedure in optimization-based and gradient-based methods.</strong></p><p>However, current generation based methods are only able to attack one specific target (category) within one model, thus making them not applicable to real classification systems that often have hundreds thousands of categories.</p><p><strong>We propose <code>the first</code> Multi-target Adversarial Network (MAN), which can generate multi-target adversarial examples with a single model, MAN can produce stronger  attack results and also have better transferability, using the adversarial examples generated by our MAN to improve the robustness of the classification model can also achieve better classification accuracy than other methods.</strong></p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Wiyatno_Physical_Adversarial_Textures_That_Fool_Visual_Object_Tracking_ICCV_2019_paper.html"><strong><code>Physical Adversarial Textures That Fool Visual Object Tracking</code></strong></a></p><p><strong>Present a method for creating inconspicuous-looking adversarial textures, cause visual object tracking systems to become confused.</strong></p><p>As a target being visually tracked moves in front of such a poster, its adversarial texture makes the tracker lock onto it, thus allowing the target to evade.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Gowal_Scalable_Verified_Training_for_Provably_Robust_Image_Classification_ICCV_2019_paper.html"><strong><code>Scalable Verified Training for Provably Robust Image Classification --DeepMind</code></strong></a></p><p>Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations:<br>$$<br>\min <em>{\theta} \rho(\theta), \quad \text { where } \quad \rho(\theta)=\mathbb{E}</em>{(x, y) \sim \mathcal{D}}\left[\max _{\delta \in \mathcal{S}} L(\theta, x+\delta, y)\right]<br>$$</p><p>But there adversarial training methods often result in difficult optimization procedures that remain hard to scale to larger networks.</p><p><strong>We show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks</strong> that beat the state-of-the-art in verified accuracy.</p><p>While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Self-Training_and_Adversarial_Background_Regularization_for_Unsupervised_Domain_Adaptive_One-Stage_ICCV_2019_paper.html"><strong><code>Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection</code></strong></a></p><p><strong>Using adversarial background helps the network extract discriminative features for target backgrounds to reduce the domain shift, improving performance of domain adaption in one-stage object detection</strong></p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Joshi_Semantic_Adversarial_Attacks_Parametric_Transformations_That_Fool_Deep_Classifiers_ICCV_2019_paper.html"><strong><code>Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers</code></strong></a></p><p>The majority of adversarial attacks assume global, fine-grained control over the image pixel space. <strong>This paper consider a different setting:  what happens if the adversary could only alter specific attributes of the input image? The results show that these would generate inputs that might be perceptibly different, but still natural-looking and enough to fool a classifier.</strong></p><p>We propose a novel approach to generate such “semantic” adversarial examples by optimizing a particular adversarial loss over the range-space of a parametric conditional generative model.</p><p>We demonstrate implementations of our attacks on binary classifiers trained on face images, and show that such natural-looking semantic adversarial examples exist</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Croce_Sparse_and_Imperceivable_Adversarial_Attacks_ICCV_2019_paper.html"><strong><code>Sparse and Imperceivable Adversarial Attacks</code></strong></a></p><p>From a safety perspective, highly sparse adversarial attacks are particularly dangerous.On the other hand the pixelwise perturbations of sparse attacks are typically large and thus can be potentially detected.</p><p><strong>We propose a new black-box technique to craft adversarial examples aiming at minimizing l0- distance to the original image.</strong></p><p>The proposed attack approach is not only better than existing SOTA models, but also can integrate additional bounds on the component-wise perturbation. Allowing pixels to change only in region of high variation and avoiding changes along axis aligned edges makes our adversarial examples almost non-perceivable.</p><p>Moreover, we adapt the Projected Gradient Descent attack to the l0-norm integrating component-wise constraints, which allow us to do adversarial training to improve the robustness of model.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Tolias_Targeted_Mismatch_Adversarial_Attack_Query_With_a_Flower_to_Retrieve_ICCV_2019_paper.html"><strong><code>Targeted Mismatch Adversarial Attack: Query with a Flower to Retrieve the Tower</code></strong></a></p><p><strong>Propose a targeted mismatch attack for DL-based retrieval system to generate an adversarial image to conceal(隐藏) the query image.</strong></p><p>We show successful attacks to partially unknown systems, by designing various loss functions for the adversarial image construction</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Finlay_The_LogBarrier_Adversarial_Attack_Making_Effective_Use_of_Decision_Boundary_ICCV_2019_paper.html"><strong><code>The LogBarrier adversarial attack: making effective use of decision boundary information</code></strong></a></p><p><strong>Propose a new untargeted attack, which uses the well-regarded logarithmic barrier method(best practices from the optimization literature), to solve the constrained minimization question in gradient-based attacks.</strong></p><p>The proposed LogBarrier attack perturbs all images more efficiently than other attacks: the distance needed to perturb all images is significantly smaller with the LogBarrier attack than with other state-ofthe-art attacks.</p><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Universal_Adversarial_Perturbation_via_Prior_Driven_Uncertainty_Approximation_ICCV_2019_paper.html"><strong><code>Universal Adversarial Perturbation via Prior Driven Uncertainty Approximation --Hong Liu</code></strong></a></p><p>Compared to the conventional supervised universal adversarial perturbations(UAPs) that suffer from the knowledge of training data, the data-independent unsupervised UAPs are more applicable. <strong>Existing unsupervised methods fail to take advantage of the model uncertainty to produce robust perturbations.</strong></p><p><strong>we propose a new unsupervised universal adversarial perturbation method to generate a robust UAP by fully exploiting the model uncertainty.</strong></p><p>A Monte Carlo sampling method is deployed to activate more neurons to increase the model uncertainty for a better adversarial perturbation. Then a textural bias prior revealing a statistical uncertainty is proposed, which helps to improve the attacking performance.</p><p>The UAP is crafted by the stochastic gradient descent algorithm with a boosted momentum optimizer, and a Laplacian pyramid frequency model is finally used to maintain the statistical uncertainty.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Addepalli_Towards_Achieving_Adversarial_Robustness_by_Enforcing_Feature_Consistency_Across_Bit_CVPR_2020_paper.html"><strong><code>What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance</code></strong></a></p><p>This paper examines a type of global image manipulation that can produce similar adverse effects.</p><p><strong>We explore how strong color casts caused by incorrectly applied computational color constancy (referred to as white balance (WB) in photography) negatively impact the performance of DNNs</strong> targeting image segmentation and classification. (探讨了由于强烈偏色导致的对模型性能所产生的负面影响)</p><p>We discuss how existing image augmentation methods used to improve the robustness of DNNs are not well suited for modeling WB errors. To address this problem, a novel augmentation method is proposed that can emulate accurate color constancy degradation.We also explore pre-processing training and testing images with a recent WB correction algorithm to reduce the effects of incorrectly white-balanced images.</p><h2 id="ECCV"><a href="#ECCV" class="headerlink" title="ECCV"></a>ECCV</h2><h3 id="2020-29-papers"><a href="#2020-29-papers" class="headerlink" title="2020(29 papers)"></a>2020(29 papers)</h3><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590766.pdf"><strong><code>Adversarial Ranking Attack and Defense</code></strong></a><br><strong>Proposed two attacks against deep ranking systems, Candidate Attack and Query Attack,  that can raise or lower the rank of chosen candidates by adversarial perturbations.</strong></p><p>Specifically, the expected ranking order is first represented as a set of inequalities, and then a triplet-like objective function is designed to obtain the optimal perturbation.</p><p>Conversely, a defense method is also proposed to improve the ranking system robustness, which can mitigate all the proposed attacks simultaneously.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500647.pdf"><strong><code>Adversarial T-shirt! Evading Person Detectors in A Physical World</code></strong></a></p><p><strong>Most of the existing works on physical adversarial attacks focus on static objects.</strong></p><p><strong>we propose Adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person’s pose changes.</strong></p><p><strong>This is the first work that models the effect of deformation for designing physical adversarial examples with respect to non-rigid objects such as T-shirts.</strong>(第一次为非刚性物体设计对抗样本)</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570239.pdf"><strong><code>AdvPC: Transferable Adversarial Perturbations on 3D Point Clouds</code></strong></a><br>In this work, we <strong>present novel data-driven adversarial attacks against 3D point cloud networks.</strong></p><p>We aim to address the following problems:</p><ol><li>in current 3D point cloud adversarial attacks: they do not transfer well between different networks</li><li>they are easy to defend against via simple statistical methods.</li></ol><p><strong>We develop a new point cloud attack (dubbed AdvPC) that exploits the input data distribution by adding an adversarial loss, after Auto-Encoder reconstruction, to the objective it optimizes.</strong> AdvPC leads to perturbations that are resilient against current defenses, while remaining highly transferable compared to state-of-theart attacks.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580069.pdf"><strong><code>Anti-Bandit Neural Architecture Search for Model Defense --Hong Liu</code></strong></a></p><p><strong>We defend against adversarial attacks using neural architecture search (NAS) which is based on a comprehensive search of denoising blocks, weight-free operations, Gabor filters and convolutions.</strong></p><p>The resulting anti-bandit NAS (ABanditNAS) incorporates a new operation evaluation measure and search process based on the lower and upper confidence bounds (LCB and UCB).</p><p><strong>Unlike the conventional bandit algorithm using UCB for evaluation only, we use UCB to abandon arms for search efficiency and LCB for a fair competition between arms.</strong></p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580375.pdf"><strong><code>API-Net: Robust Generative Classifier via a Single Discriminator --Hong Liu</code></strong></a></p><p>For classification problem, a <strong>generative classifier</strong> typically models the distribution of inputs and labels, and thus <strong>can better handle off-manifold examples</strong> at the cost of a concise structure.(生成模型中的分类器, 通常对输入的分布和 label 的分布进行建模, 这样就能在简洁的结构上来更好地处理非流形的数据)</p><p>On the contrary, a <strong>discriminative classifier only models the conditional distribution of labels given inputs</strong>, but benefits from effective optimization owing to its succinct structure.(判别模型中的分类器, 只对在给定 label 下的输入, 即条件概率进行建模, 但由于其简洁的结构能得到有效的优化)</p><p><strong>This work aims for a solution of generative classifiers that can profit from the merits of both.</strong></p><p>We propose an Anti-Perturbation Inference (API) method, which searches for anti-perturbations to maximize the lower bound of the joint log-likelihood of inputs and classes.</p><p>By leveraging the lower bound to approximate Bayes’ rule, we construct a generative classifier Anti-Perturbation Inference Net (API-Net) upon a single discriminator. It takes advantage of the generative properties to tackle off-manifold examples while maintaining a succinct structure for effective optimization.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660035.pdf"><strong><code>APRICOT: A Dataset of Physical Adversarial Attacks on Object Detection</code></strong></a></p><p>We present APRICOTy, <strong>a collection of over 1,000 annotated photographs of printed adversarial patches in public locations.</strong></p><p>Our analysis suggests that maintaining adversarial robustness in uncontrolled settings is highly challenging but that it is still possible to produce targeted detections under white-box and sometimes black-box settings.</p><p><strong>We establish baselines for defending against adversarial patches via several methods</strong>, including using a detector supervised with synthetic data and using unsupervised methods such as kernel density(密度) estimation, Bayesian uncertainty, and reconstruction error.</p><p>Our results suggest that adversarial patches can be effectively flagged(被标记), both in a high-knowledge, attack specific scenario and in an unsupervised setting where patches are detected as anomalies(异常) in natural images.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600273.pdf"><strong><code>Boosting Decision-based Black-box Adversarial Attacks with Random Sign Flip</code></strong></a><br><strong>Existing decision-based attacks perform poorly on the $l_{\infty}$ setting and the required enormous queries</strong> cast a shadow over the practicality.</p><p>We show that <strong>just randomly flipping the signs of a small number of entries in adversarial perturbations can significantly boost the attack performance</strong></p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730749.pdf"><strong><code>Defense Against Adversarial Attacks via Controlling Gradient Leaking on Embedded Manifolds --Jun Zhu</code></strong></a></p><p><strong>we present a new perspective, namely gradient leaking hypothesis, to understand the existence of adversarial examples and to further motivate effective defense strategies.</strong></p><p>We consider the low dimensional manifold structure of natural images, and empirically verify that the leakage of the gradient (w.r.t input) along the (approximately) perpendicular(垂直的) direction to the tangent(切线的) space of data manifold is a reason for the vulnerability over adversarial attacks.</p><p>We further present a new robust learning algorithm which encourages a larger gradient component in the tangent space of data manifold, suppressing the gradient leaking phenomenon consequently.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620171.pdf"><strong><code>Design and Interpretation of Universal Adversarial Patches in Face Detection  --Jun Zhu</code></strong></a></p><p><strong>We propose new optimization based approaches to automatic design of universal adversarial patches for varying goals of the attack</strong>, including scenarios in which true positives are suppressed without introducing false positives.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540426.pdf"><strong><code>Gabor Layers Enhance Network Robustness</code></strong></a><br><strong>We explore the effect of replacing the first layers of various deep architectures with Gabor layers on robustness against adversarial attacks.</strong> (i.e. convolutional layers with filters that are based on learnable Gabor parameters)</p><p><strong>Architectures with Gabor layers gain a consistent boost in robustness over regular models and maintain high generalizing test performance.</strong></p><p>We then exploit the analytical expression of Gabor filters to derive a compact expression for a Lipschitz constant of such filters, and harness(利用) this theoretical result to develop a regularizer we use during training to further enhance network robustness.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700103.pdf"><strong><code>Improving Query Eﬃciency of Black-box Adversarial Attack</code></strong></a></p><p>Existing query-based black-box adversarial attacks often require excessive model queries to maintain a high attack success rate.</p><p><strong>In order to improve query efficiency, we explore the distribution of adversarial examples around benign inputs with the help of image structure information characterized by a Neural Process, and propose a Neural Process based black-box adversarial attack (NP-Attack) in this paper.</strong></p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670562.pdf"><strong><code>Improving the Transferability of Adversarial Examples with Resized-Diverse-Inputs, Diversity-Ensemble and Region Fitting</code></strong></a><br><strong>We introduce a three stage pipeline</strong>: resized-diverse-inputs (RDIM), diversity-ensemble (DEM) and region fitting, that <strong>work together to generate transferable adversarial examples.</strong></p><p>We first explore the internal relationship between existing attacks, and propose RDIM that is capable of exploiting this relationship.</p><p>Then we propose DEM, the multi-scale version of RDIM, to generate multi-scale gradients.</p><p>Finally,  we transform value fitting into region fitting across iterations.</p><p>RDIM and region fitting do not require extra running time and these three steps can be well integrated into other attacks</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490001.pdf"><strong><code>Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors</code></strong></a><br>Using standard detection datasets, we train patterns that suppress the objectness scores produced by a range of commonly used detectors, and ensembles of detectors.</p><p><strong>We benchmark the effectiveness of adversarially trained patches</strong> under both white-box and black-box settings, and <strong>quantify transferability of attacks between datasets, object classes, and detector models.</strong></p><p><strong>We present a detailed study of physical world attacks</strong> using printed posters and wearable clothes, and rigorously <strong>quantify the performance of such attacks with different metrics.</strong></p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750290.pdf"><strong><code>Manifold Projection for Adversarial Defense on Face Recognition</code></strong></a><br><strong>A recent study has shown that in addition to regular off-manifold adversarial images, there are also adversarial images on the manifold.</strong></p><p>We propose Adversarial Variational AutoEncoder (A-VAE), a novel framework to tackle both types of attacks. <strong>We hypothesize that both off-manifold and on-manifold attacks move the image away from the high probability region of image manifold.</strong></p><p><strong>We utilize variational auto-encoder (VAE) to estimate the lower bound of the log-likelihood of image and explore to project the input images back into the high probability regions of image manifold again.</strong></p><p>At inference time, our model synthesizes multiple similar realizations of a given image by random sampling, then the nearest neighbor of the given image is selected as the final input of the face recognition model.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650239.pdf"><strong><code>Motion-Excited Sampler: Video Adversarial Attack with Sparked Prior --Yi Yang</code></strong></a><br>We aim to <strong>attack video models by utilizing intrinsic(固有的) movement pattern and regional relative motion(运动) among video frames.</strong></p><p>We propose an effective motion excited sampler to obtain motion-aware noise prior, which we term as sparked prior. Our sparked prior underlines(强调) frame correlations and utilizes video dynamics via relative motion.</p><p>By using the sparked prior in gradient estimation, we can successfully attack a variety of video classification models with fewer number of queries.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470154.pdf"><strong><code>Multitask Learning Strengthens Adversarial Robustness</code></strong></a></p><p><strong>We connect the adversarial robustness of a model to the number of tasks</strong> that it is trained on.</p><p><strong>Results show that attack difficulty increases as the number of target tasks increase and when models are trained on multiple tasks at once, they become more robust to adversarial attacks on individual tasks.</strong></p><p>While adversarial defense remains an open challenge, our results suggest that deep networks are vulnerable partly because they are trained on too few tasks.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620664.pdf"><strong><code>Open-set Adversarial Defense</code></strong></a><br><strong>We show that open-set recognition systems are vulnerable to adversarial attacks and adversarial defense mechanisms trained on known classes do not generalize well to open-set samples.</strong> we emphasize the need of an Open-Set Adversarial Defense (OSAD) mechanism.</p><p>This paper proposes an Open-Set Defense Network (OSDN) as a solution to the OSAD problem. The proposed network uses an encoder with feature-denoising layers coupled with a classifier to learn a noise-free latent feature representation.</p><p>Two techniques are employed to obtain an informative latent feature space with the objective of improving open-set performance. First, a decoder is used to ensure that clean images can be reconstructed from the obtained latent features. Then, self-supervision is used to ensure that the latent features are informative enough to carry out an auxiliary task.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730307.pdf"><strong><code>Patch-wise Attack for Fooling Deep Neural Network</code></strong></a><br>Features of a pixel extracted by deep neural networks (DNNs) are influenced by its surrounding regions, and different DNNs generally focus on different discriminative regions in recognition.</p><p><strong>We propose a patch-wise iterative algorithm – a black-box attack towards mainstream normally trained and defense models, which differs from the existing attack methods manipulating pixel-wise noise.</strong></p><p>In this way, without sacrificing the performance of white-box attack, our adversarial examples can have strong transferability.</p><p><strong>Specifically, we introduce an amplification factor to the step size in each iteration, and one pixel’s overall gradient overflowing the $\epsilon$-constraint is properly assigned to its surrounding regions by a project kernel.</strong></p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720137.pdf"><strong><code>Practical Poisoning Attacks on Neural Networks</code></strong></a><br>Data poisoning attacks means that  poisoning samples are injected at the training phase to achieve adversarial goals at test time. Existing poisoning attacks rely on certain assumptions on the adversary knowledge and capability to ensure efficacy, which may be unrealistic in practice.</p><p>This paper presents a new, practical targeted poisoning attack method, namely BlackCard.</p><p><strong>BlackCard possesses a set of critical properties for ensuring attacking efficacy in practice</strong>, which has never been simultaneously achieved by any existing work, including knowledge-oblivious, clean-label, and clean-test.</p><p>We show that the effectiveness of BlackCard can be intuitively guaranteed by a set of analytical reasoning and observations, through exploiting an essential characteristic of gradient-descent optimization which is pervasively adopted in DNN models.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550188.pdf"><strong><code>Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks</code></strong></a></p><p>First introduce the notion of “backdoor attack”. A backdoor attack installs a backdoor into the victim model by injecting a backdoor pattern into a small proportion of the training data. At test time, the victim model behaves normally on clean test data, yet consistently predicts a specific (likely incorrect) target class whenever the backdoor pattern is present in a test example.</p><p>While existing backdoor attacks are effective, they are not stealthy. The modifications made on training data or labels are often suspicious and can be easily detected by simple data filtering or human inspection.</p><p><strong>we present a new type of backdoor attack inspired by an important natural phenomenon: reflection. Using mathematical modeling of physical reflection models, we propose reflection backdoor (Refool) to plant reflections as backdoor into a  victim model.</strong></p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560766.pdf"><strong><code>Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses</code></strong></a></p><p>Adversarial examples generated by existing attacks are generally hard to transfer to defense models.</p><p>We <strong>observe the property of regional homogeneity(区域同质性) in adversarial perturbations</strong> and suggest that <strong>the defenses are less robust to regionally homogeneous perturbations.</strong></p><p><strong>Therefore, we propose an effective transforming paradigm and a customized gradient transformer module to transform existing perturbations into regionally homogeneous ones.</strong></p><p>Without explicitly forcing the perturbations to be universal, we observe that a well-trained gradient transformer module tends to output input-independent gradients (hence universal) benefiting from the under-fitting phenomenon.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540392.pdf"><strong><code>Robust Neural Networks inspired by Strong Stability Preserving Runge-Kutta methods</code></strong></a></p><p>Recent works observe that a class of widely used neural networks can be viewed as the Euler(欧拉) method of numerical discretization(数值离散化).</p><p>From the numerical discretization perspective, Strong Stability Preserving (SSP) methods are more advanced techniques than the explicit Euler method that produce both accurate and stable solutions.</p><p>Motivated by the SSP property and a generalized Runge-Kutta method, <strong>we proposed Strong Stability Preserving networks (SSP networks) which improve robustness against adversarial attacks.</strong></p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640069.pdf"><strong><code>Robust Tracking against Adversarial Attacks</code></strong></a><br>We <strong>first attempt to generate adversarial examples on top of video sequences to improve the tracking robustness</strong> against adversarial attacks.</p><p><strong>We take temporal motion into consideration when generating lightweight perturbations over the estimated tracking results frame-by-frame.</strong> On one hand, we add the temporal perturbations into the original video sequences as adversarial examples to greatly degrade the tracking performance. On the other hand, we sequentially estimate the perturbations from input sequences and learn to eliminate their effect for performance restoration.</p><p>Our defense method not only eliminates the large performance drops caused by adversarial attacks, but also achieves additional performance gains when deep trackers are not under adversarial attacks.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590018.pdf"><strong><code>SemanticAdv: Generating Adversarial Examples via Attribute-conditioned Image Editing</code></strong></a><br>Most such adversarial examples try to guarantee “subtle” perturbation(轻微的扰动) by limiting the $L_p$ norm of the perturbation.</p><p><strong>We propose SemanticAdv to generate a new type of semantically realistic adversarial examples via attribute-conditioned image editing. Our SemanticAdv enables fine-grained analysis and evaluation of DNNs with input variations in the attribute space.</strong></p><p>Our adversarial examples not only exhibit semantically meaningful appearances but also achieve high targeted attack success rates under both white-box and black-box settings.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700205.pdf"><strong><code>SPARK: Spatial-aware Online Incremental Attack Against Visual Tracking</code></strong></a><br>we identify a new task for the adversarial attack to visual tracking: online generating imperceptible perturbations that mislead trackers along with an incorrect (Untargeted Attack, UA) or specified trajectory (Targeted Attack, TA).</p><p>We first propose a spatial-aware basic attack by adapting existing attack methods, i.e., FGSM, BIM, and C W, and comprehensively analyze the attacking performance.</p><p>We identify that online object tracking poses two new challenges: 1) it is difficult to generate imperceptible perturbations that can transfer across frames, and 2) realtime trackers require the attack to satisfy a certain level of efficiency.</p><p>To address these challenges, <strong>we further propose the spatial-aware online incremental attack (a.k.a. SPARK) that performs spatial-temporal sparse incremental perturbations online and makes the adversarial attack less perceptible.</strong></p><p>SPARK quickly converges(收敛) to very small losses within several iterations by considering historical incremental perturbations, making it much more efficient than basic attacks.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670035.pdf"><strong><code>Sparse Adversarial Attack via Perturbation Factorization</code></strong></a><br>This work studies the sparse adversarial attack, which aims to generate adversarial perturbations onto partial positions of one benign image. <strong>The sparse adversarial attack involves two challenges, i.e. where to perturb, and how to determine the perturbation magnitude.</strong></p><p>Many existing works determined the perturbed positions manually or heuristically, and then optimized the magnitude using a proper algorithm designed for the dense adversarial attack.</p><p><strong>We propose to factorize(分解) the perturbation at each pixel to the product of two variables, including the perturbation magnitude and one binary selection factor (0 or 1).</strong> One pixel is perturbed if its selection factor is 1, otherwise not perturbed.</p><p><strong>We formulate the sparse attack problem as a mixed integer programming (MIP) to jointly optimize the binary selection factors and continuous perturbation magnitudes</strong> of all pixels, with a cardinality constraint on selection factors to explicitly control the degree of sparsity.</p><p>The perturbation factorization provides the extra flexibility to incorporate other meaningful constraints on selection factors or magnitudes to achieve some desired performance, such as the group-wise sparsity or the enhanced visual imperceptibility. We develop an efficient algorithm by equivalently reformulating the MIP problem as a continuous optimization problem.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620120.pdf"><strong><code>Spatiotemporal Attacks for Embodied Agents</code></strong></a><br>Existing work on adversarial attacks have mainly focused on static scenes; however, it remains unclear whether such attacks are effective against embodied agents, which could navigate and interact with a dynamic environment.</p><p><strong>We take the first step to study adversarial attacks for embodied agents.</strong></p><p>We generate spatiotemporal(时间和空间的) perturbations to form 3D adversarial examples, which exploit the interaction history in both the temporal and spatial dimensions.</p><p>Regarding the temporal dimension, since agents make predictions based on historical observations, we develop a trajectory attention module to explore scene view contributions, which further help localize 3D objects appeared with highest stimuli(刺激).</p><p>By conciliating(顺应) with clues from the temporal dimension, along the spatial dimension, we adversarially perturb the physical properties (e.g., texture and 3D shape) of the contextual objects that appeared in the most important scene views.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680477.pdf"><strong><code>Square Attack: a query-eﬃcient black-box adversarial attack via random search</code></strong></a><br>We propose the Square Attack, a score-based black-box $l_2$- and $l_1$-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking.</p><p>Square Attack is based on a randomized search scheme which selects localized square shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set.</p><p>In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least 1:8 and up to 3 compared to the recent state-of-the-art l1-attack.</p><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460596.pdf"><strong><code>Targeted Attack for Deep Hashing based Retrieval</code></strong></a><br><strong>In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on deep hashing based retrieval.</strong></p><p>We first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived.</p><p>To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the $l_{\infty}$ restriction on the perturbation.</p><h2 id="ICLR"><a href="#ICLR" class="headerlink" title="ICLR"></a>ICLR</h2><h3 id="2019-10-papers"><a href="#2019-10-papers" class="headerlink" title="2019(10 papers)"></a>2019(10 papers)</h3><p><a href="https://openreview.net/pdf?id=r1lWUoA9FQ"><strong><code>Are adversarial examples inevitable?</code></strong></a></p><p><strong>This paper analyzes adversarial examples from a theoretical perspective, and identifies fundamental bounds on the susceptibility of a classifier to adversarial attacks.</strong></p><p><strong>We show that, for certain classes of problems, adversarial examples are inescapable.</strong></p><p>we explore the implications of theoretical guarantees for real-world problems and discuss how factors such as dimensionality and image complexity limit a classifier’s robustness against adversarial examples.</p><p><a href="https://openreview.net/pdf?id=r1g4E3C9t7"><strong><code>Characterizing Audio Adversarial Examples Using Temporal Dependency</code></strong></a></p><p>Our results <strong>reveal the importance of using the temporal dependency in audio data to gain discriminate power against adversarial examples.</strong></p><p>The results suggest that (i)  input transformation developed from image adversarial defense provides limited robustness improvement and is subtle to advanced attacks; (ii) temporal dependency can be exploited to gain discriminative power against audio adversarial examples and is resistant to adaptive attacks considered in our experiments.</p><p><a href="https://openreview.net/pdf?id=BygANhA9tQ"><strong><code>Cost-Sensitive Robustness against Adversarial Examples</code></strong></a></p><p>These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications.</p><p>For some tasks, there are some adversarial transformation are more important than others, <strong>we propose cost-sensitive robustness as the criteria for measuring the classifier’s performance</strong> for these tasks.</p><p>We <strong>encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt a robust training method</strong>  to optimize for cost-sensitive robustness.</p><p><a href="https://openreview.net/pdf?id=ryetZ20ctX"><strong><code>Defensive Quantization: When Efficiency Meets Robustness --Han Song</code></strong></a></p><p><strong>We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.</strong></p><p>We first conduct an empirical study to show that <strong>vanilla quantization suffers more from adversarial attacks</strong>. We observe that <strong>the inferior robustness comes from the error amplification effect</strong>, where the quantization operation further enlarges the distance caused by amplified noise.</p><p>Then <strong>we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization</strong>, such that the magnitude of the adversarial noise remains non-expansive during inference.</p><p><a href="https://openreview.net/pdf?id=HyGIdiRqtm"><strong><code>Evaluating Robustness of Neural Networks with Mixed Integer Programming</code></strong></a></p><p><strong>We formulate verification of piecewise-linear neural networks as a mixed integer program.</strong> On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art.</p><p>We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available.</p><p>we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded $l_{\infty}$ = 0:1.</p><p><a href="https://openreview.net/pdf?id=BkfbpsAcF7"><strong><code>Excessive Invariance Causes Adversarial Vulnerability</code></strong></a></p><p><strong>We decompose these errors caused by distribution shift into two complementary sources: sensitivity and invariance.</strong></p><p>We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from $\epsilon$-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. (作者认为网络对与任务无关的输入的变化过于敏感, 而且对与任务相关的输入的变化过于不变, 这2个原因是导致网络在输入空间中脆弱的原因)</p><p>We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an informationtheoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.</p><p><a href="https://openreview.net/pdf?id=BkMiWhR5K7"><strong><code>Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors</code></strong></a></p><p>We introduce a framework that conceptually unifies much of the existing work on black-box attacks.</p><p>Despite the current SOTA methods are optimal, <strong>we show how to improve black-box attacks by bringing a new element into the problem: gradient priors.</strong></p><p>We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples.</p><p><a href="https://openreview.net/pdf?id=BkgzniCqY7"><strong><code>Structured Adversarial Attack: Towards General Implementation and Better Interpretability</code></strong></a></p><p>$l_p$ norm of the added perturbation is usually used to measure the similarity between original image and adversarial example, but s<strong>uch adversarial attacks perturbing the raw input spaces may fail to capture structural information hidden in the input.</strong></p><p><strong>The paper explores group sparsity in adversarial perturbations by sliding a mask through images aiming for extracting key spatial structures to develop a more general attack model.</strong></p><p>An ADMM (alternating direction method of multipliers)-based framework is proposed that can split the original problem into a sequence of analytically solvable subproblems and can be generalized to implement other attacking methods.</p><p>Strong group sparsity is achieved in adversarial perturbations even with the same level of $l_p$-norm distortion $(p \in {1, 2, \infty })$ as the SOTA attacks.</p><p><a href="https://openreview.net/pdf?id=HylTBhA5tQ"><strong><code>The Limitations of Adversarial Training and the Blind-Spot Attack</code></strong></a></p><p>We show that  <strong>the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network.</strong>(对抗训练的鲁棒性和测试点与训练数据的数据流形之间的距离有很大的关系)</p><p><strong>Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks</strong>(远离训练数据数据流形的测试样本会更容易被攻击)</p><p>Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the “blind-spot attack”, where the input images reside in “blind-spots” (low density regions) of the empirical distribution of training data but is still on the ground-truth data manifold.(在对抗训练这种防御方法可能受到盲点攻击, 其定义是虽然输入图像在 GT 数据流形上, 但可能已经远离了训练数据的分布, 所以也会被模型分类错误)</p><p><strong>For small datasets, these blind-spots can be easily found by simply scaling and shifting image pixel values. For large datasets, the blind-spots make defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data.</strong>(对一些小的数据集来说, 可以简单地利用缩放和移动这些像素的值来找到盲点, 但由于维度过高且对抗样本数据缺乏, 很难找到这些盲点. p.s. 生成对抗样本需要不少时间)</p><p>We find that blind-spots also exist on provable defenses because these trainable robustness certificates can only be practically optimized on a limited set of training data(我们发现在可证明的防御中也存在盲点，因为这些可训练的鲁棒性验证只能在有限的训练数据集上进行优化)</p><p><a href="https://openreview.net/pdf?id=S1EHOsC9tX"><strong><code>Towards the first adversarially robust neural network model on MNIST</code></strong></a></p><p>The widely used $L_\infty$ defense:</p><ol><li>has lower $L_0$ robustness than undefended networks and is still highly susceptible to $L_2$ perturbations</li><li>classifies unrecognizable images with high certainty</li><li>performs not much better than simple input binarization</li><li>features adversarial perturbations that make little sense to humans.</li></ol><p><strong>We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions.</strong></p><p>We demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.</p><h3 id="2020-26-papers"><a href="#2020-26-papers" class="headerlink" title="2020(26 papers)"></a>2020(26 papers)</h3><p><a href="https://openreview.net/pdf?id=SJxSDxrKDr"><strong><code>Adversarial Training and Provable Defense: Brigding The Gap</code></strong></a><br>We present COLT, a new method to train neural networks based on a novel combination of adversarial training and provable defenses.</p><p><strong>The key idea is to model neural network training as a procedure which includes both, the verifier and the adversary. In every iteration, the  verifier aims to certify the network using convex relaxation while the adversary tries to find inputs inside that convex relaxation which cause verification to fail.</strong></p><p><a href="https://openreview.net/pdf?id=Hkem-lrtvH"><strong><code>BayesOpt Adversarial Attack</code></strong></a><br>Current approaches relying on substitute model training, gradient estimation or genetic algorithms often require an excessive number of queries.</p><p><strong>We propose a query-efficient black-box attack which uses Bayesian optimisation in combination with Bayesian model selection to optimise over the adversarial perturbation and the optimal degree of search space dimension reduction.</strong></p><p><a href="https://openreview.net/pdf?id=r1xGnA4Kvr"><strong><code>Biologically Inspired Sleep Algorithm For In-Creased Generalization And Adversarial Robustness In Deep Neural Networks</code></strong></a><br>It has been hypothesized that sleep promotes generalization of knowledge and improves robustness against noise in  animals and humans.</p><p><strong>we utilize a biologically inspired sleep phase in ANNs and demonstrate the benefit of sleep on defending against adversarial attacks as well as in increasing ANN classification robustness.</strong></p><p><a href="https://openreview.net/pdf?id=SJxhNTNYwB"><strong><code>Black-box Adversarial Attack With Transferable Model-based Embedding</code></strong></a><br>Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model(以前的方法都使用梯度或者初始化来作为白盒模型的替代, 以这种方式把 transfer-based 和 socred-based 的方法联合起来), this new method <strong>tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network.</strong></p><p><strong>The method produces adversarial perturbations with high level semantic patterns that are easily transferable.</strong> We show that <strong>this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures.</strong></p><p><a href="https://openreview.net/pdf?id=HJxdTxHYvB"><strong><code>Breaking Certified Defenses: Semantic Adversarial Examples With Spoofed Robustness Certificates</code></strong></a><br><strong>We present a new attack that exploits not only the labelling function of a classifier, but also the certificate generator.</strong></p><p><strong>The proposed method applies large perturbations that place images far from a class boundary while maintaining the imperceptibility property of adversarial examples.</strong></p><p><a href="https://openreview.net/pdf?id=SJgwzCEKwH"><strong><code>Briding Mode Connectivity In Loss Landscapes And Adversarial Robustness</code></strong></a><br><strong>We propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness.</strong></p><p>When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide (诚意) data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models.</p><p>We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models. A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided.</p><p><a href="https://openreview.net/pdf?id=Skgy464Kvr"><strong><code>Detecting And Diagnosing Adversarial Images With Class-Conditional Capsule Reconstructions</code></strong></a><br><strong>We first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input.</strong></p><p>To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate.</p><p><strong>We find that CapsNets always perform better thanconvolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class.</strong></p><p>Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.</p><p><a href="https://openreview.net/pdf?id=HJem3yHKwH"><strong><code>EMPIR: Ensembles of Mixed Precision Deep Networks For Increased Robustness Against Adversarial Attacks</code></strong></a><br><strong>We propose EMPIR, ensembles of quantized DNN models with different numerical precisions</strong>, as a new approach to increase robustness against adversarial attacks.</p><p>EMPIR is <strong>based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks</strong>, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs.</p><p>EMPIR overcomes this limitation to achieve the “best of both worlds”, i.e., <strong>the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble.</strong></p><p>Further, as low precision DNN models have significantly lower computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (&lt;25% in our evaluations).</p><p><a href="https://openreview.net/pdf?id=Skgvy64tvr"><strong><code>Enhancing Adversarial Defense by k-Winners-Take-All</code></strong></a><br>Instead of using popular activation functions (such as ReLU), <strong>we advocate the use of k-Winners-Take-All (k-WTA) activation, a $C^0$ discontinuous activation function that purposely invalidates the neural network model’s gradient at densely distributed input data points.</strong></p><p><strong>The proposed k-WTA activation can be readily used in nearly all existing networks and training methods with no significant overhead.</strong> Our proposal is theoretically rationalized.</p><p><strong>We analyze why the discontinuities in k-WTA networks can largely prevent gradient-based search of adversarial examples and why they at the same time remain innocuous to the network training.</strong></p><p><a href="https://openreview.net/pdf?id=BkgWahEFvr"><strong><code>Enhancing Transformation-based Defenses Against Adversarial Attacks With A Distribution Classifier</code></strong></a><br>Stochastic input transformation methods have been proposed, where the idea is to recover the image from adversarial attack by random transformation, and to take the majority vote as consensus(一致的) among the random samples. <strong>However, the transformation improves the accuracy on adversarial images at the expense of the accuracy on clean images.</strong></p><p><strong>We study the distribution of softmax induced by stochastic transformations. We observe that with random transformations on the clean images, although the mass of the softmax distribution could shift to the wrong class, the resulting distribution of softmax could be used to correct the prediction.</strong></p><p><strong>On the adversarial counterparts, with the image transformation, the resulting shapes of the distribution of softmax are similar to the distributions from the clean images.</strong></p><p>With these observations, we propose a method to improve existing transformation-based defenses. <strong>We train a separate lightweight distribution classifier to recognize distinct features in the distributions of softmax outputs of transformed images.</strong></p><p><a href="https://openreview.net/pdf?id=HyxJhCEFDS"><strong><code>Intriguing Properties Of Adversarial Training At Scale</code></strong></a><br>We provide the first rigorous study on diagnosing elements of large-scale adversarial training on ImageNet, which reveals two intriguing properties.</p><p><strong>First, we study the role of normalization, Second, we study the role of network capacity.</strong></p><p>Batch Normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but <strong>we show it may prevent networks from obtaining strong robustness in adversarial training.</strong></p><p>One unexpected observation is that, <strong>for models trained with BN, simply removing clean images from training data largely boosts adversarial robustness</strong>, <strong>We relate this phenomenon to the hypothesis that clean images and adversarial images are drawn from two different domains.</strong> This two-domain hypothesis may explain the issue of BN when training with a mixture of clean and adversarial images, as estimating normalization statistics of this mixture distribution is challenging.</p><p>Guided by this two-domain hypothesis, we show disentangling the mixture distribution for normalization, i.e., <strong>applying separate BNs to clean and adversarial images for statistics estimation, achieves much stronger robustness</strong>. Additionally, we find that enforcing BNs to behave consistently at training and testing can further enhance robustness.</p><p>We find our so-called “deep” networks are still shallow for the task of adversarial learning. Unlike traditional classification tasks where accuracy is only marginally improved by adding more layers to “deep” networks (e.g., ResNet-152), adversarial training exhibits a much stronger demand on deeper networks to achieve higher adversarial robustness. This robustness improvement can be observed substantially and consistently even by pushing the network capacity to an unprecedented scale.</p><p><a href="https://openreview.net/pdf?id=rJx1Na4Fwr"><strong><code>MACER: Attack-Free And Scalable Robust Training via Maximizing Certified Radius</code></strong></a><br>We propose the MACER algorithm, which <strong>learns robust models without using adversarial training but performs better than all existing provable $l_2$-defenses.</strong></p><p><a href="http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf">Recent work</a> <strong>shows that randomized smoothing can be used to provide a certified $l_2$ radius to smoothed classifiers, and our algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER). The attack-free characteristic makes MACER faster to train and easier to optimize.</strong></p><p><a href="https://openreview.net/pdf?id=ByxtC2VtPB"><strong><code>Mixup Inference: Better Exploiting Mixup To Defend Adversarial Attacks --Jun Zhu</code></strong></a><br><strong>Applying mixup in training provides an effective mechanism to</strong> improve generalization performance and <strong>model robustness</strong> against adversarial perturbations, <strong>which introduces the globally linear behavior in-between training examples.</strong></p><p>But <strong>the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited.</strong>  Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions.(由于对抗扰动有局部性, 利用模型预测的全局性能更有效地破坏对抗扰动的局部性, 从而提升鲁棒性)</p><p>Inspired by simple geometric intuition, <strong>we develop an inference principle, named mixup inference (MI), for mixup-trained models.</strong> <strong>MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial.</strong>(我没理解错的话就是把一些 adversarial examples 和一些 clean images 给 mixup 起来做 adversarial training……)</p><p><a href="https://openreview.net/pdf?id=HkeryxBtPB"><strong><code>MMA Training: Direct Input Space Margin Maximization Through Adversarial Training</code></strong></a><br>We study adversarial robustness of neural networks from a margin maximization perspective, where margins are defined as the distances from inputs to a classifier’s decision boundary.(从最大边缘的角度来看待鲁棒性, 这里的边缘是指输入数据到决策边界的距离)</p><p><strong>Our study shows that maximizing margins can be achieved by minimizing the adversarial loss</strong> on the decision boundary at the “shortest successful perturbation”,** demonstrating a close connection between adversarial losses and the margins.</p><p><strong>We propose Max-Margin Adversarial (MMA) training to directly maximize the margins to achieve adversarial robustness.</strong> Instead of adversarial training with a fixed $\epsilon$, <strong>MMA offers an improvement by enabling adaptive selection of the “correct” $\epsilon$ as the margin individually for each data point.</strong></p><p><strong>In addition, we rigorously analyze adversarial training with the perspective of margin maximization, and provide an alternative interpretation for adversarial training, maximizing either a lower or an upper bound of the margins.</strong></p><p><a href="https://openreview.net/pdf?id=SJlHwkBYDH"><strong><code>Nesterov Accelerated Gradient And Scale Invariance For Adversarial Attacks</code></strong></a><br><strong>Under the blackbox setting, most existing adversaries often have a poor transferability to attack other defense models.</strong></p><p>In this work, <strong>from the perspective of regarding the adversarial example generation as an optimization process, we propose two new methods to improve the transferability</strong> of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM).</p><p>NI-FGSM aims to adapt Nesterov accelerated gradient into the iterative attacks so as to effectively look ahead and improve the transferability of adversarial examples.</p><p>SIM is based on our discovery on the scale-invariant property of deep learning models, for which we leverage to optimize the adversarial perturbations over the scale copies of the input images so as to avoid “overfitting” on the white-box model being attacked and generate more transferable adversarial examples.</p><p><a href="https://openreview.net/pdf?id=r1lF_CEYwS"><strong><code>On The Need For Topology-aware Generative Models For Manifold-based Defenses</code></strong></a><br><strong>One of the important classes of defenses are manifoldbased defenses, where a sample is “pulled back” into the data manifold before classifying. These defenses rely on the assumption that data lie in a manifold of a lower dimension than the input space. These defenses use a generative model to approximate the input distribution.</strong></p><p><strong>We prove that the generative models used in manifold-based defenses need to be topology-aware.</strong></p><p><a href="https://openreview.net/pdf?id=SyevYxHtDB"><strong><code>Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks</code></strong></a><br>Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks.(现在的针对偷模型的防御是很被动的, 例如拒绝攻击者不断地利用模型的预测, 但这种方法对抗偷模型的效果并不好)</p><p>In this paper, <strong>we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker.</strong></p><p><strong>Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries</strong></p><p><a href="https://openreview.net/pdf?id=rklk_ySYPB"><strong><code>Provable Robustness Against All Adversarial $l_p$-Perturbations For p ≥ 1</code></strong></a><br>Often seemingly robust models turn out to be non-robust when more sophisticated attacks are used. One way out of this dilemma are provable robustness guarantees.</p><p>While provably robust models for specific $l_p$-perturbation models have been developed, we show that they do not come with any guarantee against other $l_q$-perturbations.</p><p>We propose a new regularization scheme, MMR-Universal, for ReLU networks which enforces robustness wrt $l_1-$ and $l_{\infty}-$ perturbations and show how that leads to the first provably robust models wrt any $l_p \space norm$ for $p$ ≥ 1.</p><p><a href="https://openreview.net/pdf?id=Skxd6gSYDS"><strong><code>Query-Efficient Meta Attack To Deep Neural Networks</code></strong></a><br><strong>We propose a meta attack approach that is capable of attacking a targeted model with much fewer queries. Its high queryefficiency stems from effective utilization of meta learning approaches in learning generalizable prior abstraction from the previously observed attack patterns and exploiting such prior to help infer attack patterns from only a few queries and outputs.</strong></p><p><a href="https://openreview.net/pdf?id=H1lZJpVFvr"><strong><code>Robust Local Features For Improving The Generalization of Adversarial Training</code></strong></a><br>Adversarially trained models often lack adversarially robust eneralization on unseen testing data. <strong>Recent works show that adversarially trained models are more biased towards global structure features.</strong>(经过对抗训练得到的模型通常在未知数据上缺少泛化性, 最近的工作表明对抗训练得到的模型更偏向于学习全局的结构特征)</p><p><strong>We would like to investigate the relationship between the generalization of adversarial training and the robust local features, as the robust local features generalize well for unseen shape variation.</strong>(想研究对抗训练的泛化性和局部鲁棒特征之间的关系, 因为局部鲁棒性很好地概括了看不见的形状变化)</p><p>To learn the robust local features, we develop a Random Block Shuffle (RBS) transformation to <strong>break up the global structure features on normal adversarial examples</strong>.(首先对正常的对抗样本做一个随机阻塞变化以破坏其全局结构特征)</p><p>We continue to propose a new approach called Robust Local Features for Adversarial Training (RLFAT), which first <strong>learns the robust local features by adversarial training on the RBS-transformed adversarial examples, and then transfers the robust local features into the training of normal adversarial examples.</strong>(然后从被破坏了全局结构特征的对抗样本中提取局部鲁棒特征, 再把这些对抗样本恢复为正常的对抗样本)</p><p><a href="https://openreview.net/pdf?id=SklTQCNtvS"><strong><code>SIGN-OPT: A Query-Efficient Hard-Label Adversarial Attack</code></strong></a><br>Several algorithms have been proposed for <strong>hard-label black-box</strong> attack but they typically require huge amount (&gt;20,000) of queries for attacking one example.</p><p><strong><a href="https://openreview.net/forum?id=rJlk6iRqKX">Previous work </a>showed that hard-label attack can be modeled as an optimization problem</strong> where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied.</p><p><strong>We adopt the same optimization formulation but propose to directly estimate the sign of gradient at any direction instead of the gradient itself</strong>, which enjoys the benefit of single query.</p><p>Using this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack.</p><p><a href="https://openreview.net/pdf?id=Skxuk1rFwB"><strong><code>Towards Stable And Efficient Training of Verifiably Robust Neural Networks</code></strong></a><br>Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures.(几种现有方法在扰动下利用基于线性松弛的神经网络输出范围, 但根据网络结构的不同, 训练速度会下降很多倍)</p><p>Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at the beginning of training.(基于 “间隔边界传播” 的方法相比于基于线性松弛的方法而言有效且显著地提高了效果, 但这种方法容易在训练开始时容易遇到稳定性问题.)</p><p>We propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass and a tight linear relaxation based bound, CROWN, in a backward bounding pass.(我们提出的方法在于联合前向边界传递中的 “间隔边界传播” 和在反向边界传递中的 “严格线性松弛”)</p><p>CROWN-IBP is computationally efficient and consistently outperforms IBP baselines on training verifiably robust neural networks.</p><p><a href="https://openreview.net/pdf?id=rJxAo2VYwr"><strong><code>Transferable Perturbations of deep feature Distributions</code></strong></a></p><p>This work <strong>presents a new adversarial attack based on the modeling and exploitation of class-wise and layer-wise deep feature distributions.</strong></p><p><strong>Our methodology affords an analysis of how adversarial attacks change the intermediate feature distributions of CNNs, as well as a measure of layer-wise and class-wise feature distributional separability/entanglement.</strong>(我们的方法对对抗攻击如何改变中间特征的分布进行了研究, 这也可以看作是对 layer-wise 和 class-wise 特征的可分离性和纠缠度的一种度量)</p><p>We also conceptualize a transition from task/data-specific to model-specific features within a CNN architecture that directly impacts the transferability of adversarial examples.(概念化了一种转换, 这种转换把特征从 task/data-specific 转换为 model-specific, 这样直接提高了对抗样本的迁移性)</p><p><a href="https://openreview.net/pdf?id=rJgzzJHtDB"><strong><code>Triple Wins: Boosting Accuracy, Robustness And Efficiency Together by Enabling Input- Adaptive Inference</code></strong></a><br>Models cannot both have good accuracy and robustness is shown to be rooted in the inherently higher sample complexity and/or model capacity, for learning a high-accuracy and robust classifier.</p><p>In view of that, give a classification task, <strong>growing the model capacity appears to help draw a win-win between accuracy and robustness</strong>, yet at the expense of model size and latency, therefore posing challenges for resource-constrained applications.</p><p><strong>This paper studies multi-exit networks associated with input-adaptive efficient inference, showing their strong promise in achieving a “sweet point” in cooptimizing model accuracy, robustness and efficiency.</strong></p><p>Our method allows for each input (either clean or adversarial) to adaptively choose one of the multiple output layers (early branches or the final one) to output its prediction. That multi-loss adaptivity adds new variations and flexibility to adversarial attacks and defenses, on which we present a systematical investigation.</p><p><a href="https://openreview.net/pdf?id=B1gX8kBtPr"><strong><code>Universal Approximation With Certified Networks</code></strong></a></p><p>We prove that for every continuous function $f$, there exists a network $n$ such that: (i) $n$ approximates $f$ arbitrarily close, and (ii) simple interval bound propagation of a region $B$ through $n$ yields a result that is arbitrarily close to the optimal output of $f$ on $B$.(一个区域 $B$ 经过网络 $n$ 能得到近似 $B$ 在函数 $f$ 上的最佳结果)</p><p><strong>Our result can be seen as a Universal Approximation Theorem for interval-certified ReLU networks.</strong> To the best of our knowledge, this is <strong>the first work to prove the existence of accurate, interval-certified networks.</strong></p><p><a href="https://openreview.net/pdf?id=Sye_OgHFwH"><strong><code>Unrestricted Adversarial Examples via Smantic Manipulations</code></strong></a></p><p>We instead introduce “unrestricted” perturbations that <strong>manipulate semantically meaningful image-based visual descriptors – color and texture – in order to generate effective and photorealistic adversarial examples.</strong></p><p>We show that <strong>these semantically aware perturbations are effective against JPEG compression, feature squeezing and adversarially trained model.</strong> We also show that <strong>the proposed methods can effectively be applied to both image classification and image captioning tasks on complex datasets</strong></p><h2 id="ICML"><a href="#ICML" class="headerlink" title="ICML"></a>ICML</h2><h3 id="2019-17-papers"><a href="#2019-17-papers" class="headerlink" title="2019(17 papers)"></a>2019(17 papers)</h3><p><a href="http://proceedings.mlr.press/v97/bojchevski19a/bojchevski19a.pdf"><strong><code>Adversarial Attacks on Node Embeddings via Graph Poisoning</code></strong></a></p><p>We <strong>provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks</strong>. We <strong>derive efficient adversarial perturbations</strong> that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks.</p><p><a href="http://proceedings.mlr.press/v97/li19j/li19j.pdf"><strong><code>Adversarial camera stickers: A physical camera-based attack on deep learning systems</code></strong></a></p><p>We show that by placing a carefully crafted and mainly-translucent sticker over the lens of a camera, one can create universal perturbations of the observed images that are inconspicuous, yet misclassify target objects as a different (targeted) class.(就是用个东西挡住相机的一部分, 这样也能产生对抗样本)</p><p>To accomplish this, <strong>we propose an iterative procedure for both updating the attack perturbation (to make it adversarial for a given classifier), and the threat model itself (to ensure it is physically realizable)</strong></p><p><a href="http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf"><strong><code>Adversarial Examples Are a Natural Consequence of Test Error in Noise</code></strong></a></p><p>We establish close connections between the adversarial robustness and corruption robustness research programs, with the strongest connection in the case of additive Gaussian noise.(为图像添加高斯模糊这种 corruption(图像损坏) 的方法也能误导分类器, 这篇文章想说明 adversarial robustness 和 corruption robustness 有很紧密的联系…尤其是在加高斯噪声的时候)</p><p><strong>This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions.</strong></p><p>Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as ImageNet-C.</p><p><a href="http://proceedings.mlr.press/v97/bubeck19a/bubeck19a.pdf"><strong><code>Adversarial examples from computational constraints</code></strong></a></p><p><strong>We show that classifiers in high dimension are vulnerable to “adversarial” perturbations due to the computational constraints instead of information theoretic limitations</strong>.(有高维度的分类器容易被攻击是因为计算限制而不是信息理论的局限性)</p><p>We prove that for classification tasks, the robust model can be found by a  exponential-time algorithm with relatively few training examples. (我们证明了对分类来说, 鲁棒的分类器可以在指数时间内以较少的训练样本来找到)</p><p>Then construct two classification tasks where learning a robust classifier is computationally intractable.More precisely we construct two binary classifications task in high dimensional space which are (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (nonrobustly) by a simple linear separator, (iii) yet are not efficiently robustly learnable, even for small perturbations.</p><p>The results suggest that <strong>adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms.</strong></p><p><a href="http://proceedings.mlr.press/v97/li19a/li19a.pdf"><strong><code>Are Generative Classifiers More Robust to Adversarial Attacks?</code></strong></a></p><p><strong>Most recent work focuses on discriminative classifiers</strong>, which only model the conditional distribution of the labels given the inputs.</p><p><strong>We propose and investigate the deep Bayes classifier, which improves classical naive Bayes with conditional deep generative models</strong></p><p>Experimental results suggest that deep Bayes classifiers are more robust than deep discriminative classifiers.</p><p><a href="http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf"><strong><code>Certified Adversarial Robustness via Randomized Smoothing</code></strong></a><br><strong>We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $l_2$ norm, and we establish a close connection between $l_2$ robustness and Gaussian noise.</strong>(如何把在高斯噪声下分类效果好的分类器转化为在 $l_2$ norm 限制下有鲁棒性的新分类器, 也在鲁棒性和高斯噪声之间建立了紧密的联系)</p><p><a href="http://proceedings.mlr.press/v97/dohmatob19a/dohmatob19a.pdf"><strong><code>Generalized No Free Lunch Theorem for Adversarial Robustness</code></strong></a></p><p>We show that if conditioned on a class label the data distribution satisfies the W2 Talagrand transportation-cost inequality.</p><p><strong>Any classifier can be adversarially fooled with high probability once the perturbations are slightly greater than the natural noise level in the problem.</strong></p><p><a href="http://proceedings.mlr.press/v97/qin19a/qin19a.pdf"><strong><code>Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition</code></strong></a></p><p><strong>The adversarial examples to speech recognition have neither of these properties</strong>: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air.</p><p><strong>This paper makes advances on both of these fronts.</strong></p><p>First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets.</p><p>Next, we make progress towards physical-world over-the-air audio adversarial examples by constructing perturbations which remain effective even after applying realistic simulated environmental distortions.</p><p><a href="http://proceedings.mlr.press/v97/pang19a/pang19a.pdf"><strong><code>Improving Adversarial Robustness via Promoting Ensemble Diversity --Jun Zhu</code></strong></a></p><p><strong>This paper presents a new method that explores the interaction among individual networks to improve robustness for ensemble models.</strong></p><p>we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal predictions of individual members.(我们定义了集成多样性的新概念, 即个体成员的非最大值预测之间的多样性????????).</p><p>And present an adaptive diversity promoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examples difficult to transfer among individual members.</p><p>这篇文章定义了一个集成多样性的概念, 又提出了一个叫 ADP 的方法来鼓励集成多样性, 使得对抗样本难以在这些单个模型中迁移, 从而提高集成模型的鲁棒性.</p><p><a href="http://proceedings.mlr.press/v97/yang19e/yang19e.pdf"><strong><code>ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation</code></strong></a></p><p><strong>This paper proposes ME-Net, a defense method that leverages matrix estimation (ME).</strong></p><p>In ME-Net, images are preprocessed using two steps:</p><ol><li>pixels are randomly dropped from the image;</li><li>the image is reconstructed using ME.</li></ol><p><strong>We show that this process destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image.</strong></p><p><a href="http://proceedings.mlr.press/v97/li19g/li19g.pdf"><strong><code>NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks</code></strong></a></p><p>In this paper, we propose a black-box adversarial attack algorithm.</p><p><strong>Instead of searching for an “optimal” adversarial example for a benign input to a targeted DNN, our algorithm finds a probability density distribution over a small region centered around the input, such that a sample drawn from this distribution is likely an adversarial example, without the need of accessing the DNN’s internal layers or weights.</strong></p><p>Our approach is universal as it can successfully attack different neural networks by a single algorithm.</p><p>our results reveal that <strong>adversarial training remains one of the best defense techniques</strong>, and <strong>the adversarial examples are not as transferable across defended DNNs as them across vanilla DNNs</strong></p><p><a href="http://proceedings.mlr.press/v97/liu19h/liu19h.pdf"><strong><code>On Certifying Non-Uniform Bounds against Adversarial Attacks</code></strong></a></p><p><strong>This work studies the robustness certification problem of neural network models</strong>, which aims to find certified adversary-free regions as large as possible around data points.</p><p><strong>In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the decision boundary of neural network models.</strong></p><p>We formulate our target as an optimization problem with nonlinear constraints. Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits so that the relaxed problem can be solved by the augmented Lagrangian method.</p><p><strong>Experiments show the non-uniform bounds have larger volumes than uniform ones and the geometric similarity of the  non-uniform bounds gives a quantitative, data agnostic metric of input features’ robustness.</strong></p><p><a href="http://proceedings.mlr.press/v97/etmann19a/etmann19a.pdf"><strong><code>On the Connection Between Adversarial Robustness and Saliency Map Interpretability</code></strong></a></p><p><strong>Models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts.</strong></p><p><strong>We aim to quantify this behavior by considering the alignment between input image and saliency map.</strong></p><p>We hypothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models.</p><p>We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.</p><p><a href="http://proceedings.mlr.press/v97/moon19a/moon19a.pdf"><strong><code>Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization</code></strong></a></p><p>This paper <strong>proposes a method to significantly reduce the required queries</strong> compared to recently proposed methods.</p><p>Specifically, <strong>they propose an efficient discrete surrogate  (离散替代) to the optimization problem which does not require estimating the gradient and consequently becomes free of the first order update hyperparameters to tune.</strong></p><p><a href="https://arxiv.org/pdf/1902.10660.pdf"><strong><code>Robust Decision Trees Against Adversarial Examples</code></strong></a></p><p>In this paper, <strong>we show that tree based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees.</strong></p><p>Our method aims to optimize the performance under the worst case perturbation of input features, which leads to a max-min saddle point problem.</p><p>To make our approach practical and scalable, we propose efficient tree building algorithms by approximating the inner minimizer in this saddle point problem, and present efficient implementations for classical information gain based trees as well as state-of-the-art tree boosting models such as XGBoost.</p><p><a href="http://proceedings.mlr.press/v97/guo19a/guo19a.pdf"><strong><code>Simple Black-box Adversarial Attacks</code></strong></a></p><p>Propose a simple method to generate adversarial examples <strong>efficiently. (less query times)</strong></p><p>With only the mild assumption of continuous-valued confidence scores, our highly query-efficient algorithm utilizes the following simple iterative principle:  <strong>we randomly sample a vector from a predefined orthonormal basis and either add or subtract it to the target image.</strong></p><p>Despite its simplicity, the proposed method can be used for both untargeted and targeted attacks – resulting in previously unprecedented query efficiency in both settings.</p><p>We argue that our proposed algorithm should serve as a strong baseline for future black-box attacks, in particular because it is <strong>extremely fast</strong> and <strong>its implementation requires less than 20 lines of PyTorch code.</strong></p><p><a href="http://proceedings.mlr.press/v97/roth19a/roth19a.pdf"><strong><code>The Odds are Odd: A Statistical Test for Detecting Adversarial Examples</code></strong></a></p><p><strong>We investigate conditions under which test statistics exist that can reliably detect examples.</strong>(我们研究了能够可靠检测对抗样本的测试统计信息存在的条件)</p><p><strong>These statistics can be easily computed and calibrated by randomly corrupting inputs.</strong></p><p>They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints.Access to the log-odds is the only requirement to defend models.</p><p>We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective.</p><h3 id="2020-24-papers"><a href="#2020-24-papers" class="headerlink" title="2020(24 papers)"></a>2020(24 papers)</h3><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/1894-Paper.pdf"><strong><code>Adversarial Attacks on Copyright Detection Systems</code></strong></a><br>This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks.</p><p><strong>We describe a well-known music identification method and implement this system</strong> in the form of a neural net. We <strong>then attack this system using simple gradient methods and show that it is easily broken with white-box attacks.</strong></p><p>By scaling these perturbations up, we can create transfer attacks on industrial systems, such as the AudioTag copyright detector and YouTube’s Content ID system, using perturbations that are audible but significantly smaller than a random baseline.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/526-Paper.pdf"><strong><code>Adversarial Attacks on Probabilistic Autoregressive Forecasting Models</code></strong></a><br><strong>We develop an effective generation of adversarial attacks on neural models that output a sequence of probability distributions rather than a sequence of single values.</strong></p><p>The key technical challenge we address is effectively differentiating through the Monte-Carlo estimation of statistics of the joint distribution of the output sequence.</p><p>We extend prior work on probabilistic forecasting to the Bayesian setting which allows conditioning on future observations, instead of only on past observations.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/5336-Paper.pdf"><strong><code>Adversarial Nonnegative Matrix Factorization</code></strong></a><br>To overcome this limitation that existing NMF models are still vulnerable to adversarial attacks, <strong>we propose a novel Adversarial NMF (ANMF) approach in which an adversary can exercise some control over the perturbed data generation process.</strong></p><p>Different from the traditional NMF models which focus on either the regular input or certain types of noise, <strong>our model considers potential test adversaries that are beyond the pre-defined constraints, which can cope with various noises (or perturbations).</strong></p><p>We formulate the proposed model as a bilevel optimization problem and use Alternating Direction Method of Multipliers (ADMM) to solve it with convergence analysis.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2962-Paper.pdf"><strong><code>Adversarial Risk via Optimal Transport and Optimal Couplings</code></strong></a><br><strong>We investigate adversarial risk classifers from an optimal transport perspective.</strong></p><p><strong>We present a new and simple approach to show that the optimal adversarial risk for binary classifcation with 0 − 1 loss function is completely characterized by an optimal transport cost between the probability distributions of the two classes.</strong></p><p>Propose a novel coupling strategy that achieves the optimal transport cost for several univariate distributions like Gaussian, uniform, and triangular. Using the optimal couplings, we obtain the optimal adversarial classifers in these settings and show how they differ from optimal classifers in the absence of adversaries.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3863-Paper.pdf"><strong><code>Adversarial Robustness Against the Union of Multiple Perturbation Models</code></strong></a><br>While most work has defended against a single type of attack, recent work has looked at defending against multiple perturbation models using simple aggregations of multiple attacks(目前大多数防御方法只能防御一种攻击, 最近的工作是直接把多种攻击聚合起来做对抗训练以防止多种攻击), however,  these methods can be difficult to tune, and can easily result in imbalanced degrees of robustness to individual perturbation models, resulting in a sub-optimal worst-case loss over the union.(这些方法可能难以调整，并且很容易导致各个扰动模型的鲁棒性程度不平衡，从而导致在联合中出现次优的最坏情况损失。)</p><p><strong>We develop a natural generalization of the standard PGD-based procedure to incorporate(合并) multiple perturbation models into a single attack, by taking the worst-case over all steepest descent directions.</strong></p><p>This approach has the advantage of directly converging upon a trade-off between different perturbation models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against $ℓ_∞$, $ℓ_2$, and $ℓ_1$ attacks, outperforming past approaches.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/4379-Paper.pdf"><strong><code>Adversarial Robustness for Code</code></strong></a><br><strong>Explore that code system</strong>(including fnding and fxing bugs, code completion, decompilation, malware detection, type inference) <strong>are vulnerable to adversarial attacks</strong> by:</p><ol><li>instantiating adversarial attacks for code (a domain with discrete and highly structured inputs)</li><li>showing that, similar to other domains, neural models for code are vulnerable to adversarial attacks</li><li>developing a set of novel techniques that enable training robust and accurate models of code.</li></ol><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/520-Paper.pdf"><strong><code>Adversarial Robustness via Runtime Masking and Cleansing</code></strong></a><br>We raise a fundamental question—<strong>do we have to trade off natural generalization for adversarial robustness?</strong></p><p><strong>We argue that adversarial training is to employ confident adversarial data for updating the current model.</strong></p><p>We propose a novel formulation of friendly adversarial training (FAT): <strong>rather than employing most adversarial data maximizing the loss, we search for least adversarial data (i.e., friendly adversarial data) minimizing the loss</strong>, among the adversarial data that are confidently misclassified.(在这些最容易被分类错误的 adversarial data 中找到最少的数据, 使其最小化 loss 而不是用最多的数据来最大化 loss, 并以此进行对抗训练)</p><p>Our novel formulation is easy to implement by just stopping the most adversarial data searching algorithms such as PGD (projected gradient descent) early, which we call early-stopped PGD.</p><p>Theoretically, FAT is justified by an upper bound of the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier question negatively—adversarial robustness can indeed be achieved without compromising the natural generalization.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/5833-Paper.pdf"><strong><code>Black-box Certiﬁcation and Learning under Adversarial Perturbations</code></strong></a><br><strong>We analyze a PAC type framework of semi-supervised learning and identify possibility and impossibility results for proper learning of VC-classes in this setting.</strong></p><p>We further introduce and study a new setting of blackbox certification under limited query budget. We analyze this for various classes of predictors and types of perturbation.</p><p>We also consider the viewpoint of a black-box adversary that aims at finding adversarial examples, showing that the existence of an adversary with polynomial query complexity implies the existence of a robust learner with small sample complexity.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/4835-Paper.pdf"><strong><code>Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks</code></strong></a><br>Our confidencecalibrated adversarial training (CCAT) tackles this problem(robustness does not generalize to previously unseen threat models) by <strong>biasing the model towards low confidence predictions on adversarial examples. By allowing to reject examples with low confidence, robustness generalizes beyond the threat model employed during training.</strong></p><p>CCAT, trained only on $L_1$ adversarial examples, increases robustness against larger $L_{\infty}$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/6130-Paper.pdf"><strong><code>Efficiently Learning Adversarially Robust Halfspaces with Noise</code></strong></a><br><strong>We study the problem of learning adversarially robust halfspaces in the distribution-independent setting.</strong>(我们研究在分布独立的情况下学习对抗性鲁棒半空间的问题)</p><p>In the realizable setting, we provide necessary and sufficient conditions on the adversarial perturbation sets under which halfspaces are efficiently  robustly learnable.</p><p>In the presence of random label noise, we give a simple computationally  efficient algorithm for this problem with respect to any $ℓ_p$-perturbation.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/5465-Paper.pdf"><strong><code>Fundamental Tradeoffs between Invariance and  Sensitivity to Adversarial Perturbations</code></strong></a><br><strong>This paper studies a complementary failure mode, invariance-based adversarial examples, that introduce minimal semantic changes that modify an input’s true label yet preserve the model’s prediction.</strong></p><p>We show that defenses against sensitivity-based attacks actively harm a model’s accuracy on invariance-based attacks, and that new approaches are needed to resist both attack types.</p><p>Finally, we formally show that the existence of excessively invariant classifiers arises from the presence of overly-robust predictive features in standard datasets.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/381-Paper.pdf"><strong><code>Implicit Euler Skip Connections: Enhancing Adversarial Robustness via Numerical Stability</code></strong></a><br><strong>We try to address such an issue that CNNs are vulnerable to adversarial examples from the perspective of dynamic system in this work.</strong></p><p>By viewing ResNet as an explicit Euler discretization of an ordinary differential equation (ODE), for the frst time, we fnd that the adversarial robustness of ResNet is connected to the numerical stability of the corresponding dynamic system, i.e., more stable numerical schemes may correspond to more robust deep networks(通过把 ResNet 看成是一个常微分方程的显式欧拉离散化来和动态系统的数值稳定性联系起来)</p><p>Furthermore, inspired by the implicit Euler method for solving numerical ODE  problems, we propose Implicit Euler skip connections (IE-Skips) by modifying the original skip connection in ResNet or its variants.(用数学的方法来解决…)</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/5124-Paper.pdf"><strong><code>Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization</code></strong></a><br>We develop a notion of representation vulnerability that captures the maximum change of mutual(相互的) information between the input and output distributions, under the worst-case input perturbation.</p><p>We prove a theorem that establishes a lower bound on the minimum adversarial risk that can be achieved for any downstream classifier based on its representation vulnerability.</p><p><strong>We propose an unsupervised learning method for obtaining intrinsically robust representations by maximizing the worst-case mutual information between the input and output distributions.</strong></p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/6735-Paper.pdf"><strong><code>Minimally Distorted Adversarial Examples with a Fast Adaptive Boundary Attack</code></strong></a><br><strong>We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in {1, 2, \infty}$ aiming at finding the minimal perturbation necessary to change the class of a given input.</strong></p><p>It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run).</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/5885-Paper.pdf"><strong><code>Neural Network Control Policy Verification with Persistent Adversarial Perturbations</code></strong></a><br><strong>We show how to combine recent works on static neural network certification tools with robust control theory to certify a neural network policy in a control loop.</strong></p><p>We give a sufficient condition and an algorithm to ensure that the closed loop state and control constraints are satisfied when the persistent adversarial perturbation is $l_{\infty}$ norm bounded.</p><p>Our method is based on finding a positively invariant set of the closed loop dynamical system, and thus we do not require the continuity of the neural network policy. We also develop an effective attack strategy for neural network control systems that outperforms exhaustive Monte-Carlo search significantly.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2236-Paper.pdf"><strong><code>On Breaking Deep Generative Model-based Defenses and Beyond</code></strong></a><br>These generative-based defense often predict by inverting the deep generative models rather than simple feedforward propagation. Such defenses are difficult to attack due to obfuscated gradient.</p><p><strong>We develop a new gradient approximation attack to break these defenses. The idea is to view the inversion phase as a dynamical system, through which we extract the gradient w.r.t the input by tracing its recent trajectory.</strong>(轨道)</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/1661-Paper.pdf"><strong><code>Proper Network Interpretability Helps Adversarial Robustness in Classiﬁcation</code></strong></a><br>We theoretically show that with a proper measurement of interpretation, it is actually difficult to prevent prediction-evasion adversarial attacks from causing interpretation discrepancy(差异), (利用一个合适的可解释性措施, 很难防止 prediction-evasion 攻击引起的可解释性差异)</p><p><strong>We develop an interpretability-aware defensive scheme built only on promoting robust interpretation (without the need for resorting to adversarial loss minimization</strong></p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2479-Paper.pdf"><strong><code>Randomization matters  How to defend against strong adversarial attacks</code></strong></a><br><strong>We present the adversarial attacks and defenses problem as an infinite zero-sum game where classical results do not apply.</strong></p><p>We demonstrate the nonexistence of a Nash equilibrium in our game when the classifier and the Adversary are both deterministic, hence giving a negative answer to the above question in the deterministic regime.</p><p>We tackle this problem by showing that, under mild conditions on the dataset distribution, any deterministic classifier can be outperformed by a randomized one. This gives arguments for using randomization, and leads us to a simple method for building randomized classifiers that are robust to state-or-the-art adversarial attacks.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/6846-Paper.pdf"><strong><code>Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks</code></strong></a><br><strong>We first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function.</strong></p><p>We then <strong>combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness</strong></p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2933-Paper.pdf"><strong><code>Second-Order Provable Defenses against Adversarial Attacks</code></strong></a><br><strong>Computing exact robustness certificates for neural networks is difficult in general since it requires solving a non-convex optimization.</strong></p><p><strong>We provide computationally-efficient robustness certificates</strong> for neural networks with differentiable activation functions in two steps:</p><ol><li>We show that if the eigenvalues of the Hessian of the network (curvatures of the network) are bounded (globally or locally), we can compute a robustness certificate in the $l_2$ norm efficiently using convex optimization.</li><li>We derive a computationally-efficient differentiable upper bound on the curvature of a deep network.</li></ol><p>We also use the curvature bound as a regularization term during the training of the network to boost its certified robustness.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3175-Paper.pdf"><strong><code>Sharp Statistical Guarantees for Adversarially Robust Gaussian Classification</code></strong></a><br><strong>We provide the first result of the optimal minimax guarantees for the excess risk for adversarially robust classification, under Gaussian mixture model</strong></p><p>The results are stated in terms of the Adversarial Signal-toNoise Ratio (AdvSNR), which generalizes a similar notion for standard linear classification to the adversarial setting.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/724-Paper.pdf"><strong><code>Stronger and Faster Wasserstein Adversarial Attacks</code></strong></a><br>Compared to $l_p$ norm metric,  Wasserstein distance, which takes geometry in pixel space into account, has long known to be a better metric for measuring image quality and has recently risen as a compelling alternative to the $l_p$ metric in adversarial attacks.</p><p>However, c<strong>onstructing an effective attack under the Wasserstein metric is computationally much more challenging and calls for better optimization algorithms.</strong></p><p>We address this gap in two ways:</p><ol><li>we develop an exact yet efficient projection operator to enable a stronger projected gradient attack</li><li>we show for the first time that Frank-Wolfe method equipped with a suitable linear minimization oracle works extremely fast under Wasserstein constraints</li></ol><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/1310-Paper.pdf"><strong><code>Towards Understanding the Dynamics of the First-Order Adversaries</code></strong></a><br><strong>We analyze the dynamics of the maximization step towards understanding the experimentally observed effectiveness of this defense mechanism.</strong></p><p>Specifically, we investigate the non-concave landscape of the adversaries for a two-layer neural network with a quadratic loss. Our main result proves that projected gradient ascent finds a local maximum of this non-concave problem in a polynomial number of iterations with high probability.</p><p>Moreover, our analysis demonstrates that, in the initial phase of adversarial training, the scale of the inputs matters in the sense that a smaller input scale leads to faster convergence of adversarial training and a “more regular” landscape.</p><p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/1057-Paper.pdf"><strong><code>Towards Understanding the Regularization of Adversarial Robustness on Neural Networks</code></strong></a></p><p>$\epsilon$-adversarially robust (AR) is observed that such methods would lead to standard performance degradation, i.e., the degradation on natural examples.</p><p><strong>In this work, we study the degradation through the regularization perspective.</strong></p><p>We identify quantities from generalization analysis of NNs; with the identifed quantities we empirically fnd that AR is achieved by regularizing/biasing NNs towards less confdent solutions by making the changes in the feature space (induced by changes in the instance space) of most layers smoother uniformly in all directions; so to a certain extent, it prevents sudden change in prediction w.r t. perturbations.</p><p>Our studies suggest that one might consider ways that build AR into NNs in a gentler way to avoid the problematic regularization.</p>]]></content>
      
      
      <categories>
          
          <category> Adversarial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Adversarial </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Weakly Supervised Object Detection</title>
      <link href="2020/09/01/Weakly-Supervised-Object-Detection/"/>
      <url>2020/09/01/Weakly-Supervised-Object-Detection/</url>
      
        <content type="html"><![CDATA[<h1 id="Weakly-Supervised-Deep-Detection-Networks-CVPR’16"><a href="#Weakly-Supervised-Deep-Detection-Networks-CVPR’16" class="headerlink" title="Weakly Supervised Deep Detection Networks CVPR’16"></a><a href="http://arxiv.org/abs/1511.02853">Weakly Supervised Deep Detection Networks CVPR’16</a></h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h3><p>The definition of weakly supervised object detection (WSOD) is using only image-level labels(e.g. category) to learn object detectors</p><h3 id="Existing-problems"><a href="#Existing-problems" class="headerlink" title="Existing problems"></a>Existing problems</h3><ol><li>WSOD algorithms are not good enough.</li><li>Existing models are not end-to-end.</li></ol><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>By the hypothesis that since pre-trained CNNs generalize so well tto a large number of tasks, they should contain meaningful representation of the data. Hence, CNNs trained for image classification may already contain implicitly most of the information required to  perform object detection.</p><h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h3><p>Proposed the first end-to-end method for WSOD</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>The overall idea consists of three steps:</p><ol><li>Pre-train a CNN on a large-scale classification dataset. In this paper, using ImageNet ILSVRC 2012.</li><li>Construct the Weakly Supervised Deep Detection Networks(WSDNN) as an architectural modification of this CNN.</li><li>Fine-tune the WSDNN on the target dataset, using only image-level annotations.</li></ol><h3 id="Weakly-supervised-deep-detection-network"><a href="#Weakly-supervised-deep-detection-network" class="headerlink" title="Weakly supervised deep detection network"></a>Weakly supervised deep detection network</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Weakly-Supervised-Object-Detection/WSDDN_pipeline.png"></p><p>Given the pre-trained CNN, we transform it into a WSDDN by the following three steps:</p><ol><li>Replace the last pooling layer in the last convolutional block(relu5 and pool5 for example) with a SPP layer.<br>So we can get a feature vector or representation $\phi(x; R)$ by a function that takes an image $x$ and a region(bbox) $R$ as input:<br>$$<br>\phi(\mathbf{x} ; R)=\phi_{\mathrm{SPP}}(\cdot ; R) \circ \phi_{\mathrm{relu} 5}(\mathbf{x})<br>$$</li></ol><p>where $\phi_{\mathrm{relu} 5}$ needs to be computed only once for the whole image and $\phi_{\mathrm{SPP}}(\cdot ; R)$ is fast to compute for any given region $R$</p><ol start="2"><li>Given an image $x$, using Selective Search or Edge Boxes to obtain a list of candidate object regions(RoIs) $R=(R_1, …, R_n)$, then modify the SPP layer to take as input not a single region, but rather the full list $R$. In particular, $\phi(x; R)$ is defined as the concatenation of $\phi(x; R_1), …, \phi(x; R_n)$ along the forth dimension(since each individual $\phi(x; R_i)$ is a  3D tensor).</li><li>We can get the region-level feature vector after step.1 and step.2, then the features are precessed by two fully connected layers $\phi_{fc6}$ and $\phi_{fc7}$, finally we branch off two data streams on the precessed features.</li></ol><h3 id="Classification-data-stream"><a href="#Classification-data-stream" class="headerlink" title="Classification data stream"></a>Classification data stream</h3><p>The classification stream performs classification of the individual regions, <strong>by mapping each of regions to a C-dimensional vector of class scores</strong> to detect C different classes using a linear map $\phi_{fc8-cls}$. We can get the matrix $x^{cls} \in \mathbb{R}^{C \times |R|}$ containing the class prediction scores for each region, $|R|$ is the number of RoIs in an input image with Selective Search or Edge Boxes. Then use softmax to compute the probability of each class for individual $R$:<br>$$<br>\left[\sigma_{\text {class }}\left(\mathbf{x}^{cls}\right)\right]<em>{i j}=\frac{e^{x</em>{i j}^{cls}}}{\sum_{k=1}^{C} e^{x_{k j}^{cls}}}<br>\tag{1}$$</p><h3 id="Detection-data-stream"><a href="#Detection-data-stream" class="headerlink" title="Detection data stream"></a>Detection data stream</h3><p>The detection stream performs detection, <strong>by scoring regions relative to one another.</strong> This is achieved by using a linear map $\phi_{fc8-detect}$, and also resulting in a matrix of scores: $x^{detect} \in \mathbb{R}^{C \times |R|}$<br>$$<br>\left[\sigma_{\mathrm{det}}\left(\mathbf{x}^{det}\right)\right]<em>{i j}=\frac{e^{x</em>{i j}^{det}}}{\sum_{k=1}^{|\mathcal{R}|} e^{x_{i k}^{det}}}<br>\tag{2}$$</p><p>while the two streams are remarkably similar, the introduction of the $\sigma_{class}$ and $\sigma_{det}$ non-linearities in the classification and detection streams is a key difference which allows to interpret them as performing classification and detection.</p><p>The softmax in classification stream compares class scores for each region, another softmax in detection stream compares the scores of different regions. Hence, <strong>the classification softmax predicts the the most likely category for a region, but the detection softmax predicts which regions are more likely contain useful information.</strong><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Weakly-Supervised-Object-Detection/WSDNN_arch.png"></p><h3 id="Combined-region-scores-and-detection"><a href="#Combined-region-scores-and-detection" class="headerlink" title="Combined region scores and detection"></a>Combined region scores and detection</h3><p>The final score of each region is obtained by takeing the element-wise product $x^R = \sigma_{class}(x^{cls}) \odot \sigma_{det}(x^{det})$, the region scores are used to rank image regions, then standard NMS is performed to obtain the final list of class-specific detections in an image.</p><p>The way the two streams’ scores are combined is reminiscent of the bilinear network but there are three key differences:</p><ol><li>The different softmax operators explicitly breaks the symmetry of the two streams.</li><li>Using element-wise product  $\sigma_{class}(x^{cls}) \odot \sigma_{det}(x^{det})$ rather than product of two feature vectors $\sigma_{class}(x^{cls}) \otimes \sigma_{det}(x^{det})$.</li><li>Scores(<em>I think it is $\sigma_{class}(x^{cls}) \odot \sigma_{det}(x^{det})$, but it’s $\sigma_{class}(x^{cls}) \otimes \sigma_{det}(x^{det})$ in paper</em>) are computed for specific image regions rather than a fixed set of image localizations on a grid.</li></ol><h3 id="Image-level-classification-scores"><a href="#Image-level-classification-scores" class="headerlink" title="Image-level classification scores"></a>Image-level classification scores</h3><p>So far, we can get region-level scores $x^\mathcal{R}$, since different regions are obtained by using Selective Search or Edge Boxes in an image, the image-level class prediction score can be computed by summing all the region-level class scores:<br>$$<br>y_{c}=\sum_{r=1}^{|\mathcal{R}|} x_{c r}^{\mathcal{R}}<br>$$</p><p>$y_c$ means the probability that the image belongs to category $c$, it is a sum of element-wise product of softmax normalized scores over $|\mathcal{R}|$ regions and thus it is in the range of (0, 1). The reason why do not use softmax here is one image can contain many objects but each region should contain a single class.</p><h2 id="Training-WSDNN"><a href="#Training-WSDNN" class="headerlink" title="Training WSDNN"></a>Training WSDNN</h2><p>The data is a collection of images $x_i, i=1, …, n$ with image level labels $y_i \ in \left{ -1, 1 \right}^C$. Given model parameters $w$, using SGD with momentum to optimize the energy function:<br>$$<br>E(\mathbf{w})=\frac{\lambda}{2}|\mathbf{w}|^{2}+\sum_{i=1}^{n} \sum_{k=1}^{C} \log \left(y_{k i}\left(\phi_{k}^{\mathbf{y}}\left(\mathbf{x}_{i} \mid \mathbf{w}\right)-\frac{1}{2}\right)+\frac{1}{2}\right)<br>\tag{3}$$</p><p>hence optimizing a sum of $C$ binary-log-loss terms, one per class. As $(\phi_{k}^{\mathbf{y}}\left(\mathbf{x}<em>{i} \mid \mathbf{w}\right)$ is in range of (0, 1), it can be considered as a probability of class $k$ being present in image $x_i$. When the ground truth label is positive, the binary log loss becomes $log(p(y_{ki} = 1))$, otherwise $log(1-p(y</em>{ki} = 1))$</p><h2 id="Spatial-Regularizer"><a href="#Spatial-Regularizer" class="headerlink" title="Spatial Regularizer"></a>Spatial Regularizer</h2><p>As WSDDN is optimized for image-level class label, it does not guarantee any  spatial smoothness such that if a region obtains a high score for an object  class, the neighboring regions with high overlap will also have high scores.</p><p>In supervised detection case, they takes the region proposals that have at least 50% IoU with GT bbox as positive samples and then regress them into GT bbox. Since WSOD cannot use GT bbox(bbox annotations), so follow a soft regularization strategy that penalizes the feature map discrepancies at the second fully connected layer $fc7$ between the highest scoring region and the regions with at least 60% IoU during training:<br>$$<br>\frac{1}{n C} \sum_{k=1}^{C} \sum_{i=1}^{N_{k}^{+}} \sum_{r=1}^{|\bar{R}|} \frac{1}{2}\left(\phi_{k * i}^{\mathbf{y}}\right)^{2}\left(\phi_{k * i}^{\mathrm{fc} 7}-\phi_{k r i}^{\mathrm{fc} 7}\right)^{\mathrm{T}}\left(\phi_{k * i}^{\mathrm{fc} 7}-\phi_{k r i}^{\mathrm{fc} 7}\right)<br>\tag{4}$$</p><p>where $N^+_k$ is the number of positive images for the class $k$ and $* = argmax_r \phi^y_{kri}$ is the highest scoring region in image $i$ for the class $k$. Adding this regularization term to the cost function in <code>eq.(3)</code></p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>AP on VOC 2007 test:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Weakly-Supervised-Object-Detection/WSDNN_detection_result.png"></p><p>Ablation study:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Weakly-Supervised-Object-Detection/WSDNN_ablation.png"></p><ul><li>SSW: Selective Search Window</li><li>EB: Edge Boxes</li><li>Box Sc: Box Score</li><li>Sp. Reg.: Spatial Regularizer</li></ul>]]></content>
      
      
      <categories>
          
          <category> Object Detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Weakly Supervised </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Object Detection with NAS</title>
      <link href="2020/08/27/Object-Detection-with-NAS/"/>
      <url>2020/08/27/Object-Detection-with-NAS/</url>
      
        <content type="html"><![CDATA[<h1 id="DetNAS"><a href="#DetNAS" class="headerlink" title="DetNAS"></a>DetNAS</h1><h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><p>Object detectors are usually equipped with backbone networks designed for image classification, but there is a gap between the tasks of image classification and object detection, so it might be a sub-optimal choice.</p><p>Train the one-shot supernet under the typical detector training schedule:</p><ol><li>ImageNet pre-training</li><li>Detection fine-tuning</li></ol><p>Then the architecture search is performed on the trained supernet, using the detection task as the guidance to search better backbone.</p><h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ol><li>The first paper use NAS to search backbone for object detection</li><li>Proposed a powerful search space to help searched networks obtain inspiring accuracies with limited FLOPs</li><li>Outperforms the hand-crafted networks</li><li>Solve the problem that searching backbone for object detection with NAS could be hard to optimize and inefficient.</li></ol><h2 id="Framework-Overview"><a href="#Framework-Overview" class="headerlink" title="Framework Overview"></a>Framework Overview</h2><ol><li>Pre-training the one-shot supernet on ImageNet</li><li>Fine-tuning the one-shot supernet on detection datasets</li><li>architecture search on the trained supernet with an evolution algorithm</li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/Object-Detection-with-NAS/detnas_pipeline.png"></p><h2 id="Difficulties-for-searching-backbone"><a href="#Difficulties-for-searching-backbone" class="headerlink" title="Difficulties for searching backbone"></a>Difficulties for searching backbone</h2><ol><li><strong>hard to optimize</strong><br>NAS systems require accuracies on target tasks as signals, pre-training accuracy is unqualified for this requirement.</li><li><strong>inefficiency</strong><br>In order to obtain the precious performance, each candidate architecture during search has to be first pre-trained(e.g. on ImageNet) then fine-tuned on the detection dataset, which is very costly.</li></ol><p>The difficulties mainly caused by the typical detector training schedule that requires backbone networks to be pre-trained on ImageNet. Even though training from scratch is an alternative, it requires more training iterations to compensate for the lack of pre-training. Moreover, training from scratch breaks down in small datasets.</p><h2 id="Backbone-Search"><a href="#Backbone-Search" class="headerlink" title="Backbone Search"></a>Backbone Search</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>The search space $A$ can be denoted by a DAG, any path in the graph corresponds to a specific architecture, $a \in A$. For the specific architecture, its corresponding network can be represented as $N(a, w)$ with the backbone network weight $w$.</p><p>NAS aims to find the optimal backbone architecture $a^* \in A$ that minimizes the validation loss $\mathcal{L}<em>{val}(N(a^*, w^*))$, $w^*$ denotes the optimal backbone network weights of the backbone architecture $a^*$. It is obtained by minimizes the training loss. Then the NAS process can be regarded as a nested optimization problem:<br>$$\begin{array}{c}<br>\min <em>{a \in \mathcal{A}} \mathcal{L}</em>{v a l}\left(\mathcal{N}\left(a, w^{<em>}(a)\right)\right) \quad<br>\text { s.t. } w^{</em>}(a)=\underset{w}{\arg \min } \mathcal{L}</em>{t r a i n}(w, a)<br>\end{array}\tag{1}$$</p><p>The above formulation can represent NAS on tasks that work without pre-training(e.g. image classification), but for object detection, which needs pre-training and fine-tuning schedule, <code>Eq.(1)</code> needs to be reformulated as follow:<br>$$\begin{array}{c}<br>\min <em>{a \in \mathcal{A}} \mathcal{L}</em>{v a l}\left(\mathcal{N}\left(a, w^{<em>}(a)\right)\right) \<br>w^{</em>}(a)=\underset{w \leftarrow w_{p}(a)^{<em>}}{\arg \min } \mathcal{L}<em>{\text {train}}^{\text {det}}(w, a) \quad \text { s.t. } w</em>{p}(a)^{</em>}=\underset{w_{p}}{\arg \min } \mathcal{L}<em>{\text {train}}^{c l s}\left(w</em>{p}, a\right) \tag{2}<br>\end{array}$$</p><p>where $w \leftarrow w_{p}(a)^{<em>}$ is to optimize $w$ with $w_p(a)^</em>$ as initialization. The pre-trained backbone weights $w_p(a)^*$ on ImageNet cannot directly serve for the <code>Eq.(1)</code>, but it is necessary for $w(a)^{<em>}$, thus we can not skip the ImageNet pre-training. However, ImageNet pre-training usually cost several GPU days just for a single network. It is unaffordable to train all candidate networks individually, in one-shot NAS methods, search space is encoded in a supernet consists of all candidate architectures, sharing the weights in their common nodes, in this way, <code>Eq.(1)</code> become:<br>$$\min <em>{a \in \mathcal{A}} \mathcal{L}</em>{\text {val}}\left(\mathcal{N}\left(a, W_{\mathcal{A}}^{</em>}(a)\right)\right) \quad \text { s.t. } W_{\mathcal{A}}^{*}=\underset{W}{\arg \min } \mathcal{L}_{\text {train}}(\mathcal{N}(\mathcal{A}, W)) \tag{3}$$</p><p>where all individual network weights $w(a)$ are inherited from the one-shot supernet $W_{A}$. <strong>The process of training supernet ($W_A$ optimization) is decoupled from the process of searching architecture ($a$ optimization).</strong> Combine <code>Eq.(2)</code> and <code>Eq.(3)</code> can get the final representation:<br>$$\begin{array}{l}<br>\min <em>{a \in \mathcal{A}} \mathcal{L}</em>{v a l}^{\operatorname{det}}\left(\mathcal{N}\left(a, W_{\mathcal{A}}^{<em>}(a)\right)\right) \<br>\text { s.t. } W_{\mathcal{A}}^{</em>}=\underset{W \leftarrow W_{p \mathcal{A}}^{<em>}}{\arg \min } \mathcal{L}<em>{\text {train}}^{\text {det}}(\mathcal{N}(\mathcal{A}, W)) \<br>W</em>{p \mathcal{A}}^{</em>}=\arg \min <em>{W</em>{p}} \mathcal{L}<em>{\text {train}}^{c l s}\left(\mathcal{N}\left(\mathcal{A}, W</em>{p}\right)\right)<br>\end{array}$$</p><h3 id="NAS-Pipeline"><a href="#NAS-Pipeline" class="headerlink" title="NAS Pipeline"></a>NAS Pipeline</h3><ol><li>Supernet pre-training</li></ol><p><strong>pre-training supernet on ImageNet</strong> and <strong>adopt a path-wise manner</strong> to make sure the trained supernet can reflect the relative performance of candidate networks. Specifically, in each iteration, only one single path is sampled for forward and backward propagation. No gradient or weight update acts on other paths or nodes in the super graph.<br>2. Supernet fine-tuning<br>The supernet fine-tuning is also <strong>path-wise</strong> but <strong>equipped with detection heads and metrics and datasets</strong>. Another important detail is <strong>using Synchronized Batch Normalization instead of Batch Normalization during supernet training</strong>.<br>Typically the parameters of BN, during fine-tuning are fixed as the pre-training batch statistics, but the freezing BN is infeasible in DetNAS because the features to normalize are not equal on different paths, On the other hand, Object detection usually with small batch size.<br>3. Search on supernet with EA<br>This part is also about BN for detail, during training, different child networks are sampled path-wise in the supernet. The issue is that the batch statistics on one path should be independent of others.Therefore, we need to recompute batch statistics for each single path (child networks) before each evaluation. This detail is indispensable in DetNAS</p><h3 id="Search-Space"><a href="#Search-Space" class="headerlink" title="Search Space"></a>Search Space</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/Object-Detection-with-NAS/detnas_search_space.png"></p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/Object-Detection-with-NAS/detnas_ablation_study.png"></p><ul><li>ShuffleNet is a baseline.</li><li><strong>ClsNASNet</strong> is the best backbone architecture searched on ImageNet but not using fine-tuning</li><li><strong>DetNAS-scratch</strong> means training supernet from scratch on detection datasets instead of pre-training on ImageNet</li><li><strong>DetNAS</strong>, as said, pre-training on ImageNet and fine-tuning on detection datasets.</li></ul><p>The ablation study shows that both pre-training and fine-tuning are important.</p><h3 id="Comparisons-to-random-search"><a href="#Comparisons-to-random-search" class="headerlink" title="Comparisons to random search"></a>Comparisons to random search</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/Object-Detection-with-NAS/detnas_results_compare_to_random_search.png"><br>It shows that EA are outperform than random search…but why use EA rather than gradient-based/RL methods? Another confusion is what’s the necessity of ablation study on search strategy in object detection papers???</p><h1 id="NAS-FPN"><a href="#NAS-FPN" class="headerlink" title="NAS-FPN"></a>NAS-FPN</h1><h2 id="Idea-1"><a href="#Idea-1" class="headerlink" title="Idea"></a>Idea</h2><p>PANet shows adding an extra bottom-up pathway on FPN features improves feature representations for lower resolution features.  So searching FPN maybe a good way to improve the performance of detectors.</p><p>The most difficult problem of searching feature architecture is its huge search space. The number of possible connections to combine features from different scales grow up exponentially with the number of layers. </p><p>So inspired by NASNet, NAS-FPN propose a search space of scalable architecture to generate, specifically, like NASNet, searching an atomic architecture(same input and output levels) and then stack(repeat) them to a large architecture.</p><h2 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution"></a>Contribution</h2><ol><li>Designed a search space that covers all possible cross-scale connections to generate multi-scale features.</li><li>The searched NAS-FPN architecture can be applied repeatedly like NASNet, so it is manageable and can achieve “early exit”.</li><li>Using scalable search space and NAS can tradeoff accuracy and speed.</li></ol><h2 id="Merging-Cell"><a href="#Merging-Cell" class="headerlink" title="Merging Cell"></a>Merging Cell</h2><p>Merging cell is a fundamental building block of a FPN, to merge any two input features layers into a output layer. A FPN consists of N(given during search) merging cells, in a merging cell, all feature layers have the same number of filters.  The decisions of how to conduct the merging cell are made by a controller RNN, each merging cell has 4 prediction steps made by distinct softmax classifier:</p><ol><li>Select a feature $h_i$ from candidates.</li><li>Select another feature $h_j$ from candidates.</li><li>Select the output feature resolution.</li><li>Select a operation to combine $h_i$ and $h_j$ and generate a feature layer with the resolution selected in Step 3.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/Object-Detection-with-NAS/TODO.png"></li></ol><p>There are two binary operations for Step 3, including sum and global pooling:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/Object-Detection-with-NAS/TODO.png"></p><p>Then the newly-generated feature layer is appended to the list of existing input candidates and become a new candidate for the next merging cell. There can be multiple candidate features share the same resolution during the search.</p><p>The order of output feature is predicted by the controller RNN, each output feature layer is then generated by repeating the step 1~4 until the output feature pyramid is fully generated.</p><p>In the end, the last 5 merging cells are designed to output feature pyramid ${ { P_3, P_4, P_5, P_6, P_7 } }$, taking all feature layers that have not been connected to any of output layer and sum them to the output layer that has the corresponding resolution.</p><h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h2><p>Basically follow the design of RetinaNet, which uses the last layer in each group of feature layers as the inputs to first pyramid network. <strong>For scalable NAS-FPN, the output of the first pyramid network are the input to the next pyramid network.</strong></p><p>Since both inputs and outputs of a NAS-FPN are feature layers in the identical scales, so the NAS-FPN can be stacked repeatedly for better accuracy.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/Object-Detection-with-NAS/TODO.png"></p><h2 id="Early-Exit"><a href="#Early-Exit" class="headerlink" title="Early Exit"></a>Early Exit</h2><p>We can attach classifier and box regression heads after all intermediate pyramid networks and train it with deep supervision. During inference, the model dose not need to finish the forward pass for all pyramid networks, Instead, it can stop at the output of any pyramid network and generate detection results.</p><p>This can be a desirable property when computation resource or latency is concerned.</p><h2 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h2><p>To show results, define <code>R-50, 5@256</code> for a model with backbone ResNet-50, 5 stacked NAS-FPN and 256 feature dimensions.</p>]]></content>
      
      
      <categories>
          
          <category> Object Detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NAS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Network Architecture Search Survey</title>
      <link href="2020/08/04/NAS-Survey/"/>
      <url>2020/08/04/NAS-Survey/</url>
      
        <content type="html"><![CDATA[<p>这篇博客主要是对 <a href="https://arxiv.org/abs/1808.05377">Neural Architecture Search: A Survey</a> 这篇文章进行翻译并整理，文章写的很好，推荐去看。还有一篇综述是 <a href="https://arxiv.org/abs/2008.01475">Weight-Sharing Neural Architecture Search: A Battle to Shrink the Optimization Gap</a> 主要是关于 weight-sharing 和可微分 NAS 的，在 Valse 2020 上由华为诺亚的 <a href="http://lingxixie.com/Home.html">谢凌曦</a> 老师写的，谢老师讲的很好，大家也可以去看他在 Valse 2020 上的分享。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>NAS 可以看成是 AutoML 的一个子领域，和元学习以及超参优化有很多重叠的部分，NAS 方法可以划分为以下 3 个不同的方向：</p><ol><li>Search Space</li><li>Search Strategy</li><li>Performance Estimation Strategy</li></ol><ul><li><p>Search Space<br>搜索空间定义了可以表示哪些网络结构，结合适合特定任务的先验知识可以减小搜索空间的大小并简化搜索，但这也会引入人为的偏见，这可能会导致无法搜索到搜出人类理解之外的网络结构。</p></li><li><p>Search Strategy<br>搜索策略详细描述了如何来对搜索空间进行探索，在很大的搜索空间中如何找到合适的网络结构，搜索策略一方面需要找到一个好的网络结构，另一方面需要避免过早地收敛到次优的网络结构。</p></li><li><p>Performance Estimation Strategy<br>性能估计策略是在未知的数据集上对模型进行评估的方法，最简单的选择是对每个网络结构都进行标准的训练和验证，但这样非常耗费计算资源并且限制了能够搜索的网络结构数量，所以也有不少研究集中于如何减少性能估计策略所需要的大量计算资源。</p></li></ul><p>整个 NAS 的结构可以由下图来表示，首先定义一个搜索空间，利用搜索策略在搜索空间中采样一个网络结构，使用性能估计策略来对网络结构进行评估，将评估结果反馈给搜索策略，使其产生更好的网络结构：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/NAS-Survey/nas_review.png"></p><h1 id="Search-Space"><a href="#Search-Space" class="headerlink" title="Search Space"></a>Search Space</h1><p>搜索空间定义了 NAS 可能发现的网络结构，这里主要有 2 种形式的搜索空间，一种是链式网络结构的搜索空间，另一种是 Cell-Based 网络结构的搜索空间。</p><h2 id="Chain-Structured-Search-Space"><a href="#Chain-Structured-Search-Space" class="headerlink" title="Chain-Structured Search Space"></a>Chain-Structured Search Space</h2><p>链式网络结构的搜索空间如下图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/NAS-Survey/nas_search_spaces.png"></p><p>一个最简单的链式网络结构的搜索空间 $A$ 如上图中左边部分所示，可以看成是有着 $n$ 个 layers 的序列，第 $i$ 个 layer $L_i$ 的输入为 $L_{i-1}$ 的输出，$L_i$ 的输出作为 $L_{i+1}$ 的输入，这样链式网络结构 $A$ 就可以表示为多个 layer 的形式：<br>$$<br>A=L_{n} \circ \ldots L_{1} \circ L_{0}<br>$$</p><p>这样整个搜索空间就可以由以下几个参数来定义：</p><ol><li>最大的层数 $n$，即网络的深度</li><li>每一层执行的操作(e.g. 池化，卷积或者深度可分离卷积等)</li><li>和每一层执行的操作有关的超参数(e.g.  kernel size，stride 和卷积核个数等)</li></ol><p>主要注意的是 3 中的超参数基于 2 中的操作，因此搜索空间的参数化的结果并不是固定长度的，而是一个条件空间(conditional space)。</p><p>这种最简单的链式网络结构的搜索空间肯定是不行的，需要为其加上 ResNet 中的 skip-connection 和 InceptionNet 中的 multi-branches 结构，如上图中右边所示。在这种情况下每一层 $L_i$ 的输入可以看成是结合了之前所有层输出的一个函数：<br>$$<br>g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)<br>$$</p><p>这样就可以显著地增加模型的灵活性 (ResNet 中的跳接结构可以看成是增加了模型的灵活性，让模型自己去选择更倾向于什么都不做还是做卷积运算)，具体的，为了得到以下几种多分支的网络结构，上述的函数的结果分别为：</p><ol><li>最简单的链式网络结构：$g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right) = L^{out}_{i-1}$</li><li>类似 ResNet 的跳接结构：$g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right) = L^{out}<em>{i-1} + L^{out}</em>{j}$，其中 $j &lt; i-1$</li><li>类似 DenseNet 的密集连接结构：$g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right) =concat(L^{out}<em>{i-1},…,L^{out}</em>{0})$</li></ol><h2 id="Cell-Based-Search-Space"><a href="#Cell-Based-Search-Space" class="headerlink" title="Cell-Based Search Space"></a>Cell-Based Search Space</h2><p>收到 ResNet 中 Block 的启发，<a href="https://arxiv.org/pdf/1707.07012.pdf">NASNet</a> 提出了 Cell-Based 的搜索空间，主要想法是搜索 Norm Cell 和 Reduction Cell，然后把它们进行堆叠从而得到最终的网络结构，其中 Norm Cell 不改变输入输出特征图的分辨率，Reduction Cell 的输出特征图的分辨率为输入特征图分辨率的一半。如下图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/NAS-Survey/nas_Cell_based_search_space.png"></p><p>和链式网络结构的搜索空间相比，这种搜索空间主要有 3 个优点：</p><ol><li>搜索每个 Cell 相比于搜索整个网络结构而言肯定很容易，所以搜索 Cell 再堆叠起来能大幅度减少搜索空间的大小，从而减少搜索时间。</li><li>可以简单地通过更改模型中 Cells 的数量 (使网络更深) 或者卷积核数量 (使网络更宽) 来把在小数据集上搜到的网络迁移到更大的数据集上。</li><li>ResNet 和 LSTM 已经证明了这种堆叠式的网络结构是有效的。</li></ol><p><strong>我个人认为这种搜索空间只是一种由于计算资源限制而导致的 trade-off，这种堆叠式的网络结构虽然是有效的，但不一定是最好的，换句话说虽然这种搜索空间加入了人类已知的先验知识，但也带入了人类设计网络结构中的一些 bias，这种搜索空间有点偏离了 NAS 的真正目标。</strong></p><p>这种 Cell-Based 的搜索空间可以看成搜索 2 个结构，一种是宏观结构，用于决定搜多少个 Cells，以及如何将这些 Cells 结合起来作为整个网络模型，另一种是微观结构，代表每个 Cell 中具体执行哪些操作(卷积、池化等)，如何执行(卷积核尺寸、stride 等)。无论是宏观结构还是微观结构，都可以像上面说的链式网络一样添加多分支的网络结构。最好的方法是共同优化宏观结构和微观结构，如果只搜索微观结构，那么就需要人为地设置宏观结构，这就不一定能获得最优的结构。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/NAS-Survey/macro_and_micro_search_space.png"></p><p><a href="https://arxiv.org/abs/1711.00436">Hierarchical Representations for Efficient Architecture Search</a> 这篇文章提出了使用分层搜索空间来优化宏观结构，这种分层搜索空间包括若干个层，每一层都是一个或者多个计算图：</p><ul><li>第一层定义一组基本操作</li><li>第二层通过有向无环图 (DAG) 来连接第一层中定义好的基本操作</li><li>第三极对如何连接第二层进行 encode<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/NAS-Survey/hierarchical_architecture_representation.png"></li></ul><p>Cell-Based 的搜索空间可以看作是这种分层搜索空间的一种特殊情况，其中级别数为3，第二层对应于 Cell，第三级为 encode 宏体系结构。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>搜索空间的定义很大程度上决定了优化问题的难度，即使在固定宏观网络结构的情况下来搜索每个 Cell，Cell 的搜索空间仍然面临 2 个问题：</p><ol><li>搜索空间不是连续的</li><li>搜索空间维度很高(为了得到更复杂效果更好的模型)</li></ol><h1 id="Search-Strategy"><a href="#Search-Strategy" class="headerlink" title="Search Strategy"></a>Search Strategy</h1><p>文中介绍了一些方法在 NAS 上应用的历史…太长不看系列，这里有 5 种搜索策略，分别是：</p><ul><li>Random Search</li><li>Bayesian Optimization</li><li>Evolutionary Methods</li><li>Reinforcement Learning (RL)</li><li>Gradient-Based Methods</li></ul><h2 id="Random-Search"><a href="#Random-Search" class="headerlink" title="Random Search"></a>Random Search</h2><p>Random Search 现在更多是用于验证一个搜索空间的好坏，如果一个搜索空间使用 Random Search 都能得到很好的网络结构，那么就说明这个搜索空间是好的。做法也很简单，顾名思义随机搜索呗。</p><h2 id="Bayesian-Optimization"><a href="#Bayesian-Optimization" class="headerlink" title="Bayesian Optimization"></a>Bayesian Optimization</h2><p>贝叶斯优化是很火的超参数优化的方法之一，但在 NAS 上很少用，因为比较好的贝叶斯优化 toolboxes 都基于高斯过程，且主要为了解决低维连续优化问题，上面 Search Space 的总结部分说到了搜索空间既不是连续的，维度又很高，所以一般不用贝叶斯优化来做，这部分具体的就不说了。</p><h2 id="Evolutionary-Methods"><a href="#Evolutionary-Methods" class="headerlink" title="Evolutionary Methods"></a>Evolutionary Methods</h2><p>进化算法(蚁群、遗传等) 最近 2 年貌似用的不是很多，有一些工作是用 Gradient-Based 方法来优化权重，用进化算法来优化网络结构，这是近几年用进化算法作为 NAS 搜索策略的文章：</p><ul><li><a href="https://arxiv.org/abs/1804.09081v2">ICLR 2019：Efficient multi-objective neural architecture search via lamarckian evolution</a></li><li><a href="https://arxiv.org/abs/1802.01548v6">AAAI 2019：Aging Evolution for Image Classifier Architecture Search</a></li><li><a href="https://arxiv.org/abs/1711.00436">ICLR 2018：Hierarchical Representations for Efficient Architecture Search</a> </li></ul><p>进化算法会产生一组比较多的模型，在每个进化 step 中至少要采样种群中的一种模型并将其作为父代，通过利用突变来产生后代，对应于 NAS，其中的变异是指本地操作，比如添加或者删除某层、更改某层参数、添加跳接结构或者改变训练超参数等，训练结束后评估子代的适应度(在验证集中的性能)并添加到种群中，为了生成后代，大多数方法都对子网络进行随机初始化。* <a href="https://arxiv.org/abs/1802.01548v6">AAAI 2019：Aging Evolution for Image Classifier Architecture Search</a> 这篇文章做了结果对比，发现强化学习和进化算法都能取得比较好的结果，进化算法在任何时候相比之下都能取得更好的结果，同时也有着更小的网络结构。</p><h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p>NAS 火起来的那篇 ICLR 2017 的文章就是用强化学习来做的，叫做 <a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search with Reinforcement Learning</a>，最近 2 年没看到在 NAS 上改进强化学习的文章，用的也不是很多。</p><p>强化学习的搜索策略是把网络结构的产生看作是一系列 agent 的 actions，这里的 action space 和 NAS 中的 Search Space 相同，agent 得到的 reward 是搜到的网络结构在未知数据集上的性能，不同的 RL 方法在如何表示 agent 的策略和如何优化其策略上有所不同， <a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search with Reinforcement Learning</a> 用 RNN 来顺序地采样一个用于 encode 网络结构的字符串，最开始使用策略梯度来训练网络，但后面使用 Proximal Policy Optimization，<a href="https://arxiv.org/abs/1611.02167">Designing neural network architectures using reinforcement learning</a> 使用 Q-Learning 来训练一种策略，这种策略会一次选择层的类型和相应的超参数。</p><h2 id="Gradient-Based-Methods"><a href="#Gradient-Based-Methods" class="headerlink" title="Gradient-Based Methods"></a>Gradient-Based Methods</h2><p><a href="arxiv.org/abs/1806.09055">DARTS</a> 引领了近一年内的 NAS 工作，提出的可微分 NAS 显著提升了 NAS 的效率，基于 PyTorch 的<a href="https://github.com/quark0/darts">代码</a>也写的好，最近的很多工作都是基于 DARTS 的。</p><p>DARTS 的思想是对搜索空间进行一个连续的松弛化以使用基于梯度的方法来直接进行优化，和固定每个特定层的单个操作 $o_i$ 不同，而是从一组操作 $\left{o_{1}, \ldots, o_{m}\right}$ 中计算一个凸组合，具体的，对于给定输入 $x$，一个 layer 的输出 $y$ 可以通过下式计算得到：<br>$$<br>y=\sum_{i=1}^{m} \alpha_{i} o_{i}(x), \alpha_{i} \geq 0, \sum_{i=1}^{m} \alpha_{i}=1<br>$$</p><p>其中凸系数 $\alpha_{i}$ 有效地参数化了整个网络的结构，然后在同时在训练集上优化网络权重 $w$ 和在验证集上优化网络结构的参数化表示(encoding) $\alpha$，最后为每一个 layer 选择最有可能的操作 $i^{*}=\arg \max <em>{i} \alpha</em>{i}$ 就能得到一个离散的网络结构。</p><p>和优化每个可能的操作的权重 $\alpha_i$ 不同，2 篇都来自 ICLR 2019 的文章<a href="https://arxiv.org/abs/1812.09926?context=stat.ML">SNAS: stochastic neural architecture search</a> 和 <a href="https://arxiv.org/abs/1812.00332v1">ProxylessNAS: Direct neural architecture search on target task and hardware</a> 提出在所有可能的操作上优化其参数分布，而不是其 encoding。</p><h1 id="Performance-Estimation-Strategy"><a href="#Performance-Estimation-Strategy" class="headerlink" title="Performance Estimation Strategy"></a>Performance Estimation Strategy</h1><p>最简单的评估策略是在训练集上训练模型并在验证集上进行评估，但如果对每个搜索得到的网络结构都从头开始训练并验证，那么需要耗费巨大的计算资源，所以需要提出一些用于加速网络性能评估的方法，作者将这些方法分为 4 类：<br>|  加速方法   | 如何实现加速 | 参考链接 |<br>|  :—-:  | :—- | :—-: |<br>| Lower Fidelity Estimates  | 通过在数据集的子集上训练、训练更少的 epochs、缩减模型大小等方法来减少训练时间 | [1] [2] [3] [4] [5] [6]|<br>| Learning Curve Extrapolation  | 网络的性能只需要经过几次训练就可以推断出，我的理解是只对网络进行几次训练，如果这几次训练的结果不好就直接放弃这个结构，从而减少时间消耗 | [1] [2] [3] [4] |<br>| Weight Inheritance/Network Morphisms  | 对采样得到的网络结构不从头开始训练，而是通过继承其他模型的权重来热启动 (例如 parent model) | [1] [2] [3] [4] |<br>| One-Shot Models/Weight Sharing  | 只需要训练一次模型，这个模型看成是一个 super graph，其权重在不同的网络结构之间是共享的，每个网络结构都是 super graph 中的一个 sub graph | [1] [2] [3] [4] [5] [6] |</p><p>对应的会议名称和 Papers：</p><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看 Lower Fidelity Estimates 的文章</span></div>    <div class="hide-content"><table><thead><tr><th align="center">加速方法</th><th align="center">参考链接</th><th align="center">名称地址</th></tr></thead><tbody><tr><td align="center">Lower Fidelity Estimates</td><td align="center">[1]</td><td align="center"><a href="https://arxiv.org/abs/1603.06560v2">ICLR 2017 <br> Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</a></td></tr><tr><td align="center">Lower Fidelity Estimates</td><td align="center">[2]</td><td align="center"><a href="https://arxiv.org/abs/1806.07912v1">arXiv 2018 <br> Resource-Efficient Neural Architect</a></td></tr><tr><td align="center">Lower Fidelity Estimates</td><td align="center">[3]</td><td align="center"><a href="https://arxiv.org/abs/1807.06906">ICML 2018 workshop <br> Towards automated deep learning: Efficient joint neural architecture and hyperparameter search</a></td></tr><tr><td align="center">Lower Fidelity Estimates</td><td align="center">[4]</td><td align="center"><a href="https://arxiv.org/abs/1807.01774?context=cs">ICML 2018 <br> BOHB: Robust and efficient hyperparameter optimization at scale</a></td></tr><tr><td align="center">Lower Fidelity Estimates</td><td align="center">[5]</td><td align="center"><a href="https://arxiv.org/abs/1802.01548v6">AAAI 2019 <br> Aging Evolution for Image Classifier Architecture Search</a></td></tr><tr><td align="center">Lower Fidelity Estimates</td><td align="center">[6]</td><td align="center"><a href="https://arxiv.org/abs/1812.11951?context=cs.LG">ICLR2019 <br> Learning to Design RNA</a></td></tr></tbody></table></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看 Learning Curve Extrapolation 的文章</span></div>    <div class="hide-content"><table><thead><tr><th align="center">加速方法</th><th align="center">参考链接</th><th align="center">名称地址</th></tr></thead><tbody><tr><td align="center">Learning Curve Extrapolation</td><td align="center">[1]</td><td align="center"><a href="https://arxiv.org/abs/1603.06560v2">arXiv 2014 <br> Freeze-thaw bayesian optimization</a></td></tr><tr><td align="center">Learning Curve Extrapolation</td><td align="center">[2]</td><td align="center"><a href="https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/view/11468/0">IJCAI 2015 <br> Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves</a></td></tr><tr><td align="center">Learning Curve Extrapolation</td><td align="center">[3]</td><td align="center"><a href="https://openreview.net/forum?id=S11KBYclx&noteId=r15rc0-Eg">ICLR 2017 <br> Learning curve prediction with Bayesian neural networks</a></td></tr><tr><td align="center">Learning Curve Extrapolation</td><td align="center">[4]</td><td align="center"><a href="https://arxiv.org/abs/1705.10823">NIPS 2017 workshop <br> Accelerating Neural Architecture Search using Performance Prediction</a></td></tr></tbody></table></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看 Weight Inheritance/Network Morphisms 的文章</span></div>    <div class="hide-content"><table><thead><tr><th align="center">加速方法</th><th align="center">参考链接</th><th align="center">名称地址</th></tr></thead><tbody><tr><td align="center">Weight Inheritance/Network Morphisms</td><td align="center">[1]</td><td align="center"><a href="https://arxiv.org/abs/1703.01041">ICML 2017 <br> Large-scale evolution of image classifiers</a></td></tr><tr><td align="center">Weight Inheritance/Network Morphisms</td><td align="center">[2]</td><td align="center"><a href="https://arxiv.org/abs/1711.04528">NIPS 2017 workshop <br> Simple And Efficient Architecture Search for Convolutional Neural Networks</a></td></tr><tr><td align="center">Weight Inheritance/Network Morphisms</td><td align="center">[3]</td><td align="center"><a href="https://arxiv.org/abs/1804.09081v2">ICLR 2019 <br> Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution</a></td></tr><tr><td align="center">Weight Inheritance/Network Morphisms</td><td align="center">[4]</td><td align="center"><a href="https://arxiv.org/abs/1707.04873">AAAI 2018 <br> Efficient architecture search by network transformation</a></td></tr></tbody></table></div></div><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看 One-Shot Models/Weight Sharing 的文章</span></div>    <div class="hide-content"><table><thead><tr><th align="center">加速方法</th><th align="center">参考链接</th><th align="center">名称地址</th></tr></thead><tbody><tr><td align="center">One-Shot Models/Weight Sharing</td><td align="center">[1]</td><td align="center"><a href="https://arxiv.org/abs/1606.02492v2">NIPS 2016 <br> Convolutional neural fabrics</a></td></tr><tr><td align="center">One-Shot Models/Weight Sharing</td><td align="center">[2]</td><td align="center"><a href="https://arxiv.org/abs/1802.03268">ICML 2018 <br> Efficient neural architecture search via parameter sharing</a></td></tr><tr><td align="center">One-Shot Models/Weight Sharing</td><td align="center">[3]</td><td align="center"><a href="http://proceedings.mlr.press/v80/bender18a.html">ICML 2018 <br> Understanding and simplifying one-shot architecture search</a></td></tr><tr><td align="center">One-Shot Models/Weight Sharing</td><td align="center">[4]</td><td align="center"><a href="arxiv.org/abs/1806.09055">ICLR 2019 <br> DARTS: Differentiable architecture search</a></td></tr><tr><td align="center">One-Shot Models/Weight Sharing</td><td align="center">[5]</td><td align="center"><a href="https://arxiv.org/abs/1812.00332v1">ICLR 2019 <br> ProxylessNAS: Direct neural architecture search on target task and hardware</a></td></tr><tr><td align="center">One-Shot Models/Weight Sharing</td><td align="center">[6]</td><td align="center"><a href="https://arxiv.org/abs/1812.09926?context=stat.ML">ICLR 2019 <br> SNAS: stochastic neural architecture search</a></td></tr></tbody></table></div></div><h2 id="Lower-Fidelity-Estimates"><a href="#Lower-Fidelity-Estimates" class="headerlink" title="Lower Fidelity Estimates"></a>Lower Fidelity Estimates</h2><p>主要通过在数据集的子集上训练、训练更少的 epochs、缩减模型大小等方法来得到一个低逼真度的近似值，用于代替完全进行训练和验证的模型性能，以此来缩短评估时间。</p><p>尽管这些低逼真度的近似降低了计算成本，但其性能通常会被低估，所以可能在估计值中引入偏差，但只要搜索策略只依赖于对不同的网络结构进行排名且相对排名保持稳定，那么这个方法就不会出现大的问题，但当这些低逼真度的近似值和完全评估直接差距太大时，相对排名就会发生很大的变化。</p><h2 id="Learning-Curve-Extrapolation"><a href="#Learning-Curve-Extrapolation" class="headerlink" title="Learning Curve Extrapolation"></a>Learning Curve Extrapolation</h2><p>通过建立学习曲线推断来评估模型性能，可以通过终止那些预期的表现不佳的曲线(可以理解为放弃那些表现不好的模型)来加快搜索过程。李飞飞组的文章 <a href="https://arxiv.org/abs/1712.00559">ECCV 2018: Progressive Neural Architecture Search</a> 提出了使用一种“替代”模型来预测新模型的性能，这种方法不采用学习曲线推断，而是根据网络结构或者是 Cells 的属性来预测性能，并且推导到比训练期间更大的网络结构或者 Cell 上，预测网络结构性能的主要挑战在于：为了加快搜索过程，需要基于较少的评估在较大的搜索空间中作出良好的预测。</p><h2 id="Weight-Inheritance-Network-Morphisms"><a href="#Weight-Inheritance-Network-Morphisms" class="headerlink" title="Weight Inheritance/Network Morphisms"></a>Weight Inheritance/Network Morphisms</h2><p>这种方法是用以前已经训练过的其他网络结构的权重来初始化新网络结构的权重，其中一种方法被称为“网络形态学”，也有一些研究允许修改网络结构但同时保持网络代表的功能不变，例如 <a href="https://arxiv.org/abs/1711.04528">NIPS 2017 workshop: Simple And Efficient Architecture Search for Convolutional Neural Networks</a> ，具体可以看上面表格中的文献。</p><h2 id="One-Shot-Models-Weight-Sharing"><a href="#One-Shot-Models-Weight-Sharing" class="headerlink" title="One-Shot Models/Weight Sharing"></a>One-Shot Models/Weight Sharing</h2><p>这是目前最火工作最多的方法，这种方法把所有的网络结构看成是 super-graph(one-shot model) 中的 sub-graphs，并且在 super-graph 中有边网络结构之间共享权重。这种方法仅需要训练一个 one-shot 模型的权重，其他的网络结构可以通过继承 one-shot 模型的权重来进行性能评估，这种方式不需要对模型进行训练，所以可以大大加快网络结构的性能评估，但有一些研究指出这种 one-shot 模型会产生很大的 bias，因为这种 one-shot 模型严重低估了最佳网络结构的实际性能，但它可以对不同的网络结构进行排名，只要估计的性能和实际性有很强的相关性，那就不会产生大的影响。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/08/NAS-Survey/one_shot_architecture_search.png"></p><p>不同的 one-shot NAS 方法在如何训练 one-shot 模型方面有所不同，ENAS 学习一种 RNN 控制器，该控制器从搜索空间中对网络结构进行采样，并基于通过 RL 得到的策略梯度来训练 one-shot 模型。DARTS 优化了 one-shot 模型的所有权重，并通过在 one-shot 模型的每个边上进行 softmax 来对搜索空间进行连续的松弛化，SNAS 则并没有对 DARTS 中操作的真实权重进行优化，而是优化可能操作的分布，通过采用具体分布和重新进行参数化来松弛化离散分布并使其可微，从而可以通过梯度下降来进行优化。ProxylessNAS 对架构权重进行了“二值化”，只对每一个操作保留一条边，删除其他的边。然后通过采用二值化的网络结构和使用 BinaryConnect 来更新相应的概率，从而得到边是否被 masked out 的概率。</p><p>尽管之前提到的方法是在整个训练过程中优化网络结构的分布，但有的研究表明，权重贡献和一个固定的分布可能是 one-shot NAS 需要的唯一要素，和这些方法有关的是超网络的元学习，它会为新的网络结构生成权重，因此仅需要训练超网络而无需训练架构本身，这里主要的区别是权重不是严格贡献的，而是由贡献的超网络生成的。</p><p>one-shot NAS 的限制在于，为 super-graph 定义的先验限制了 sub-graph 的搜索空间，此外，考虑到在网络结构搜索期间要求整个 super-graph 都要保存在 GPU 内存中，这种情况就导致了只能使用比较小的 super-graph 和比较小的搜索空间，尽管目前权重共享方法极大地减少了 NAS 所需要的计算资源，但目前还不了解如果网络结构的采样分布和 one-shot 模型一起优化而不是固定网络结构的采样分布会引入哪些 bias。例如，在搜索空间中搜索网络结构时，如果某些部分比其他地方有着更多的初始偏差，那么就会导致 one-shot 模型更好地适合于这些网络结构，从而反过来会加大对搜索部分的搜索偏差，这会导致 NAS 过早的收敛或者 one-shot 模型的性能和真实性能之间的相关性很小。总的来说就是对不同性能评估方法所引入的偏差进行更加系统的分析是未来 NAS 研究的一个很好方向。</p><h1 id="Future-Directions"><a href="#Future-Directions" class="headerlink" title="Future Directions"></a>Future Directions</h1><ol><li>把 NAS 应用于其他领域，而不仅仅是图像分类。</li><li>开发用于解决 multi-tasks 和 multi-objective 问题的 NAS 方法，例如将计算资源和模型性能一起作为 NAS 的优化目标，这个其实 MIT 的<a href="https://songhan.mit.edu/">韩松</a>老师一直有在做，这个方向和模型压缩很相关，都是为了找到轻量且性能好的模型，韩松老师也用过 NAS 来做模型压缩。</li><li>用 NAS 来找到面对对抗样本时更加鲁棒的模型。</li><li>定义更加通用和更加灵活的搜索空间，其实有很多 module 都没有放入搜索空间里面，比如 attention，HRNet 等，比如 Cell-Based 的搜索空间可以在不同数据集的分类任务上有很好的迁移性，这很大程度上是因为人类对图像分类的经验，但在其他任务例如检测、分割上这种 Cell-Based 的搜索空间不一定是有效的，所以可以考虑针对特定领域的制定特定的搜索空间。</li><li>现有论文虽然都基于 CIFAR-10 数据集，在搜索空间、计算资源等其他方面实验都有所不同，所以无法对不同方法的可重复性进行比较是很困难的，有的使用例如余弦学习率、CutOut 或者 MixUp 就能在 CIFAR-10 上大幅提点，所以很难说这是论文中的方法提升了结果还是这些 tricks 提升了结果，好在 UTS 的杨易老师组和 Google 都提出了 Benchmark。</li><li>NAS 搜出来的结构为什么高效以及为什么每次独立运行搜出来的结果都很相似？知道为什么这些网络对高性能很重要以及能否将其用于其他问题，这个方向也不错。(模型的可解释性还没好好解决，又要解决 NAS 的可解释性吗…)</li></ol><p>个人觉得可以做的方向：</p><ol><li>把 NAS 用于其他领域，例如模型压缩、检测等。</li><li>在特定硬件或者资源限制条件下的 NAS</li><li>找到适用于其他特定 task 的搜索空间</li></ol>]]></content>
      
      
      <categories>
          
          <category> NAS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NAS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Network Architecture Search</title>
      <link href="2020/07/20/NAS/"/>
      <url>2020/07/20/NAS/</url>
      
        <content type="html"><![CDATA[<h1 id="NAS-with-RL"><a href="#NAS-with-RL" class="headerlink" title="NAS with RL"></a>NAS with RL</h1><p>原文是: <a href="https://arxiv.org/pdf/1611.01578.pdf">Neural Architecture Search with Reinforcement Learning</a><br>作者观察到神经网络的结构和连通性可以用一个长度可变的字符串决定，所以使用一个 RNN 的 controller 来生成这样的字符串，即使用控制器来采样得到一个网络结构 A，在该神经网络结构下训练数据并且得到相应的验证集上的准确率 R，使用该准确率来表征本次搜索得到的神经网络结构的好坏，进而将此作为信号来训练 RNN 控制器，利用 policy gradient 来更新 RNN 控制器，从而得到更好的结果。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/overview_NAS_with_RL.png"></p><h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>首次提出 NAS 的概念，成功为深度学习挖了新坑 (逃</p><h2 id="Generate-Model-with-RNN"><a href="#Generate-Model-with-RNN" class="headerlink" title="Generate Model with RNN"></a>Generate Model with RNN</h2><p>这里使用一个由 RNN 组成的控制器来生成网络结构的各种超参数，假设前向传播的网络都是卷积层，那么生成每一层卷积的结构可以用下图实现：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/controller_rnn_sample.png"></p><p>上图中每一个预测结果都由 softmax 来得到，将得到的结果作为下一个时间步的输入，当网络深度达到一定时，停止生成。每当 RNN 生成一个结构后，整个网络就会进行训练，然后把在验证集上的精度作为反馈来更新 RNN 中的参数 $\theta_c$</p><h2 id="Training-with-RL"><a href="#Training-with-RL" class="headerlink" title="Training with RL"></a>Training with RL</h2><p>可以把预测的结果看成是一系列用于产生 child network 的 action: $\alpha_{1:T}$，然后 child network 会得到一个在验证集上的准确度 $R$，使用这个 $R$ 作为 reward signal 并使用强化学习来训练控制器，为了找到最佳的结构，控制器需要最大化 reward：<br>$$<br>J(\theta_{c}) = E_{P(a_{1:T}; \theta_{c})}[R]<br>$$</p><p>由于 $R$ 是不可微的，所以作者使用策略梯度(policy gradient)的方式来更新控制器中的参数：<br>$$\nabla_{\theta_{c}} J\left(\theta_{c}\right)=\sum_{t=1}^{T} E_{P\left(a_{1: T} ; \theta_{c}\right)}\left[\nabla_{\theta_{c}} \log P\left(a_{t} \mid a_{(t-1): 1} ; \theta_{c}\right) R\right]$$</p><p>上式的计算可以近似为：<br>$$<br>\frac{1}{m} \sum_{k=1}^{m} \sum_{t=1}^{T} \nabla_{\theta_{c}} \log P\left(a_{t} \mid a_{(t-1): 1} ; \theta_{c}\right) R_{k}<br>$$</p><p>其中 $m$ 为采样的不同的结构数目，$T$ 是控制器必须预测的超参数的数目，$R_{k}$ 是第  k个神经网络结构在训练数据集上训练后所达到的验证集精度。上式是梯度的一个无偏估计，但有一个很高的方差，为了减少这个方差作者使用了一个 baseline function $b$：<br>$$<br>\frac{1}{m} \sum_{k=1}^{m} \sum_{t=1}^{T} \nabla_{\theta_{c}} \log P\left(a_{t} \mid a_{(t-1): 1} ; \theta_{c}\right)(R_{k} - b)<br>$$</p><p>只要 baseline function 不依赖于当前的动作，那就仍然是一个梯度的无偏估计，在文中 $b$(baseline function) 是之前网络结构准确度的一个移动指数平均。</p><h2 id="Parallelism-and-Asynchronous-Updates"><a href="#Parallelism-and-Asynchronous-Updates" class="headerlink" title="Parallelism and Asynchronous Updates"></a>Parallelism and Asynchronous Updates</h2><p>这一块主要是为了加速计算，一共有 $S$ 个 Parameter Server用于存储 $K$ 个 Controller Replica 的共享参数。然后每个 Controller Replica 生成 $m$ 个并行训练的自网络，controller 会根据 $m$ 个子网络结构在收敛时得到的结果收集得到梯度值，然后为了更新所有 Controller Replica，会把梯度值传递给 Parameter Server，当训练迭代次数超过一定次数则认为子网络收敛。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/accurate_compute_in_nas_training.png"></p><h2 id="Add-Skip-Connection-and-Other-Layers"><a href="#Add-Skip-Connection-and-Other-Layers" class="headerlink" title="Add Skip Connection and Other Layers"></a>Add Skip Connection and Other Layers</h2><p>这部分是介绍 set-selection type attention 方法用于添加类似于 ResNet 的跳接结构以及类似于 GoogleNet 的 branching 结构。具体的，在第 $N$ 层，作者添加了一个 anchor point，用于根据前面得到的 $N-1$ 个层的 $sigmoid$ 来判断是否与前面的 $N-1$ 层相连，每个 $sigmoid$ 都是当前控制器的 hidden state 和之前 $N-1$ 个 anchor points 的 hidden states 的方程：<br>$$<br>\mathrm{P}(\text { Layer } \mathrm{j} \text { is an input to layer } \mathrm{i})=\operatorname{sigmoid}\left(v^{\mathrm{T}} \tanh \left(W_{\text {prev}} * h_{j}+W_{\text {curr}} * h_{i}\right)\right)<br>$$</p><p>其中 $h_{j}$ 表示控制器在第 $j$ 个 layer 的 anchor point 的隐藏态，$j \in [0, N-1]$，这样就能够决定使用之前哪个层的输出作为当前层的输入了。其中矩阵 $W_{prev}, W_{curr}, v$ 都是可以训练得到的参数。由于这些连接可以通过这种方式被定义为概率分布，那么之前使用的强化学习方法就能够在不进行显著修改的情况下使用。</p><p>利用 anchor point 和 set-selection type attention 进行添加层级结构的示意图如下：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/nas_with_rl_form_skip_connections.png"></p><p>这里有几点需要注意的地方：</p><ol><li>如果一个层有来自多个之前层的输入，那么把它们在通道维度进行 concatenate。</li><li>如果一个层没有任何一个层的输出作为输入，那么把原图片作为输入。</li><li>在最后一层把所有没有被 connected 的输出在通道维度上  concatenate 起来作为输入。</li><li>如果需要 concatenated 的输入层有不同的 size，那么将小一点的层通过补 zero-padding 来保证一样大小。</li></ol><h2 id="Generate-Recurrent-Cell-Architectures"><a href="#Generate-Recurrent-Cell-Architectures" class="headerlink" title="Generate Recurrent Cell Architectures"></a>Generate Recurrent Cell Architectures</h2><p>考虑到普通 RNN 和 LSTM 的结构特点，作者还是使用了 LSTM 来生成最终的 recurrent Cells，这样相比于 RNN 就多了 2 个变量，分别是细胞态 $c_{t-1}$ 和 $c_{t}$</p><p>作者使用树这种数据结构来生成 Cells，在每一个时间步都需要将前一时间步的隐藏态 $h_{t-1}$，细胞态 $c_{t-1}$ 和当前的 $x_t$ 作为输入，从而得到输出的隐藏态 $h_{t}$ 和 细胞态 $c_{t}$，这 2 个输出又作为下一个时间步的输入。</p><p>控制器需要使用一种结合方法(例如 addition, element-wise multiplication) 对树中的每个节点进行标记，并使用一种激活函数来得到最终输出，为了让控制器 RNN 选择这些方法和函数，作者按顺序索引树中的节点，以便控制器 RNN 可以逐个访问每个节点并标记所需的超参数。</p><p>为了更好地进行说明这个过程，作者根据下图举了个例子，控制器首先需要预测 3 个 blocks, 每个 block 定义了一种结合方法和激活函数，然后需要预测最后 2 个 blocks，用于如何将 $c_{t}$ 和 $c_{t-1}$ 和树中的 2 个临时变量联系起来：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/example_of_nas_with_rl.png"></p><ul><li>左边的图表示每一个 recurrent Cell 都由一颗树组成，这棵树有 2 个叶子结点和 1 个内部结点，每颗树都代表了控制器预测的计算步骤。</li><li>中间的图表示控制器针对树中每个计算步骤作出的一组预测示例。</li><li>右边的图表示根据控制器预测示例结果所得到的 recurrent Cell 的计算图。</li></ul><p>根据上面的例子，会发生以下几个计算步骤：</p><ol><li>控制器为 index=0 的 tree 预测了 $Add$ 和 $Tanh$，所以需要计算 $a_{0} = tanh(W_{1} \cdot x_t + W_{2} \cdot h_{t-1})$</li><li>控制器为 index=1 的 tree 预测了 $ElemMult$ 和 $ReLU$，所以需要计算 $a_{1}=\operatorname{ReLU}\left(\left(W_{3} * x_{t}\right) \odot\left(W_{4} * h_{t-1}\right)\right)$</li><li>控制器为 “Cell Index” 的第二个元素预测了 0，说明 $a_0$ 的值需要更新, 为 “Cell Inject” 预测了 $Add$ 和 $ReLU$，所以需要计算 $a_{0}^{new} = ReLU(a_0 + c_{t-1})$</li><li>控制器为 index 2 的 tree 预测了 $ElemMult$ 和 $Sigmoid$，所以需要计算 $a_2 = sigmoid(a_0^{new} \odot a_1)$，由于树的最大 index 是 2，所以令 $h_t = a_2$</li><li>控制器为 “Cell Index” 的第一个元素预测了 1，意味着应该将输出的细胞态 $c_t$ 设置为 index=1 的 tree 在激活函数之前的输出：$c_{t} = (W_{3} * x_{t}) \odot (W_{4} * h_{t-1})$</li></ol><p>在上面的例子中一颗树只有 2 个叶结点，所以被称为 “base 2”，在实验中作者将 base number 设置为 8 来确保每个 Cell 的表达能力。</p><h2 id="Overview-of-NAS-Architecture"><a href="#Overview-of-NAS-Architecture" class="headerlink" title="Overview of NAS Architecture"></a>Overview of NAS Architecture</h2><p>可能还是有的同学不明白图 4 和图 5 的关系，其实图 4 展示的是如何生成每一层的卷积(每一层具体的网络结构，例如卷积核的大小，数量等)，而图 5 展示的是如何使用 LSTM 来生成整个网络结构(如何生成每个时间步的细胞态$c_t$ 和隐藏态$h_t$)，图 5 中每个时间步的输入 $x_t$ 其实就是上一个时间步的输出 $y_{t-1}$，网络的总体结构可以用下图来表示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/pipeline_nas.png"></p><p>上图中的隐藏态 $h_{t-1}$ 和细胞态 $c_{t-1}$ 其实就是之前卷积层 anchor point 得到的特征图，$x_t$ 是当前时间步的输入也是上一时间步的输出，为上图中的 $add$，$Tanh$ 等操作。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/nas_result_on_cifar10.png"></p><p>可以看到效果相对手工设计的网络来说还是不错的。</p><h1 id="NASNet"><a href="#NASNet" class="headerlink" title="NASNet"></a>NASNet</h1><p>原文是<a href="https://arxiv.org/pdf/1707.07012.pdf">Learning Transferable Architectures for Scalable Image Recognition</a></p><h2 id="Problems-in-NAS-with-RL"><a href="#Problems-in-NAS-with-RL" class="headerlink" title="Problems in NAS with RL"></a>Problems in NAS with RL</h2><p>上面的 NAS with RL 能够完成实在是因为 Google 太太太太太太太太(8个)有钱了，在 CIFAR-10 上都得用 800 个 GPU 跑快一个月，就更别提 ImageNet 这种数据集了，更加困难，也没哪个网络会在 CIFAR-10 这种数据集上说自己厉害，换句话说，NAS with RL 搜出来的结构没有在大规模的数据集上有良好的表现，所以还不足以让人信服，而且耗费时间太长，无论是学术界还是工业界，真的玩不起。这次的 NASNet 速度比上次快多了，500 块 GPU 只跑了 4 天就跑完了（再次劝退）。</p><h2 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution"></a>Contribution</h2><p>先搜索一个 Cell，再通过堆叠更多的这些网络单元生成最终的网络，这样搜出来的网络被称为<code>NASNet</code>，有点类似于直接搜索 residual block，然后把搜索得到的 residual block 堆叠起来就得到了最终的 NAS-ResNet， 真·曲线救国。</p><h2 id="Predetermined-Architectures"><a href="#Predetermined-Architectures" class="headerlink" title="Predetermined Architectures"></a>Predetermined Architectures</h2><h3 id="Convolutional-Cell"><a href="#Convolutional-Cell" class="headerlink" title="Convolutional Cell"></a>Convolutional Cell</h3><p>在 NASNet 中，完整的网络的结构还是需要手动设计的，NASNet 学习的是完整网络中被堆叠、被重复使用的网络单元。为了能够适应任何尺寸的图像，作者设计 2 种类型的卷积 Cell：</p><ol><li>Normal Cell: 输入和输出特征图的 size 相同</li><li>Reduction Cell: 输出特征图的 size 是输入特征图 size 的 $\frac{1}{2}$</li></ol><h3 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h3><p>作者为 CIFAR-10 和 ImageNet 分别进行了网络的设计，如图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/architectures_on_diff_datasets.png"></p><p>ImageNet 上用的图像尺寸更大，所以也有更多的 Reduction Cell。</p><h2 id="Search-Space"><a href="#Search-Space" class="headerlink" title="Search Space"></a>Search Space</h2><p>每个 Cell 都会接收 2 个 hidden states: $h_{i}$ 和 $h_{i-1}$ 作为输入，这 2 个 hidden states 要么是之前层的 hidden state 输出，要么是输入的原图片，然后给定这两个初始隐藏状态的情况下，控制器 RNN 递归地预测卷积单元的其余结构，控制器对每个 Cell 的预测分为 $B$ 个 block，其中每个 block 具有 5 个预测步骤，这些预测步骤由 5 个不同的 softmax 分类器做出，分别对应于一个 block 中元素的离散选择：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/controller_model_architecture_in_nasnet.png"></p><p>图中的 $N$ 为 Normal Cell 重复的次数，和初始卷积核个数一样是根据数据集的规模而制定的自由参数。上面的右图就是一个 block 的示例，可以看到每一个 block 的输入为 2 个 hidden states，输出为一个 hidden state，具体算法如下：</p><ol><li>从 $h_{i}$, $h_{i-1}$ 或者从先前 blocks 中已经创建的隐藏状态集里面选择一个 hidden state</li><li>按照第一步再选择一个 hidden state</li><li>对第一步中的 hidden state 选择一个操作</li><li>对第二步中的 hidden state 选择一个操作</li><li>选择一个方法来结合步骤 3 和步骤 4 中的输出，从而得到新的 hidden state</li></ol><p>上面的步骤 3 和步骤 4 中需要选择一个操作用于 hidden states，作者给出了以下的操作：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/options_in_nasnet.png"></p><p>上面的步骤 5 控制器 RNN 需要选择一个方法来结合 2 个隐藏态，这里设置为 element-wise add 以及 concatenation，最后所有卷积 Cell 中生成但未被使用的隐藏状态在 depth dimension 上进行 concatenate 用于最后 Cell 的输出。</p><p>为了让控制器 RNN 能够同时预测 Normal Cell 以及 Reduction Cell，作者简单地让控制器有 $2 \times 5B$ 个预测，第一个 $5B$ 用于预测 Normal Cell，第 2 个 $5B$ 用于预测 Reduction Cell，在原文的实验中 $B=5$ 取得了好的结果。</p><h2 id="Discovery"><a href="#Discovery" class="headerlink" title="Discovery"></a>Discovery</h2><p>作者除了使用 NAS with RL 中的强化学习方法以外还使用了随机搜索来搜索网络结构，对于随机搜索，不使用 softmax 来决定，而是从均匀分布中来取样从而得到结果。作者发现随机搜索的效果只比强化学习差一点点……作者身为资深炼丹师，对此作出了 2 个说明：</p><ol><li>作者认为 NASNet 所设计的搜索空间的结构合理，因此即便是随机搜索也可以执行得很好</li><li>随机搜索是一个很难战胜的 baseline…(completely fart..)</li></ol><h2 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h2><p>在 CIFAR-10 搜到的网络结构长这个样子：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/nasnet.png"><br>在 CIFAR-10 上的结果如下：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/nasnet_result_on_cifar10.png"><br>然后作者把这个结构放到 ImageNet 上重新训练，得到在 ImageNet 上的结果：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/nasnet_results_on_imagenet.png"></p><p>作者搜了 3 个精度最高的模型，分别称为 NASNet-A、NASNet-B 和 NASNet-C，上表中的 <code>4 @ 640</code> 表示 Normal Cell 重复的次数 $N=4$ 以及模型倒数第二层的卷积核数目为 640.</p><h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><h3 id="Scheduled-Drop-Path"><a href="#Scheduled-Drop-Path" class="headerlink" title="Scheduled Drop Path"></a>Scheduled Drop Path</h3><p>在优化类似于 Inception 的多分支结构时，以一定概率随机丢弃掉部分分支是避免过拟合的一种非常有效的策略，例如 DropPath。但是 DropPath 对 NASNet 不是非常有效。在 NASNet 的 Scheduled Drop Path 中，丢弃的概率会随着训练时间的增加线性增加。这么做的动机很好理解：训练的次数越多，模型越容易过拟合，DropPath 的避免过拟合的作用才能发挥的越有效。</p><h3 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h3><p>在 NASNet 中，强化学习的搜索空间大大减小，很多超参数已经由算法写死或者人为调整。这里说一下 NASNet 需要人为设定的超参数。</p><ol><li>激活函数统一使用 ReLU，实验结果表明 ELU nonlinearity 效果略优于 ReLU；</li><li>全部使用 Valid 卷积，padding 值由卷积核大小决定；</li><li>Reduction Cell 的 Feature Map 的数量需要乘以 2，Normal Cell 数量不变。初始数量人为设定，一般来说数量越多，计算越慢，效果越好；</li><li>深度可分离卷积在深度卷积和单位卷积中间不使用BN或ReLU</li><li>使用深度可分离卷积时，Search Space 中提到算法执行两次，DW 卷积一次，PW 卷积一次，作者发现这样能改善最终结果。</li><li>所有卷积遵循 ReLU-&gt;卷积-&gt;BN 的计算顺序</li><li>为了保持 Feature Map 的数量的一致性，必要的时候添加 $1 \times 1$ 卷积。</li></ol><h1 id="ENAS"><a href="#ENAS" class="headerlink" title="ENAS"></a>ENAS</h1><p>像上面的那 2 种方法一般人是玩不起的，因为计算量摆在那，为了解决这个问题 ENAS 被提出来了，原文是 <a href="https://arxiv.org/abs/1802.03268">Efficient Neural Architecture Search via Parameter Sharing</a>，ENAS 在一个 1080TI 的显卡上用了快 16 个小时把结构给搜出来了。</p><h2 id="Contribution-2"><a href="#Contribution-2" class="headerlink" title="Contribution"></a>Contribution</h2><ol><li>作者发现 NAS 最大的瓶颈就是为了得到准确度需要将每个子模型都训练到收敛，然后就把之前训练的权重都丢掉了，对于下一个子模型又重新开始训练直到收敛。</li><li>通过强制所有子模型共享权重来避免从头到尾地训练每个子模型从而提高 NAS 的效率。</li></ol><h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><p>作者观察到 NAS 最终迭代的所有图都可以看作是一个大图的子图，即我们可以把 NAS 的 Search Space 看成是一个有向无环图(DAG)，直观上来说，ENAS 的 DAG 是 NAS 搜索空间中所有可能的子模型的叠加，其中节点代表 local computations，边缘代表信息流，每个结点的 local computations 有它们自己的参数，这些参数仅在激活特定的计算时才使用，所以 ENAS 的设计允许所有的 child networks 共享参数。</p><h2 id="Designing-Recurrent-Cells"><a href="#Designing-Recurrent-Cells" class="headerlink" title="Designing Recurrent Cells"></a>Designing Recurrent Cells</h2><p>为了设计 Recurrent Cells，使用了包含 $N$ 个结点的 DAG，其中每个结点代表一个 local computations，边缘代表结点之间的信息流。ENAS 的控制器 RNN 决定以下 2 个事情：</p><ol><li>激活DAG 中的哪条边</li><li>在 DAG 的每个结点上进行哪些计算</li></ol><p>相比于把网络的拓扑结构固定为树并仅学习树上每个结点的操作，这种搜索空间允许 ENAS 在 RNN 单元中设计拓扑和操作，因此更加灵活。作者用 4 个结点的 DAG 来举例来说明 ENAS 的机制，假设 $x_t$ 为一个 recurrent Cell 的输入，$h_{t-1}$ 是之前时间步的输出，按照如下方式进行采样：</p><ol><li>对于 node 1：控制器首先采样一个激活函数，在本例子中假设为 tanh，那么这个 recurrent Cell 的 node 1 应该计算 $h_1 = tanh(x_t \cdot W^{(x)} + h_{t-1} \cdot W_{1}^{(h)})$</li><li>对于 node 2：控制器对之前的一个索引和一个激活函数进行采样，假设最终选择 $index=1$ 以及 ReLU，那么 recurrent Cell 的 node 2 应该计算 $h_2 = ReLU(h_1 \cdot W_{2,1}^{(h)})$</li><li>对于 node 3：控制器再次采样一个之前的索引和一个激活函数，假设为 $index=2$ 和  ReLU，那么 node 3 计算 $h_3 =  ReLU(h_2 \cdot W_{3,2}^{(h)})$</li><li>对于 node 4：控制器还是采样一个之前的索引和一个激活函数，假设为 $index=1$ 和  tanh，那么 node 4 计算 $h_4 = tanh(h_1 \cdot W_{4,1}^{(h)})$</li><li>作者将所有稀疏的结果求评价，即将所有未被选择作为其他结点输入的结点的输出求均值，在这个例子中， $index=1$ 和 $index=2$ 都被选中过了，那么这个 recurrent Cell 的输出就为 $(h_3 + h_4) / 2$，因为 node 3 和 node 4 都没有被选中。</li></ol><p>在上述的例子中，对于每个结点对 $j&lt;\ell$，都有一个独立的参数矩阵 $W_{\ell, j}$，通过选择之前的一个索引，控制器就能够决定使用哪个参数矩阵，因此在 ENAS 的策略中所有在同一个 Search Space 的 recurrent Cell 可以使用相同的参数集。</p><p>如果一个 recurrent Cell 有 $N$ 个结点和 4 个激活函数，那么搜索空间就有 $4^{N} \times N!$ 种配置，在论文中 $N=12$，意味着搜索空间中一共约有 $10^{15}$ 个模型。</p><h2 id="Training-ENAS-and-Deriving-Architectures"><a href="#Training-ENAS-and-Deriving-Architectures" class="headerlink" title="Training ENAS and Deriving Architectures"></a>Training ENAS and Deriving Architectures</h2><p>论文中的控制器网络是一个有 100 个 hidden units 的 LSTM，通过 softmax 以自回归的方式对决策进行采样，上一步的输出作为下一步的输入。在 ENAS 中需要学习 2 组参数，分别是控制器 LSTM 的参数 $\theta$ 以及所有 child networks 的共享参数 $\omega$，ENAS 的训练过程由 2 组交替进行的阶段组成：第一个阶段训练 child networks 共享的参数 $\omega$，第二个阶段训练控制器 LSTM 的参数 $\theta$.</p><h3 id="Training-shared-parameters"><a href="#Training-shared-parameters" class="headerlink" title="Training shared parameters"></a>Training shared parameters</h3><p>对于控制器的 policy $\pi(m; \theta)$，使用 SGD 来最小化误差函数 $\mathbb{E}<em>{\mathbf{m} \sim \pi}[\mathcal{L}(\mathbf{m} ; \omega)]$，其中 $\mathcal{L}(\mathbf{m} ; \omega)$ 是标准的交叉熵损失，在一个训练集的 min-batch 上计算得到，这个损失是由模型 $\mathbf{m}$ 的得到的，梯度可以利用蒙特卡洛算法近似求得：<br>$$<br>\nabla</em>{\omega} \mathbb{E}<em>{\mathbf{m} \sim \pi(\mathbf{m} ; \theta)}[\mathcal{L}(\mathbf{m} ; \omega)] \approx \frac{1}{M} \sum</em>{i=1}^{M} \nabla_{\omega} \mathcal{L}\left(\mathbf{m}_{i}, \omega\right)<br>$$</p><p>虽然这个估计相比于标准的 SGD 有更高的方差，但作者发现 $M=1$ 的效果很好，即可以用 $\pi(m; \theta)$ 中采样的任何一个 single 模型的梯度来更新参数 $\omega$</p><h3 id="Training-controller-parameters"><a href="#Training-controller-parameters" class="headerlink" title="Training controller parameters"></a>Training controller parameters</h3><p>这里需要固定 $\omega$，然后最大化期望的 reward：<br>$$<br>\mathbb{E}_{\mathbf{m} \sim \pi(\mathbf{m}: \theta)}[\mathcal{R}(\mathbf{m}, \omega)]<br>$$</p><p>这里和 NAS with RL ，梯度是利用强化学习方法来计算，使用带有移动指数平均的 baseline function 来减少方差。</p><h3 id="Deriving-Architecture"><a href="#Deriving-Architecture" class="headerlink" title="Deriving Architecture"></a>Deriving Architecture</h3><p>首先从已经训练好的 policy $\pi(m; \theta)$ 中采样得到几个模型，对每一个采样得到的模型，计算其在验证集的一个 single mini-batch 的 reward，只选择 reward 最高的模型来从头训练。</p><h2 id="Designing-Convolutional-Networks"><a href="#Designing-Convolutional-Networks" class="headerlink" title="Designing Convolutional Networks"></a>Designing Convolutional Networks</h2><p>在之前的 Designing Recurrent Cells 中，控制器 RNN 需要对每个 decision block 采样 2 个 decisions:</p><ol><li>决定连接之前哪个 node</li><li>决定使用哪种激活函数</li></ol><p>对于卷积模型的搜索空间，控制器 RNN 也需要对每个 decision block 采样 2 个：</p><ol><li>决定连接之前哪个 node</li><li>决定使用哪种计算操作</li></ol><p>决定连接之前哪个 node 是用于实现 skip connection，例如对于第 $k$ 个 layer，已经有 $k-1$ 个采样过的索引了，即对于第 $k$ 个 layer，有 $2^{k-1}$ 种可能，举个例子，在 $k=4$ 的 layer，控制器采样了索引 {1, 3}，所以 layer 1 和 layer 3 的输出在 depth dimension 上 concatenated 后作为 layer 4 的输入。</p><p>对于决定使用哪种计算操作，决定了把这个层是设置为卷积层还是池化层，原文中有 6 种操作，分别是：</p><ul><li>$3 \times 3$ convolution</li><li>$5 \times 5$ convolution</li><li>$3 \times 3$ depthwise-separable convolution</li><li>$5 \times 5$ depthwise-separable convolution</li><li>$3 \times 3$ max-pooling</li><li>$5 \times 5$ avg-pooling</li></ul><p>假设一个网络有 $L$ 层，那么就需要做 $L$ 次采样，所以搜索空间中总的模型数量为 $6^L \times 2^{L(L-1)/2}$，作者实验中 $L=12$，即有 $1.6 \times 10^{29}$ 个可能的网络。</p><h2 id="Designing-Convolutional-Cells"><a href="#Designing-Convolutional-Cells" class="headerlink" title="Designing Convolutional Cells"></a>Designing Convolutional Cells</h2><p>这里和 NASNet 做的事情是一样的，就是希望搜索一个 block，再把 block 进行堆叠从而得到更好的结果。作者使用含有 $B$ 个结点的 DAG 来表示每个 Cell 本地的计算，这里面 node 1 和 node 2 作为 Cell 的输入，对于剩下的 $B-2$ 个 nodes，使用控制器 RNN 来做 2 个决定：</p><ol><li>决定之前的哪 2 个 node 的输出作为当前结点的输入</li><li>决定对当前采样得到的 2 个结点使用哪 2 种操作</li></ol><p>这里的 5 种操作为：</p><ul><li>identity</li><li>$3 \times 3$ depthwise-separable convolution</li><li>$5 \times 5$ depthwise-separable convolution</li><li>$3 \times 3$ max-pooling</li><li>$3 \times 3$ avg-pooling</li></ul><p>作者以 $B=4$ 举了个例子来说明：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/controller_example_in_designing_conv_Cell.png"></p><ol><li>node 1 和 node 2 都是输入的结点，所以不需要做什么，$h_1$ 和 $h_2$ 就是它们的输出</li><li>从上图中左边的部分，看到控制器对 node 3 进行了采样，采样结果为 node 2，node 2，separable_conv_$5 \times 5$$(h_2)$ 和 $identity$，所以 $h_3 = sep_{conv_{5 \times5}}(h_2) + id(h_2)$</li><li>看到控制器对 node 4 进行了采样，采样结果为 node 3，node 1，avg_pool_3x3 和 sep_conv_3x3，所以 $h_4 = $ avg_pool_3x3($h_3$) + sep_conv_3x3($h_1$)</li><li>$h_4$ 没有被任何一个其他 node 所用到作为输入，所以 $h_4$ 就是 Cell 的输出</li></ol><p>上面说的都是 Normal Cell，对于 Reduction Cell，只需要把 stride 设置为 2 即可。</p><h2 id="Results-2"><a href="#Results-2" class="headerlink" title="Results"></a>Results</h2><p>先看下搜出来的 Normal Cell 和 Reduction Cell<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/enas_results_on_Cells.png"><br>再看下用于分类时直接搜出来整个网络<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/enas_network.png"><br>最后看下结果<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/enas_results.png"></p><h1 id="MnasNet"><a href="#MnasNet" class="headerlink" title="MnasNet"></a>MnasNet</h1><p>把在移动端上的推理时延作为了一个限制条件(也是 reward)，来进行特定条件下的 NAS<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/mnasnet_overview.png"></p><h2 id="Contribution-3"><a href="#Contribution-3" class="headerlink" title="Contribution"></a>Contribution</h2><ol><li>提出了一种多目标任务的神经网络搜索，使得在移动设备上能同时实现精度和推理速度</li><li>提出了一种分解式分层搜索空间，在灵活性和搜索空间大小之间取得了 trade-off</li><li>搜出来的网络在移动端推理速度的限制下，在 ImageNet 分类任务和 COCO 检测下都取得了好结果</li></ol><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p>作者考虑实际情况，希望能找到一个同时具有高精度和低推理时延的模型，直接上公式：<br>$$\begin{array}{ll}<br>\underset{m}{\operatorname{maximize}} &amp; A C C(m) \<br>\text { subject to } &amp; L A T(m) \leq T<br>\end{array}$$</p><p>$ACC(m)$ 表示模型 $m$ 在给定任务下的准确率，$LAT(m)$ 为特定移动端上的推理时延，$T$ 是目标推理时延，通常的做法是把 $T$ 看成一个硬限制。但这样的做法不能得到多个 Pareto 最优解，Pareto 最优解是指在没有增加推理时延的情况下能达到最佳精度或者在没有减少精度的情况下有最短的推理时延。为了解决这个问题，作者使用一种加权乘积法来近似  Pareto 最优解，这样优化目标就变成了：<br>$$<br>\operatorname{max}_{m} \quad A C C(m) \times\left[\frac{L A T(m)}{T}\right]^{w}<br>$$</p><p>其中 $w$ 是权重系数：<br>$$w=\left{\begin{array}{ll}<br>\alpha, &amp; \text { if } L A T(m) \leq T \<br>\beta, &amp; \text { otherwise }<br>\end{array}\right.$$</p><p>这样<code>其实是把上面的硬约束转化为了一种软约束</code>，$w$ 是由特定的平台所决定的，但作者认为比较好的 $\alpha$ 和 $\beta$ 应该使得  Pareto 最优解在不同的 精度-速度 的 trade-off 中有相近的 reward，例如，作者发现推理时间每增加一倍，准确度大约会提升 5%，那么对于给定的 2 个模型：一个的推理时间为 $l$，准确度为 $a$，另一个模型的推理时间为 $2l$，准确度为 $1.05a$，这 2 个模型的 reward 应该是相近的：<br>$$<br>Reward(M2) = a \cdot (1 + 0.05) \cdot (2l/T)^{\beta} \approx Reward(M1) = a \cdot (l/T)^{\beta}<br>$$</p><p>以上面为例可以解得 $\beta \approx -0.07$，作者在实验中设置为 $\alpha = \beta = -0.07$，作者也做了实验进行了硬约束和软约束的对比：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/mnasnet_compare_hard_soft_contraist.png"></p><p>图中上半部分为硬约束的目标值，下半部分是软约束的目标值，目标值就是要优化目标 $A C C(m) \times\left[\frac{L A T(m)}{T}\right]^{w}$</p><h2 id="Architecture-Search"><a href="#Architecture-Search" class="headerlink" title="Architecture Search"></a>Architecture Search</h2><p>大部分方法都是只搜索一小部分 Cell，然后把它们给堆叠起来，作者认为这对于实现高精度和低延迟至关重要。所以作者引入了分解式分层搜索空间，将整个模型分为若干个单独的 block，每个 block 分为不同的 layer，每个 block 中的 layer 是一样的，这样做的原理是作者认为需要根据输入和输出的形状来搜索最佳操作以获得更好的 accuracy-latency trade-off。例如，浅层的 CNN 要处理比较大的数据，所以网络的浅层对推理时延的影响要比网络的深层大。搜索空间的结构如下图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/mnasnet_search_space.png"></p><p>可以看到把模型分成了一系列预定义好的 blocks，逐渐减少分辨率并增大通道数，每个 block 都有若干个 layer，每个 layer 相同，每个 layer 的操作和连接由每个 block 的子搜索空间决定，具体地，每个 block $i$ 的子空间由以下几个选择组成：</p><ol><li>卷积操作($ConvOp$)：regular conv、depthwise conv、 mobile inverted bottleneck conv</li><li>卷积核尺寸($Kernel Size$)：$3 \times 3$、$5 \times 5$</li><li>$SERatio$：0、0.25</li><li>Skip Options：pooling、identity residual、no skip</li><li>每个 block 输出的通道数 $F_i$</li><li>每个 block 的 layer 数目 $N_i$</li></ol><h2 id="Search-Algorithm"><a href="#Search-Algorithm" class="headerlink" title="Search Algorithm"></a>Search Algorithm</h2><p>同样使用强化学习来为多目标优化问题找最优解，这里用强化学习的原因是其 reward 容易自定义，但作者认为其他优化算法也是有效的。具体的说，遵循和 NAS with RL 一样的策略，将搜索空间中的每个 CNN 模型映射到 tokens 列表，这些 tokens 由强化学习基于其参数 $\theta$ 的一系列 action $a_{1:T}$ 决定，最终的目标是最大化期望的 reward：<br>$$<br>J=E_{P\left(a_{1: T} ; \theta\right)}[R(m)]<br>$$</p><p>$m$ 是由一系列 action $a_{1:T}$ 决定的被采样的模型，$R(m)$ 是目标值 $A C C(m) \times\left[\frac{L A T(m)}{T}\right]^{w}$。</p><p>训练过程和 NAS with RL 一样，在每个 step 中，控制器首先根据其 RNN 的 softmax logit 预测义序列的 tokens，从而使用其当前参数 θ 对一批模型进行采样，对于每个被采样的模型 $m$，计算 $ACC(m)$ 和 $LAT(m)$，然后得到 reward value $R(m)$，在每个 step 的最后最大化期望的 reward 从而更新控制器的参数 $\theta$，重复进行直到达到最大 step 或者 $\theta$ 收敛。</p><h2 id="Results-3"><a href="#Results-3" class="headerlink" title="Results"></a>Results</h2><p>先看在 ImageNet 上的结果，MnasNet-A1 是搜索出来的 baseline，MnasNet-A2 和 MnasNet-A3 是在不同的推理时延下用相同方法和参数搜出来的其他模型。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/mnasnet_results.png"><br>再看目标检测上的结果<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/mnasnet_results_coco.png"></p><p>接下来的消融性验证，之前已经比较过了软约束和硬约束的目标值，作者还比较了软约束和硬约束的搜索结果，当 $\alpha=0, \beta=-1$ 时推理时延被看成是硬约束，所以控制器倾向于采样更快的模型以避免推理时间的惩罚，当 $\alpha= \beta=-0.07$ 时推理时延被看成是软约束，所以如图所示产生的模型在推理时延上有比较大的范围。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/mnasnet_search_results.png"></p><p>作者认为自己方法好的原因主要在于多目标奖励和新的搜索空间，作者也对此进行了实验，可以看出多目标奖励会减少推理时延也会降低精度，但是新的搜索空间让精度大大增加了。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/mnasnet_ablation_study.png"></p><p>最后看下搜出来的 baseline —— MnasNet-A1 和搜出来的相关 layers<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/mnasnet_structures_and_layers.png"></p><h1 id="EfficientNet"><a href="#EfficientNet" class="headerlink" title="EfficientNet"></a>EfficientNet</h1><h2 id="Contribution-4"><a href="#Contribution-4" class="headerlink" title="Contribution"></a>Contribution</h2><ol><li>分析了网络三个维度(深度、宽度和分辨率)的缩放对结果的影响</li><li>提出了联合缩放来平衡网络三个维度的缩放</li><li>通过对 MnasNet 搜到的 EfficientNet-B0 进行缩放得到了更快更好的 EfficientNet 系列</li></ol><h2 id="Idea-1"><a href="#Idea-1" class="headerlink" title="Idea"></a>Idea</h2><p>在以往的模型设计中，通常只对深度、宽度和分辨率这 3 个维度中的一个来进行缩放(e.g. ResNet 缩放深度，InceptionNet 缩放宽度)，尽管可以任意缩放两个或三个维度，但是任意缩放需要繁琐的手动调整参数，并且仍然会导致精度和效率的不尽人意。</p><p>作者发现使用一组固定的缩放系数来均匀缩放网络宽度，深度和分辨率，从而使得网络深度、宽度和分辨率的能够保持一个平衡，这对好的结果至关重要。具体的说，如果有 $2^N$ 倍的计算资源，那么就可以简单的把网络的深度、宽度和分辨率分别增加到原来的 $\alpha^N$， $\beta^N$ 和  $\gamma^N$ 倍，其中 $\alpha$、$\beta$ 和 $\gamma$ 是在原来的小模型(e.g. MobileNet)上利用网格搜索得到的常系数，以这种方式加大模型的参数和复杂度，从而得到更好的结果。</p><h2 id="Compound-Model-Scaling"><a href="#Compound-Model-Scaling" class="headerlink" title="Compound Model Scaling"></a>Compound Model Scaling</h2><h3 id="Problem-Formulation-1"><a href="#Problem-Formulation-1" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>一个卷积层 $i$ 可以被定义成一个公式 $Y_i = \mathcal{F}<em>i (X_i)$，$Y_i$，$X_i$ 分别树输出和输入的 tensor，所以一个卷积网络可以表示为多个卷积层的列表：<br>$$<br>\mathcal{N}=\mathcal{F}_{k} \odot \ldots \odot \mathcal{F}</em>{2} \odot \mathcal{F}<em>{1}\left(X</em>{1}\right)=\odot_{j=1 \ldots k} \mathcal{F}<em>{j}\left(X</em>{1}\right)<br>$$</p><p>在例如 ResNet 的很多网络中都有很多的 Stage，每个 Stage 除了第一个 layer 用于降采样以外，都有着相同的结构，所以一个卷积网络可以表示为下面的形式：<br>$$<br>\mathcal{N}=\bigoplus_{i=1 \ldots s} \mathcal{F}<em>{i}^{L</em>{i}}\left(X_{\left\langle H_{i}, W_{i}, C_{i}\right\rangle}\right)<br>$$</p><p>其中 $\mathcal{F}<em>{i}^{L</em>{i}}$ 表示在第 $i$ 个 stage 中，卷积层 $\mathcal{F}<em>{i}$ 重复 $L_i$ 次，$\left\langle H_{i}, W</em>{i}, C_{i}\right\rangle$ 表示输入 tensor 的 shape，通常的卷积网络设计是为了得到更好的 $\mathcal{F}<em>{i}$，而本文却是希望固定 $\mathcal{F}</em>{i}$，通过缩放网络的深度 $L_i$，宽度 $C_i$ 和分辨率 $(H_i, W_i)$ 来得到好的网络结构。为了减少搜索空间，作者限制所有 layers 都必须以恒定的比率进行均匀缩放，因此整个问题就可以被定义为在约束条件下的最优化问题：<code>公式(2)</code><br>$$\begin{array}{ll}<br>\max <em>{d, w, r} &amp; \operatorname{Accuracy}(\mathcal{N}(d, w, r)) \<br>\text {s.t.} &amp; \mathcal{N}(d, w, r)=\odot</em>{i=1 \ldots s} \hat{\mathcal{F}}<em>{i}^{d \cdot \hat{L}</em>{i}}\left(X_{\left\langle r \cdot \hat{H}<em>{i}, r \cdot \hat{W}</em>{i}, w \cdot \hat{C}<em>{i}\right\rangle}\right) \<br>&amp; \text { Memory }(\mathcal{N}) \leq  target</em>{memory}  \<br>&amp; \text {FLOPS}(\mathcal{N}) \leq  target_{flops}<br>\end{array}$$</p><p>$d, w, r$ 是缩放网络深度，宽度和分辨率的系数，$\hat{\mathcal{F}}<em>{i}, \hat{L}</em>{i}, \hat{H}<em>{i}, \hat{W}</em>{i}, \hat{C}_{i}$ 是预定义好的参数。</p><h3 id="Scaling-Dimensions"><a href="#Scaling-Dimensions" class="headerlink" title="Scaling Dimensions"></a>Scaling Dimensions</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/efficientnet_model_scaling.png"></p><p>作者基于 2 个观察到的现象进行模型的缩放：</p><ol><li>增大网络的深度、宽度和分辨率中的任何一个都能够提升准确率，但是准确率的提升速度会随着模型的增大而降低。</li><li>作者观察到不同的缩放维度都不是独立的，所以为了得到更好的准确度和效率，缩放时必须平衡好所有的网络维度。</li></ol><ul><li>关于现象 1 的举例：这个应该很明显了，网络越深越不好训练，虽然 ResNet 很大程度上解决了这个问题，但这个问题依然是存在的。</li><li>关于现象 2 的举例：简单的例子就是对于大分辨率的图像应该用更深的网络来得到更大的感受野，得到更好的特征，也可以用更宽的网络来得到更加细粒度的特征。</li></ul><p>作者也对现象 2 做了实验：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/efficientnet_results_scaling_parameter.png"></p><p>得到的结论是如果只对宽度 $w$ 进行缩放，而不缩放深度和分辨率的话，精度会很快达到饱和(对应上图中的 $d=1.0, r=1.0$)</p><h3 id="Compound-Scaling-Methods"><a href="#Compound-Scaling-Methods" class="headerlink" title="Compound Scaling Methods"></a>Compound Scaling Methods</h3><p>作者使用一个复合系数 $\phi$ 来对网络深度、宽度和分辨率进行均匀缩放：<code>公式(3)</code><br>$$<br>\begin{array}{ll}<br>depth: d=\alpha^{\phi} \<br>width: w=\beta^{\phi} \<br>resolution: r=\gamma^{\phi} \<br>s.t. \alpha \cdot \beta^{2} \cdot \gamma^{2} \approx 2 \<br>\alpha \geq 1, \beta \geq 1, \gamma \geq 1 \<br>\end{array}<br>$$</p><p>其中 $\alpha, \beta, \gamma$ 是由一个小的 grid search 决定的常量，这里需要注意的是，一个标准卷积的 FLOPs 和 $d, w^2, r^2$ 成正比，即如果网络的深度加倍，那么 FLOPs 会加倍，而网络的宽度或者分辨率的加倍会导致 FLOPs 增加四倍，这也是上式中 $s.t. \alpha \cdot \beta^{2} \cdot \gamma^{2} \approx 2$ 的由来。这里的 2 是作者设置的，希望对于新的计算资源 $\phi$，总的 FLOPs 会大约增加 $2^{\phi} 。$</p><h2 id="EfficientNet-Architecture"><a href="#EfficientNet-Architecture" class="headerlink" title="EfficientNet Architecture"></a>EfficientNet Architecture</h2><h3 id="EfficientNet-B0"><a href="#EfficientNet-B0" class="headerlink" title="EfficientNet-B0"></a>EfficientNet-B0</h3><p>上面已经说到了模型三个维度的缩放并不影响每一卷积层的表现方式 $\hat{\mathcal{F}}_i$，所以有一个好的 baseline 是很重要的事情，只有 baseline 好了，那么基于 baseline 并经过 scaling 之后的模型才能有好的表现。然后作者使用 MnasNet 一样的办法，在把 accuracy 和 FLOPs 都作为优化目标的情况下来搜索网络结构，搜索空间也和 MnasNet 一样，不同的是这里的优化目标是 FLOPs 而不是延迟，因为并没有在任何特定的硬件上测试。作者把这样搜出来的网络作为 baseline 进行 scaling，并且称作 Efficient-B0.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/efficient-b0.png"></p><h3 id="EfficientNet-B1-to-EfficientNet-B7"><a href="#EfficientNet-B1-to-EfficientNet-B7" class="headerlink" title="EfficientNet-B1 to EfficientNet-B7"></a>EfficientNet-B1 to EfficientNet-B7</h3><p>有了 EfficientNet-B0 作为 baseline，就开始使用  Compound Scaling Methods 来进行缩放，缩放步骤如下：</p><ol><li>首先固定 $\phi=1$，在上面提到的公式(2)和公式(3)上做一个小的 grid search，对于 EfficientNet-B0  作者得到的最佳值是 $\alpha = 1.2, \beta = 1.1, \gamma =1.15$</li><li>公式(3)上固定 $\alpha, \beta, \gamma$ 的值，放大 $\phi$ 的值，最终得到 EfficientNet-B1 to EfficientNet-B7。</li></ol><p>作者还提了一嘴，直接在大模型上做网格搜索求得 $\alpha, \beta, \gamma$ 可能会得到更好的结果，但也会导致更大的计算量，所以这里只是在小网络(Efficient-B0)上搜索 $\alpha, \beta, \gamma$，然后进行缩放。</p><h2 id="Results-4"><a href="#Results-4" class="headerlink" title="Results"></a>Results</h2><p>先看来最终的结果，不同的 Efficient 在 ImageNet 上的精度：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/efficientnet_results.png"></p><p>作者也对比了只缩放网络深度、宽度或者分辨率中一种的结果，可以看出联合缩放的效果更好。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/efficientnet_results_compound_scaling.png"></p><p>作者也做实验并讨论了为什么联合缩放会更有效，作者给出了不同缩放方法的类激活图(Class Activation Map)，可以看出使用联合缩放能够让模型关注具有更多物体细节的区域，而单独对某个维度进行缩放会导致模型缺失物体细节或者无法捕获图像中的所有物体。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/efficientnet_results_cam.png"></p><p>最后来张广为流传的图吧，FLOPs vs. ImageNet Accuracy<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/efficientnet_results_image.png"></p><h1 id="Darts"><a href="#Darts" class="headerlink" title="Darts"></a>Darts</h1><h2 id="Contribution-5"><a href="#Contribution-5" class="headerlink" title="Contribution"></a>Contribution</h2><ol><li>提出了基于 bilevel 优化的可微分 NAS算法</li><li>可微分 NAS 显著地提升了 NAS 的效率</li><li>作者的实验表明，Darts 搜出来的网络不仅精度高，也能够很好地迁移到其他数据集上(实验只看到了分类有比较好的迁移性)</li></ol><h2 id="Idea-2"><a href="#Idea-2" class="headerlink" title="Idea"></a>Idea</h2><p>上面提到的所有算法的搜索空间都是离散且不可微的，DARTS 通过把搜索空间连续松弛化，这样就可以通过梯度下降来搜索网络结构。</p><h2 id="Search-Space-1"><a href="#Search-Space-1" class="headerlink" title="Search Space"></a>Search Space</h2><p>和 NASNet 一样先搜索 Cell，然后堆叠起来作为最终的网络结构，这里的搜索空间和 ENAS 差不多每个 Cell 可以看作是一个有着 $N$ 个结点的 DAG 图，每个结点 $x^{(i)}$ 是一种潜在的表示(可以理解为卷积网络的特征图)，每条边和能够改变结点 $x^{(i)}$ 的操作 $o^{(i, j)}$ 相关联。</p><p>假设每个 Cell 有 2 个输入结点和 1 个输出结点，那么对于 convolutional Cells 而言，输入结点就是前面 2 层 Cell 的输出，这里和 NASNet 的定义是一样的，对于 recurrent Cells 而言，每个 Cell 的输出是由 Cell 中所有的中间结点经过一个 reduction operation 得到的(e.g. concatenation)，每个中间结点 $x^{(j)}$ 都是根据其所有的前面得到的结点进行计算得到：<br>$$<br>x^{(j)}=\sum_{i&lt;j} o^{(i, j)}\left(x^{(i)}\right)<br>$$</p><p>这里也有个 zero operation，表明 2 个结点之间缺乏连接(没有任何的 operation)，学习每个 Cell 本质上就是学习 DAG 中每条边上的操作 operation。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/darts_overview.png"></p><p>对照着解释下上面的图：</p><ul><li>(a)：定义一个 Cell，一个 Cell 可以表示为 DAG 的形式，这个例子中是 4 个结点，每条边代表了一种操作(e.g. $3 \times 3 \quad conv$)，初始化的时候为 unknown</li><li>(b)：通过在每条边上执行混合候选操作来对搜索空间进行连续松弛化(以 Softmax 权值叠加的方式)，把每条边都看成是子操作。</li><li>(c)：通过解决双层优化问题（bilevel optimization problem）来对混合概率和网络权重进行联合优化，上图中边的粗细可以看成是每个子操作的概率</li><li>(d)：从学习到的混合概率中得出最后的网络结构，就是选择概率最大子操作(最粗的边)</li></ul><p>不难看出重点就在于如何解决 (b) 和 (c)，接下来要说的就是如何解决这 2 个问题。</p><h2 id="Continuous-Relaxation-and-Optimization"><a href="#Continuous-Relaxation-and-Optimization" class="headerlink" title="Continuous Relaxation and Optimization"></a>Continuous Relaxation and Optimization</h2><p>设 $\mathcal{O}$ 是可选的操作集合(e.g. convolution，max pooling，zero)，其中每个操作代表用于结点 $x^{(i)}$ 上的函数 $o(\cdot)$，为了让搜索空间变得连续，作者将特定操作的分类选择放宽到了一个 softmax 上，这个 softmax 包含了所有可能的操作：<br>$$<br>\bar{o}^{(i, j)}(x)=\sum_{o \in \mathcal{O}} \frac{\exp \left(\alpha_{o}^{(i, j)}\right)}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left(\alpha_{o^{\prime}}^{(i, j)}\right)} o(x)<br>$$</p><p>每对结点 $(i,j)$ 的混合权重是一个维度为 $|\mathcal{O}|$ 的向量 $\alpha^{(i,j)}$，这样就把整个结构的搜索变成了学习一组连续变量 $\alpha=\left{\alpha^{(i, j)}\right}$，在搜索的最后可以通过把每个混合操作 $\bar{o}^{(i, j)}$ 替换为可能性最大的操作 ($o^{(i, j)}=\operatorname{argmax}<em>{o \in \mathcal{O}} \alpha</em>{o}^{(i, j)}$) 来得到一个离散的结构，这里的 $\alpha$ 可以看成是网络结构的 encoding，其实也就是和 ENAS 一样，是 2 个结点之间的共享权重。</p><p>在对搜索空间进行松弛化以后，接下来在所有的混合操作中共同学习网络架构 $\alpha$ 和权重 $w$，和其他方法不同的是，这是使用梯度下降来优化 validation loss，而不是用 RL 或者进化算法来最大化期望的 reward 或者 fitness</p><p>用 $ \mathcal{L}<em>{train}$ 和 $\mathcal{L}</em>{test}$ 分别表示训练误差和验证误差，均由网络结构 $\alpha$ 和网络权重 $w$ 决定，那么结构搜索的目标是找到一个网络结构 $\alpha^{<em>}$ 使得验证误差 $\mathcal{L}_{val}(w^{</em>}, \alpha^{<em>})$ 最小，通过最小化训练损失 $w^{</em>}=\operatorname{argmin}<em>{w} \mathcal{L}</em>{t r a i n}\left(w, \alpha^{<em>}\right)$ 来得到和网络结构相关的权重 $w^{</em>}$。这其实是一个双层的优化问题，其中 $\alpha$ 是上层变量，$w$ 是下层变量：<br>$$\begin{array}{ll}<br>\min <em>{\alpha} &amp; \mathcal{L}</em>{v a l}\left(w^{<em>}(\alpha), \alpha\right) \<br>\text { s.t. } &amp; w^{</em>}(\alpha)=\operatorname{argmin}<em>{w} \mathcal{L}</em>{\text {train}}(w, \alpha)<br>\end{array}$$</p><p>DARTS 的算法可以总结如下：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/darts_algo.png"></p><p>其实核心思想到这里就结束了，即如何把搜索空间变成连续且松散的，如何把 loss 作为优化目标。后面的部分是详细介绍了梯度下降的应用以及网络结构的搜索细节。下面的公式我其实也没看懂…但是还是能把论文翻译一下的…(大哭)</p><h2 id="Approximate-Architecture-Gradient"><a href="#Approximate-Architecture-Gradient" class="headerlink" title="Approximate Architecture Gradient"></a>Approximate Architecture Gradient</h2><p>由于损失的准确梯度很难得到，所以作者提出了一种近似方案：<br>$$\begin{aligned}<br>&amp; \nabla_{\alpha} \mathcal{L}<em>{v a l}\left(w^{*}(\alpha), \alpha\right) \<br>\approx &amp; \nabla</em>{\alpha} \mathcal{L}<em>{v a l}\left(w-\xi \nabla</em>{w} \mathcal{L}_{t r a i n}(w, \alpha), \alpha\right)<br>\end{aligned}$$</p><p>其中 $w$ 是算法保持的当前权重，$\xi$ 是学习率，这个想法是在训练中仅使用一个 step 就可以对 $w$ 进行调整来近似估计 $w^{*}(\alpha)$，而不是通过训练直到收敛来解决优化问题，值得注意的是当 $w$ 是一个局部最优时，上式可以简化为 $\nabla_{\alpha} \mathcal{L}<em>{v a l}(w, \alpha)$，此时 $\nabla</em>{w} \mathcal{L}_{train}(w, \alpha) = 0$。</p><p>把求导的链式准则应用到上面的近似梯度上可以得到<code>公式(7)</code>：<br>$$\nabla_{\alpha} \mathcal{L}<em>{v a l}\left(w^{\prime}, \alpha\right)-\xi \nabla</em>{\alpha, w}^{2} \mathcal{L}<em>{t r a i n}(w, \alpha) \nabla</em>{w^{\prime}} \mathcal{L}_{v a l}\left(w^{\prime}, \alpha\right)$$</p><p>其中 $w’ = w - \xi \nabla_{w} \mathcal{L}<em>{train}(w, \alpha)$ 表明了每一个 step 模型前向传播的权重，上面的表达式在第二项中包含了计算代价昂贵的矩阵向量乘积，作者使用有限差分近似来降低复杂度，令 $\epsilon$ 为一个比较小的标量，令 $w^{\pm}=w \pm \epsilon \nabla</em>{w^{\prime}} \mathcal{L}<em>{v a l}\left(w^{\prime}, \alpha\right)$，那么就有：<br>$$\nabla</em>{\alpha, w}^{2} \mathcal{L}<em>{\text {train}}(w, \alpha) \nabla</em>{w^{\prime}} \mathcal{L}<em>{\text {val}}\left(w^{\prime}, \alpha\right) \approx \frac{\nabla</em>{\alpha} \mathcal{L}<em>{\text {train}}\left(w^{+}, \alpha\right)-\nabla</em>{\alpha} \mathcal{L}_{\text {train}}\left(w^{-}, \alpha\right)}{2 \epsilon}$$</p><p>对有限差分进行评估仅需要对权重 $w$ 进行 2 次前向遍历和对网络结构的 encoding $\alpha$ 进行 2 次反向传播，使用有限差分后算法的时间复杂度就从 $O(|\alpha| \cdot |w|)$ 降低为了  $O(|\alpha| + |w|)$</p><p>当 $\xi = 0$ 时，公式(7)中的二阶导数就会消失，在这种情况下，梯度为 $\nabla_{\alpha} \mathcal{L}_{val}(w, \alpha)$，这是通过假设当前的 $w = w^*(\alpha)$，并利用启发式算法得到的。这会导致一些加速，但会降低性能，在后面的部分中将 $\xi=0$ 称为一阶逼近，将 $\xi &gt; 0$ 的梯度公式称为二阶逼近。</p><h2 id="Deriving-Discrete-Architectures"><a href="#Deriving-Discrete-Architectures" class="headerlink" title="Deriving Discrete Architectures"></a>Deriving Discrete Architectures</h2><p>为了得到离散结构中的每个结点，作者保留了从之前所有结点收集到的 non-zero 操作中强度最高的 Top-K 个操作，每个操作来自不同的结点，每个操作的强度由 $\frac{\exp \left(\alpha_{o}^{(i, j)}\right)}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left(\alpha_{o^{\prime}}^{(i, j)}\right)}$ 定义，这里直接排除了 zero operation，原因有 2 个：</p><ol><li>需要每个结点正好有 k 个非 0 输入边，以方便和现有方法进行比较</li><li>zero operation 的强度是不确定的，因为增加 zero operation 仅会影响所得结点的表示规模，在 Batch Norm 的影响下不会影响最终的分类结果。</li></ol><h2 id="Experiment-Details"><a href="#Experiment-Details" class="headerlink" title="Experiment Details"></a>Experiment Details</h2><ol><li>Create可以使用的 operations 包括：</li></ol><ul><li>$3 \times 3$ 和 $5 \times 5$ 的可分离卷积</li><li>$3 \times 3$ 和 $5 \times 5$ 的 dilated 可分离卷积</li><li>$3 \times 3$ 的最大池化和平均池化</li><li>identity</li><li>zero</li></ul><ol start="2"><li>所有操作的 stride = 1，并且对特征图进行填充以保证空间分辨率，对所有的卷积操作使用 ReLU-Conv-BN 的顺序，每个可分离卷积都总是使用 2 次。</li><li>每个 Cell 由 7 个结点组成，其中输出结点被定义为所有中间结点(不包括中间结点)在深度这个维度上的 concatenation。第 $k$ 个 Cell 的第一个结点和第二个结点分别为第 $k-2$ 和第 $k-1$ 个 Cell 的输出</li><li>位于网络深度 $1/3$ 和 $2/3$ 处的 Cell 为 reduction Cell(stride =2 )，所以网络结构可以被编码为权重向量 $\alpha_{normal}$ 和 $\alpha_{reduction}$，分别被 norm Cells 和 reduction Cells 贡献。</li></ol><h2 id="Results-5"><a href="#Results-5" class="headerlink" title="Results"></a>Results</h2><p>最先给出的是 DARTS 在CIFAR-10 和 Penn Treebank 上的搜索过程<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/darts_search_progress.png"></p><p>看下 DARTS 搜出来的 norm Cell 和 reduction Cell<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/ndarts_norm_and_reduction_cell.png"></p><p>然后是在 CIFAR-10 上的结果<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/darts_results_cifar10.png"></p><p>然后把网络结构移到 ImageNet 数据集上训练得到结果<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/darts_results_on_imagenet.png"></p>]]></content>
      
      
      <categories>
          
          <category> NAS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NAS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Anchor Free Object Detection</title>
      <link href="2020/06/20/Anchor-Free-Object-Detection/"/>
      <url>2020/06/20/Anchor-Free-Object-Detection/</url>
      
        <content type="html"><![CDATA[<h1 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h1><p>Pipeline 如下：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cornernet_pipeline.png"></p><h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ol><li>通过检测目标的左上角点和右下角点来检测出目标, 做到了 anchor free</li><li>提出了 corner pooling 来更好地定位角点</li></ol><h2 id="Anchor-Based-Methods-存在的问题"><a href="#Anchor-Based-Methods-存在的问题" class="headerlink" title="Anchor Based Methods 存在的问题"></a>Anchor Based Methods 存在的问题</h2><ol><li>Anchor-Based 方法都会产生大量的 anchors/proposals, 例如 DSSD 会产生超过 40k 个 anchors, RetinaNet 会产生 100k 个 anchors, 但是只有很少一部分的 anchors 是和 Ground Truth 有比较大的 overlap 的, 这就导致了 positive anchors 和 negative anchors 之间的数据不平衡从而影响训练速度和精度。</li><li>Anchor 机制会使得模型有很多的超参数和设计选择, 例如每个 grid 生成多少个 boxes, 每个 box 的 size 以及 aspect ratio 等等, 这种选择很大程度上是通过临时启发式方法做出的, 当与多尺度体系结构结合使用时, 可能会变得更加复杂。</li></ol><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>如下图所示, 在经过 backbone 后接 2 个预测模块, 每个模块都有自己的 corner pooling 层, 经过 corner pooling 后分别得到 heatmap, embedding vector 和 offsets, 如图所示：</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cornernet.png"></p><p>其中 Heatmap 的 size 为 $H \times W \times C$, $C$ 为类别数目, Embedding Vector 的 size 为 $H \times W \times l$, 这里的 $l$ 代表 Embedding Vector 的长度, 原文中 $l=1$, Offsets 的 size 为 $H \times W \times 2$, 预测的是 x 方向和 y 方向上的偏移量。Heatmap 用于检测角点, Embedding Vector 用于对角点进行配对分组, offsets 用于调整 2 个角点的坐标。所以可以看出这里有 3 个 Loss 值, 分别是 $L_{detection}, L_{group}, L_{offset}$</p><h2 id="角点检测"><a href="#角点检测" class="headerlink" title="角点检测"></a>角点检测</h2><p>这里预测 2 个热力图, 分别用于左上角点和右下角点, 每个热力图的通道数都是 $C$, 也就是类别数, 每个 channel 都是一个二值的 mask, 用于表示该类的角点位置。对每个角点而言都只有一个 Ground Truth, 其他的点都是 negative, 训练时并非简单地对其他 negative corners 都做一样的惩罚, 而是对离 Ground Truth 更近的点施以更小的惩罚, 在半径为 $r$ 的圆内, 惩罚的减少量满足非标准的 2-D 的高斯分布: $e^{-\frac{x^2 + y^2}{2\sigma^2}}$</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/2d_gaussian_distribution.jpg"></p><p>即 $y_{cij}$ 满足下式：<br>$$<br>y_{c i j}=\left{\begin{array}{ll}<br>e^{-\frac{x^{2}+y^{2}}{2 \sigma^{2}}}, &amp; x^{2}+y^{2} \leq r^{2} \<br>0, &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p><p>利用 2 个角点生成的 bounding box 必须和 Ground Truth 的 IoU 大于某个值来确定半径 $r$ 的值。2D高斯分布的中心就是真正的角点坐标, $\sigma = \frac{r}{3}$。</p><p>设 $p_{cij}$ 是在热力图的坐标 $(i, j)$ 位置上为类别 $c$ 预测的值, $y_{cij}$ 是非标准的二维高斯分布的值(上图所示), 也就是热力图上的 Ground Truth, 那么就能得到一个 Focal Loss 的变体, 为检测损失:</p><p>$$<br>L_{d e t}=-\frac{1}{N} \sum_{c=1}^{C} \sum_{i=1}^{H} \sum_{j=1}^{W}\left{\begin{array}{cc}<br>\left(1-p_{c i j}\right)^{\alpha} \log \left(p_{c i j}\right) &amp; \text { if } y_{c i j}=1 \<br>\left(1-y_{c i j}\right)^{\beta}\left(p_{c i j}\right)^{\alpha} \log \left(1-p_{c i j}\right) &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p><p>这里的 otherwise 并非 $y_{cij}=0$, 因为在半径内, $y_{cij}$ 的值满足二维的高斯分布。$\alpha$ 和 $\beta$ 是用于控制每个点权重的超参数(文中$\alpha=2, \beta=4$), 根据上面的 Focal Loss 变体可以看出：</p><ol><li>在角点位置 $(i, j)$ 时, 预测这个点为角点的概率 $p_{cij}$ 的值越大, 那么这里的 Detection Loss 就会越小。</li><li>不在角点位置时, 固定 $p_{cij}$, 离角点越近, $y_{cij}$ 越大, $1-y_{cij}$ 就越小, Detection Loss 越小。</li><li>不在角点位置时, 固定 $y_{cij}$, 如果预测这个点为角点的概率 $p_{cij}$ 越大, 那么Detection Loss 越大。</li></ol><p>所以这个公式是希望越接近角点的那些点被预测为角点的概率 $p_{cij}$ 越大, 越远离角点的那些点被预测为角点的概率越小。</p><p>把位置从热力图上重新映射到原图上时, 由于下采样因子的存在会存在像素偏移, 图像中的位置 $(x, y)$ 映射到热力图上是 $(\lfloor \frac{x}{n} \rfloor, \lfloor \frac{y}{n} \rfloor)$, $n$ 是下采样因子, 为了解决像素偏移的问题这里还预测了一个 offset：</p><p>$$<br>\boldsymbol{o}<em>{k}=\left(\frac{x</em>{k}}{n}-\left\lfloor\frac{x_{k}}{n}\right\rfloor, \frac{y_{k}}{n}-\left\lfloor\frac{y_{k}}{n}\right\rfloor\right)<br>$$</p><p>在测试阶段, 则根据预测的坐标偏差来修改坐标, 设 heatmap 上预测为角点的位置为 $(x,y)$, offset 为 $\hat{\sigma}=(\Delta x, \Delta y)$ 那么在原图上角点的坐标为 $x_0 = \lfloor (x+\Delta x)n \rfloor, y_0 = \lfloor (y+\Delta y)n \rfloor$, 对应得到一个 Smooth L1 的偏移损失：</p><p>$$<br>L_{o f f}=\frac{1}{N} \sum_{k=1}^{N} \text { SmoothL1Loss }\left(\boldsymbol{o}<em>{k}, \hat{\boldsymbol{o}}</em>{k}\right) \<br>\text { SmoothL1Loss }(x)=\left{\begin{array}{ll}<br>0.5 x^{2} &amp; \text { if }|x|&lt;1 \<br>|x|-0.5 &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p><h2 id="角点分组"><a href="#角点分组" class="headerlink" title="角点分组"></a>角点分组</h2><p>如果一张图像中有多个 objects, 那么肯定有多个左上角点和右下角点, 需要将左上角点和右下角点进行配对来组成一个 bounding box, 作者这里使用 embedding 的方法, 为每个 corner 生成 embedding vector, 如果 2 个 corners 是来自同一个目标, 那么对应的 2 个 embedding vector 应该很小, 在训练的时候, 会分别生成 2 个 embedding vector, 其输出 size 为 $h \times w$, 和 heatmap 的 size 相同, 这里作者简单的把通道数设置为 1, 即 embedding vector 的 size 为 $H \times W \times 1$, 这个 embedding 没有 target value, 引入 pull loss 和 push loss 来学习 embedding 的预测, 对于第 k 个目标的左上角点的 embedding $e_{tk}$, 和第 k 个目标的右下角点的 embedding $e_{bk}$, $L_{pull}$ 和 $L_{push}$ 分别为</p><p>$$<br>\begin{array}{c}<br>L_{p u l l}=\frac{1}{N} \sum_{k=1}^{N}\left[\left(e_{t k}-e_{k}\right)^{2}+\left(e_{b k}-e_{k}\right)^{2}\right] \<br>L_{p u s h}=\frac{1}{N(N-1)} \sum_{k=1}^{N} \sum_{j=1}^{N} \max \left(0, \Delta-\left|e_{k}-e_{j}\right|\right) \<br>e_{k}=\frac{e_{t k}+e_{b k}}{2}<br>\end{array}<br>$$</p><p>其中 $N$ 是一张图像中的 object 数目, $\Delta=1$, $L_{pull}$ 是为了将同一目标的 2 个 embedding 拉的更近, 同一目标的 2 个 embedding $e_{tk}$ 和 $e_{bk}$ 越接近那么 $L_{pull}$ 就越小。</p><p>$L_{push}$ 的目的是让不同目标的 average embedding 尽可能的远, 远离的程度由 $\Delta$ 控制, 当 2 个不同目标的 average embedding 的 L1 范数超过 $\Delta$ 时就可以认为此时 corner 是来自不同的目标, 这个时候 $L_{push} = 0$, 当 2 个不同目标的 average embedding 的 L1 范数没有超过 $\Delta$ 时认为没有真正区分开这 2 个目标的 embedding, 所以要添加惩罚。</p><p>embedding 本身没有 target value（事实上也很难去给出一个绝对的值）, embedding 是人为设计出来的一套自洽系统或者说一个空间, 只要 corner 的 embedding 遵循一定的规则, 就可以, 所以使用 pull 和 push 损失学习, 可以看成是为了让生成的 embedding 遵守规则, 这样在 Test 阶段, 就可以基于这些规则然后根据 embedding 值对 corner 进行同目标归并。</p><p>测试阶段别从两组 heatmap 上得到 K 个 top-left corners 和 K 个 bottom-right corners, 那么共有 $k^2$ pairs（一个 top-left 与一个 bottom-right corner 组成一个 pair）, 对每个 pair 计算 $e_{tk} - e_{bj}$, 如果超过一个阈值(0.5)那么这个 pair 就会被忽略, 实际上在源码中还有其他一些筛选条件, 比如得分（来自于 heatmap）, 坐标值范围检测, pair 中 top-left 与 bottom-right 两个 corner 的分类是否一致等。筛选之后还要（对每个分类分别）使用 soft nms 等方法, 才得到最终的检测结果。具体请阅读<a href="https://github.com/princeton-vl/CornerNet">源码</a>。</p><h2 id="Corner-Pooling"><a href="#Corner-Pooling" class="headerlink" title="Corner Pooling"></a>Corner Pooling</h2><p>以 Top-Left 为例给出 Corner Pooling 的操作过程：</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/corner_pooling_1.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/corner_pooling_2.png"></p><p>如果想判断一个在 $(x, y)$ 的像素点是否为左上角点, $f_t$ 和 $f_l$ 是 top-left corner pooling layer 的输入, $f_{t_{ij}}, f_{l_{ij}}$ 代表在 $f_t$ 和 $f_l$ 上 $(x, y)$ 位置的向量, $f_t$ 和 $f_l$ 的 size 都是 $H \times W$, 首先最大池化 $f_t$ 中在 $(i, j)$ 和 $(i, H)$ 中的所有特征值, 使之成为特征向量 $t_{ij}$, 然后最大池化 $f_l$ 中在 $(i, j)$ 和 $(W, j)$ 中的所有特征值, 使之成为特征向量 $l_{ij}$, 最后把 $t_{ij}$ 和  $l_{ij}$ 进行 element-wise add 操作。公式如下：</p><p>$$<br>\begin{array}{l}<br>t_{i j}=\left{\begin{array}{cl}<br>\max \left(f_{t_{i j}}, t_{(i+1) j}\right) &amp; \text { if } i&lt;H \<br>f_{t_{H j}} &amp; \text { otherwise }<br>\end{array}\right. \<br>l_{i j}=\left{\begin{array}{cl}<br>\max \left(f_{l_{i j}}, l_{i(j+1)}\right) &amp; \text { if } j&lt;W \<br>f_{l_{i W}} &amp; \text { otherwise }<br>\end{array}\right.<br>\end{array}<br>$$</p><p>右下角点的 corner pooling 也是相同的思路。</p><h2 id="Prediction-Module"><a href="#Prediction-Module" class="headerlink" title="Prediction Module"></a>Prediction Module</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/prediction_module.png"></p><p>预测模块的第一部分是残差模块的修改版, 修改后的残差模块中, 将第一个 $3 \times 3$ 卷积替换为一个 Corner Pooling 模块。这个残差模块首先通过具有个 128 通道的 2 个 $3 \times 3$ 卷积模块的主干网络进行特征提取, 然后应用一个 Cornet Pooling 层。残差模块之后, 我们将池化特征输入到具有个 256 通道的 $3 \times 3$ 的 Conv+BN 层, 同时为这一层加上瓶颈结构。修改后的残差模块后面接一个具有个 256 通道的 $3 \times 3$ 卷积模块和 256个通道的 3 个 Conv-ReLU-Conv 来产生热力图, 嵌入和偏移量。</p><h2 id="Hourglass-Network"><a href="#Hourglass-Network" class="headerlink" title="Hourglass Network"></a>Hourglass Network</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/hourglass_network.jpg"></p><p>这个网络最早在在人体姿态估计任务中被引入, Hourglass 网络是全卷积网络, 包含一个或者多个 Hourglass 模块, 在 Hourglass 网络中首先使用一些卷积核池化层对输入特征进行下采样（注意论文指出这里没有使用池化层来做下采样, 而是只使用步长为卷积层）, 然后再上采样使得特征图的分辨率回到原始大小, 由于 Max-Pooling 层会丢失细节信息, 所以增加跳跃连接层将低级特征信息带到上采样特征图中, 因此 hourglass 不仅仅结合了局部特征还结合了全局特征, 当堆叠多个 hourglass 模块时就可以重复这个过程, 从而得到更加高级的特征。</p><h2 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h2><p>设置了网络的输入分辨率 511 × 511, 使用数据增强, 包括随机水平翻转、随机缩放、随机裁剪和随机色彩抖动, 其中包括调整图像的亮度, 饱和度和对比度使用 PyTorch 默认的初始化, 由于使用了 Focal Loss, 所以按照 RetinaNet 论文中指出的方式来对某些卷积层的偏置进行初始化. 总损失就是检测损失, 分组损失和偏移损失的加权和:<br>$$<br>L = L_{det} + \alpha L_{pull} + \beta L_{push} + \gamma L_{off}<br>$$</p><h2 id="Test-Details"><a href="#Test-Details" class="headerlink" title="Test Details"></a>Test Details</h2><ol><li>如何判断某个位置是角点？先在得到的 corner heatmaps 上利用 $3 \times 3$ 的最大池化层做 NMS, 不改变特征图大小(stride=1, padding=1), 在最大池化时值改变了的全部置 0, 值未变的保留原值。</li><li>经过 NMS 后最多从 heatmap 选择 Top 100 个左上角点和 Top 100 个右下角点。</li><li>对每个角点的坐标利用预测的 offsets 进行调整, 计算左上角点和右下角点 embedding 之间的 L1 距离, 对于这个距离大于某个阈值(0.5)或者不是同一类的角点对, 取消配对。</li><li>左上角点和右下角点的平均 score 作为最终检测的 score.</li><li>测试时保持原图像的 size 不变, 如果原来的图像长和宽不相等, 使用 zero-padding 的方式来填充.</li></ol><h1 id="ExtremeNet"><a href="#ExtremeNet" class="headerlink" title="ExtremeNet"></a>ExtremeNet</h1><h2 id="贡献-1"><a href="#贡献-1" class="headerlink" title="贡献"></a>贡献</h2><p>CornerNet 的问题是角点检测不准确，ExtremeNet 通过预测上下左右 4 个极值点以及一个中心点来检测目标。</p><h2 id="和-CornerNet-的不同"><a href="#和-CornerNet-的不同" class="headerlink" title="和 CornerNet 的不同"></a>和 CornerNet 的不同</h2><p>ExtremeNet 和 CornerNet 主要有 2 处不同，第一点是 CornerNet 的角点通常是在目标的外部，往往没有强烈的局部特征。而 ExtremeNet 的极值点就在物体的边缘上，视觉上很好辨认。第二点是 ExtremeNet 纯粹依赖于几何关系进行极值点分组，没有隐含的特征学习，效果更好。</p><h2 id="极值点和中心点"><a href="#极值点和中心点" class="headerlink" title="极值点和中心点"></a>极值点和中心点</h2><p>网络的 pipeline 如下图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/extremenet.png"></p><p>输出特征图的通道数为 $5 \times C + 4 \times 2$，$C$ 代表类别数，对于每个类别预测一张 heatmap 和一张 center map，所以是 $5 \times C$，对 4 个极值点预测 $x, y$ 方向上的 offset，不对中心点预测 offset，极值点的定义为一个 $3 \times 3$ 滑动窗口中的最大值且大于设定的阈值 $t_p$，论文中 $t_p = 0.1$</p><h2 id="中心点分组"><a href="#中心点分组" class="headerlink" title="中心点分组"></a>中心点分组</h2><p>对于给定的 4 个极值点 $t, b, r, l$，计算其几何中心 $c = (\frac{l_x + r_x}{2}, \frac{t_y + b_y}{2})$，如果这个几何中心的值在 center map 中是高响应的(大于阈值$t_c$)，那么就把这 5 个点看做是一个有效的检测，论文中 $t_c = 0.1$，该算法的流程图和伪代码如下所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/extremenet_points_match.png"><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/extremenet_center_grouping.png"></p><h2 id="Ghost-Box-Suppression"><a href="#Ghost-Box-Suppression" class="headerlink" title="Ghost Box Suppression"></a>Ghost Box Suppression</h2><p>中心点分组可能会得到一些高置信度但是是假阳性的检测结果，这种情况往往在 3 个及以上相同尺寸的物体共线性的时候出现，这就会导致最终预测一个超级大的 box，这个 bbox 会把 3 个 ground truth box 都包含进去，如下图所示，3 个实线框是3 个 ground truth box，虚线框是 ghost box。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/extremenet_bbox_suppression.png"></p><p>作者使用 soft non-maxima suppression 方法，根据 Algorithm 1，得到所有的 bbox 以及对应的得分。假设某个 bbox，其 score 为 $s_0$ ，所有被这个 bbox 包含的其他 bbox 的得分之和超出 $s_0$ 的 3 倍，那么这个 bbox 的 score 降为一半为 $\frac{s_0}{2}$ （视作对这样的 bbox 的 score 的惩罚，后期再对 bbox 得分低于 0.5 的进行过滤，就能把 ghost box 过滤掉了）。为什么是 3 倍？因为如上图所示，至少有 3 个  bbox，才会有选择中间 bbox 还是选择紧紧包围三个 bbox 的大 bbox 这样两难的问题。实际上，必须有奇数个 bbox，才会有可能出现 ghost box 的可能，而一个 image 中有大于 3 的相等大小的 bbox 水平或垂直排列，这种可能性极低，所以只要考虑 3 倍足以（纯粹个人理解）。</p><h2 id="Edge-aggregation"><a href="#Edge-aggregation" class="headerlink" title="Edge aggregation"></a>Edge aggregation</h2><p>有时候，极值点并不是唯一的，举例来说，一辆汽车，可能顶部这一条类似于水平的线上，每一个位置都有可能成为最顶部的极值点。也就是说，若极点来自物体的垂直或水平边，则沿边缘任何一点都可被认为是极点。因此，我们的网络沿着对象的任何对齐边缘产生弱响应，而不是单一的强峰值响应。这种弱响应有两个不足：</p><ol><li>弱响应点的得分可能会小于阈值，那么极值点就会被错过</li><li>即使检测到了这些极值点，它的得分将低于具有强峰值响应的稍微旋转的对象。<br>为了解决这个问题，提出了 edge aggregation。对每一个取局部最大值得到的极值点，若是左边和右边的极值点，那么选择竖直方向进行聚合；同理，若是顶部，底部的极值点，则选择从水平方向进行分数聚合。聚合的方法为：沿着聚合方向，聚合那些单调递减的 score，并在达到局部最小值的时候停止聚合。假定 m 为极值点，定义一个包含m点的水平线在 heatmaps 上的分数集：<br>$$<br>N_{i}^{m} = \hat{Y}_{m_x + i}<br>$$<br>由于 m 是极值点，极值点的选择机制是局部最大原则，所以需要在极值点m的左边和右边找到两个局部最小值，记为$i_0$ 和 $i_1$，应该满足：</li></ol><p>$$<br>\begin{array}{l}<br>N_{i_{0}-1}^{(m)}&gt;N_{i_{0}}^{(m)} \<br>N_{i_{1}}^{(m)}&lt;N_{i_{1}+1}^{(m)}<br>\end{array}<br>$$</p><p>此时原始极值点 m 的更新为：</p><p>$$<br>\tilde{Y}<em>{m}=\hat{Y}</em>{m}+\lambda_{a g g r} \sum_{i=i_{0}}^{i_{1}} N_{i}^{(m)}<br>$$</p><p>其中 $\lambda_{aggr}$ 是聚合权重，论文中设置为 0.1，下图为经过 edge aggregation 的效果展示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/edge_aggression.png"></p><h2 id="Extreme-Instance-Segmentation"><a href="#Extreme-Instance-Segmentation" class="headerlink" title="Extreme Instance Segmentation"></a>Extreme Instance Segmentation</h2><p>比起 bounding box，极点携带了更多的目标信息，bbox 的标准只要 4 个数据，左上右下两个 corner 的 4 个坐标值，而极点标注则是 4 个极点共 8 个坐标值。基于几点创建一个八边形来近似出一个目标的 mask，这个八边形的边以极点为中心，具体做法是：将四个极点分别沿着 bbox 四个边的方向延伸，得到 4 个线段，长度分别是 bbox 四个边的 1/4，如果延伸时遇到 bbox 的 corner，则截断（不再延伸），将得到的四个线段的端点按顺序连接起来，就得到八边形。同时将 ExtremeNet 获得的极值点传到 Deep Extreme Cut 网络可以获得一个类别未知的分割 Mask，注意类别其实在 ExtremeNet 中已经知道了，这就相当于一个双阶段的实例分割。</p><h1 id="CenterNet-Triplets"><a href="#CenterNet-Triplets" class="headerlink" title="CenterNet: Triplets"></a>CenterNet: Triplets</h1><h2 id="贡献-2"><a href="#贡献-2" class="headerlink" title="贡献"></a>贡献</h2><ol><li>相比于 CornerNet 只预测 2 个角点，这里还多预测了一个中心点，由于角点大都在物体之外，这使得 Corner Pooling 更加关心目标的边缘信息而对内部的感知不强，添加中心点能够得到更多的视觉信息。</li><li>提出 Center Pooling 来更好地定位中心点。</li><li>提出 Cascade Corner Pooling 给角点提供更加丰富的物体内部语义信息。</li></ol><h2 id="CornerNet-的不足"><a href="#CornerNet-的不足" class="headerlink" title="CornerNet 的不足"></a>CornerNet 的不足</h2><p>作者首先分析了 CornerNet 的结果中存在的一个严重问题是错误的 bounding box 数量很多，即使在 IoU 阈值很低的情况下也是这样，如下图所示，其中 $FD$ 代表错误的 bounding box 的百分比<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/false_discovery_in_cornernet.png"></p><p>作者认为出现这种情况的<strong>一个可能的原因</strong>是 CornerNet 仅预测角点，无法感知物体内部的信息，虽然可以添加 RoI Pooling 把 CornerNet 变成一个 Two-Stage 的方法来得到 bounding box 内部的信息，但这样计算量太大，所以才多预测了一个 Center Keypoint.</p><h2 id="Object-Detection-as-keypoint-triplets"><a href="#Object-Detection-as-keypoint-triplets" class="headerlink" title="Object Detection as keypoint triplets"></a>Object Detection as keypoint triplets</h2><p>在 CornerNet 的基础上多预测了一个中心点的 heatmap 和 offsets，然后和 CornerNet 一样利用角点生成 top-k 个 bounding box，接着按照以下步骤来检测 center keypoint:</p><ol><li>根据 heatmap 上的 score 选择 top-k 个 center keypoint</li><li>使用预测的 offsets 来重新映射 center map 在 input image 的位置</li><li>为每个 bounding box 定义一个 central region，并判断这个 central region 内是否含有 center keypoint(这里 center keypoint 的 class label 要和 bounding box 的 class label 一致)</li><li>如果一个 center keypoint 在 central region 内被检测到, 那么这个 bounding box 就是有效的, 且其 score 变为 3 个点的 scores 均值, 否则这个 bounding box 无效, 会被移除.</li></ol><p>可以看到 central region 的 size 会影响最终结果, 如果定义的 central region 太小会导致很多小尺度的错误目标框无法被去除，而 central region 会导致很多大尺度的错误目标框无法去除, 所以需要根据 bounding box 的大小来动态地调整 central region 的 size, 假设左上角点 $i$ 的坐标为 $(tl_x, tl_y)$, 右下角点 $j$ 的坐标为 $(br_x, br_y)$, 那么 central region 的坐标满足以下公式:<br>$$\left{\begin{array}{l}<br>\operatorname{ctl}<em>{\mathrm{x}}=\frac{(n+1) \mathrm{t} \mathrm{l}</em>{\mathrm{x}}+(n-1) \mathrm{br}<em>{\mathrm{x}}}{2 n} \<br>\mathrm{ctl}</em>{\mathrm{y}}=\frac{(n+1) \mathrm{t} \mathrm{l}<em>{\mathrm{y}}+(n-1) \mathrm{br}</em>{\mathrm{y}}}{2 n} \<br>\operatorname{cbr}<em>{\mathrm{x}}=\frac{(n-1) \mathrm{t} 1</em>{\mathrm{x}}+(n+1) \mathrm{br}<em>{\mathrm{x}}}{2 n} \<br>\operatorname{cor}</em>{\mathrm{y}}=\frac{(n-1) \mathrm{t} 1_{\mathrm{y}}+(n+1) \mathrm{br}_{\mathrm{y}}}{2 n}<br>\end{array}\right.$$</p><p>$n$ 是个超参数, 论文中将 bounding box size &lt; 150 的设置为 3, bounding box size &gt; 150 的设置为 5.</p><h2 id="Center-Pooling"><a href="#Center-Pooling" class="headerlink" title="Center Pooling"></a>Center Pooling</h2><p>下图中的Figure 4(a) 展示了 Center Pooling 的原理, Center Pooling 提取中心点水平方向和垂直方向的最大值并相加，给中心点提供除了所处位置以外的信息，这使得中心点有机会获得更易于区分于其他类别的语义信息。Center Pooling 可通过不同方向上的 Corner Pooling 的组合实现，例如一个水平方向上的取最大值操作可由 Left Pooling 和 Right Pooling 通过串联实现。同理，一个垂直方向上的取最大值操作可由 Top Pooling 和 Bottom Pooling 通过串联实现，具体操作如 Figure5（a）所示，特征图两个分支分别经过一个 $Conv_{3 \times 3}-BN-ReLU$，然后做水平方向和垂直方向的 Corner Pooling，最后再相加得到结果。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/corner_center_cascade_corner_pooling.png"><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/center_pooling_and_cascade_corner_pooling.png"></p><h2 id="Cascade-Corner-Pooling"><a href="#Cascade-Corner-Pooling" class="headerlink" title="Cascade Corner Pooling"></a>Cascade Corner Pooling</h2><p>这里是为了更好地预测目标的左上和右下角点, Figure 4（b）是CornerNet中的做法即提取物体边界最大值进行相加，该方法只能提供关联物体边缘语义信息，对于更加丰富的物体内部语义信息则很难提取到。为了能够提取物体内部的语义信息, 作者提出了 Cascade Corner Pooling. 原理如 Figure 5（b）所示，它首先提取出目标边缘最大值，然后在边缘最大值处继续向物体内部(如Figure 4(c)所示)提取最大值，并和边界最大值相加，以此给角点提供更丰富的关联目标语义信息。Figure 5(b)展示了 Cascade Top Corner Pooling 的原理。这里需要注意一下 Cascade Corner Pooling 只是为了通过内部信息丰富角点特征，也就是级联不同方向的 Corner Pooling 实现内部信息的叠加，最终的目的还是要预测角点，所以最终左上角点通过 Cascade Top Corner Pooling + Cascade Left Corner Pooling 实现，右下角点通过 Cascade Right Corner Pooling + Cascade Bottom Corner Pooling 实现。</p><h2 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h2><p>这里的损失函数和 CornerNet 是差不多的, 只是多了个中心点的损失而已, co 代表角点, ce 代表中心点:<br>$$<br>\mathrm{L}=\mathrm{L}<em>{\text {det }}^{\text {co }}+\mathrm{L}</em>{\text {det }}^{\text {ce }}+\alpha \mathrm{L}<em>{\text {pull }}^{\text {co }}+\beta \mathrm{L}</em>{\text {push }}^{\text {co }}+\gamma\left(\mathrm{L}<em>{\text {off }}^{\text {co }}+\mathrm{L}</em>{\text {off }}^{\text {ce }}\right)<br>$$</p><p>其定义和 CornerNet 中完全一致.</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/centernet_triplets_result.png"></p><p>可以看到 CenterNet 获得了 47% 的 mAP，超过了所有的 One-Stage 算法，领先幅度越 5%，并且精度和 Two-Stage 的目标检测算法的最好结果也是接近的。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/centernet_triplets_result_compare_to_cornernet.png"></p><p>上面的 Table 3 是 CenterNet 与 CornerNet 的单独对比，可以看出在 MS COCO 数据集上 CenterNet 消除大量误检框，尤其是在小物体上。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/centernet_triplets_ablation_study.png"></p><p>Table 4 是消融实验。分别说明了 CRE（中心点加入），CTP（中心点池化），CCP（级联角点池化）的有效性。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/centernet_triplets_error_analysis.png"><br>作者将检测的中心点用真实的中心点代替，实验结果表明中心点的检测准确度还有很大的提升空间。同时该结果还表明要<strong>想更进一步的提升检测精度，需要进一步提升中心点的检测精度</strong>。</p><h1 id="CenterNet-Points"><a href="#CenterNet-Points" class="headerlink" title="CenterNet: Points"></a>CenterNet: Points</h1><p>目前见过最优雅的方法, 速度快精度高而且拓展性非常强! 微信和支付宝扫一扫目前都是用的这个方法.可视化效果如下图所示:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cornernet_sample.png"></p><h2 id="贡献-3"><a href="#贡献-3" class="headerlink" title="贡献"></a>贡献</h2><ol><li>用 heatmap 预测的目标中心点代替 Anchor 机制来预测目标，使用更大分辨率的输出特征图（相对于原图缩放了 4 倍），因此无需用到多层特征，实现了真正的 Anchor-Free。</li><li>网络可拓展性非常强，论文中介绍了实现 3D 目标检测和人体姿态估计任务。具体来说对于 3D 目标检测，直接回归得到目标的深度信息，3D目标框的尺寸，目标朝向；对于人体姿态估计来说，将关键点位置作为中心的偏移量，直接在中心点回归出这些偏移量的值。例如对于姿态估计任务需要回归的信息如 Figure 4 所示。</li><li>模型设计简单，因此在运行速度和精度的平衡上取得了很好的结果。</li></ol><p>CenterNet 和 Anchor-Based 检测器的区别下图所示:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/diff_between_anchor_and_free.png"></p><h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p>整体思路很简洁, 在 backbone 后面接上 3 个卷积层来分别作为 heatmap, coordinate offsets 和 size(不是 offset) 就行. heatmap 用于预测中心点的位置, coordinate offset 预测中心点的偏移量, size offset 预测宽和高的偏移量, 直接看<a href="">源码</a>来理解:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(hm): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">80</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br><span class="line">(wh): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">2</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br><span class="line">(reg): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">2</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="前置内容"><a href="#前置内容" class="headerlink" title="前置内容"></a>前置内容</h2><p>对于 $I \in R^{H \times W \times 3}$ 的输入图像, 需要生成关键点的热力图 $\hat{Y} \in [0, 1]^{\frac{W}{R} \times \frac{H}{R}} \times C$, $R$ 是缩放步长, $C$ 是类别数, 原文中设置为 4. $\hat{Y}<em>{x,y,c}=1$ 表示检测到的关键点, $\hat{Y}</em>{x,y,c}=0$ 表示为背景, 对于原图上的坐标 $p$, 经过下采样之后的坐标为 $\tilde{p} = \lfloor \frac{p}{R} \rfloor$, GT 也是通过二维的高斯核来得到:</p><p>$$<br>Y_{x y c}=\exp \left(-\frac{\left(x-\tilde{p}<em>{x}\right)^{2}+\left(y-\tilde{p}</em>{y}\right)^{2}}{2 \sigma_{p}^{2}}\right)<br>$$</p><p>其中 $\sigma_{p}$ 是目标尺度-自适应的标准方差, 和 CornerNet 中一致, 如果两个高斯分布发生了重叠，直接取元素间最大的就可以, 损失函数也和 CornerNet 一样是 Focal Loss 的变体:</p><p>$$<br>L_{k}=\frac{-1}{N} \sum_{x y c}\left{\begin{array}{cl}<br>\left(1-\hat{Y}<em>{x y c}\right)^{\alpha} \log \left(\hat{Y}</em>{x y c}\right) &amp; \text { if } Y_{x y c}=1 \<br>\left(1-Y_{x y c}\right)^{\beta}\left(\hat{Y}<em>{x y c}\right)^{\alpha}<br>\log \left(1-\hat{Y}</em>{x y c}\right) &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p><p>$\alpha=2, \beta=4$ 是超参数, $N$ 是关键点的个数, 用于将 Focal Loss 归一化, 由于下采样会对关键点的坐标产生偏差, 所以这里使用 L1 Loss 对每个中心点都预测了偏移量:</p><p>$$<br>L_{o f f}=\frac{1}{N} \sum_{p}\left|\hat{O}_{\tilde{p}}-\left(\frac{p}{R}-\tilde{p}\right)\right|<br>$$</p><h2 id="检测原理"><a href="#检测原理" class="headerlink" title="检测原理"></a>检测原理</h2><p>上面已经能够得到中心点的坐标和偏移了, 所以为了得到最终的 bounding box 还需要预测 w 和 h, 对于目标 k 的 bounding box, 其坐标为 $(x_1^{(k)}, y_1^{(k)}, x_2^{(k)}, y_2^{(k)})$, 那么中心点的坐标为 $p_{k}=\left(\frac{x_{1}^{(k)}+x_{2}^{(k)}}{2}, \frac{y_{1}^{(k)}+y_{2}^{(k)}}{2}\right)$. 每个 bounding box 的真实尺寸为$s_{k}=\left(x_{2}^{(k)}-x_{1}^{(k)}, y_{2}^{(k)}-y_{1}^{(k)}\right)$, 从上面的代码可以看出这里生成一个 $\hat{S} \in \mathcal{R}^{\frac{W}{R} \times \frac{H}{R} \times 2}$ 特征图用于预测 $w$ 和 $h$, 那么尺寸误差就定义为中心点预测的尺度和真实尺度的 L1 Loss:<br>$$<br>L_{s i z e}=\frac{1}{N} \sum_{k=1}^{N}\left|\hat{S}<em>{p</em>{k}}-s_{k}\right|<br>$$</p><p>最终的总损失函数为:<br>$$<br>L_{d e t}=L_{k}+\lambda_{s i z e} L_{s i z e}+\lambda_{o f f} L_{o f f}<br>$$</p><p>原理就是在 backbone 后预测一个 $W \times H \times (C + 2 + 2)$ 的特征图, $C$ 用于预测关键点 $\hat{Y}$, 2 个 2 分别用于预测中心点的偏移量 $\hat{O}$ 以及 尺寸大小 $\hat{S}$</p><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>推理的时候需要得到 heatmap 上每个类别的峰值点, 做法是将热力图上的所有响应点与其连接的 8 个临近点进行比较，如果该点响应值大于或等于其 8 个临近点值则保留(可以使用 $3 \times 3$ 的 maxpool 来实现)，最后我们保留所有满足之前要求的 top-100 个峰值点。对于检测到的中心点 $\hat{P}$, 预测的坐标为 $(\hat{x}<em>i, \hat{y}<em>i)$, 预测的偏移量为 $\left(\delta \hat{x}</em>{i}, \delta \hat{y}</em>{i}\right)=\hat{O}<em>{\hat{x}</em>{i} \hat{y}<em>{i}}$, 预测的 size 为 $\left(\hat{w}</em>{i}, \hat{h}<em>{i}\right)=\hat{S}</em>{\hat{x}<em>{i}, \hat{y}</em>{i}}$, 最终得到的 bounding box 的坐标为:</p><p>$$\begin{array}{l}<br>\left(\hat{x}<em>{i}+\delta \hat{x}</em>{i}-\hat{w}<em>{i} / 2, \hat{y}</em>{i}+\delta \hat{y}<em>{i}-\hat{h}</em>{i} / 2\right. \<br>\left.\hat{x}<em>{i}+\delta \hat{x}</em>{i}+\hat{w}<em>{i} / 2, \hat{y}</em>{i}+\delta \hat{y}<em>{i}+\hat{h}</em>{i} / 2\right)<br>\end{array}$$</p><h2 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h2><p>可以看到 CenterNet 的精度吊打了 YOLOv3，并且完全的 Anchor-Free 使得我们看到了目标检测更好的思路!<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cornernet_results.png"></p><p>作者也做了消融性验证:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/centernet_ablation_study.png"></p><h1 id="FCOS"><a href="#FCOS" class="headerlink" title="FCOS"></a>FCOS</h1><p>这是 CVPR 2019 的一篇文章，也是 Anchor-Free 的文章，2019 真的算是 Anchor-Free 的当打之年啊。</p><h2 id="贡献-4"><a href="#贡献-4" class="headerlink" title="贡献"></a>贡献</h2><ol><li>FCOS 框架和许多基于 FCN 的框架统一, 这样就能更好地用于其他任务</li><li>FCOS 是 anchor-free 的, 这样减少了超参的个数, 解放了调参侠</li><li>通过消除锚框，新探测器完全避免了复杂的 IoU 计算以及训练期间锚框和真实边框之间的匹配，并将总训练内存占用空间减少了 2 倍左右。</li><li>FCOS 可以作为 Two-Stage 中的 RPN 来生成 proposals, 而且效果还比 RPN 好!</li></ol><h2 id="Anchor-Based-的不足"><a href="#Anchor-Based-的不足" class="headerlink" title="Anchor-Based 的不足"></a>Anchor-Based 的不足</h2><p>还是批评了 Anchor-Based 方法的不足之处：</p><ol><li>检测结果的好坏对于一些超参数是很敏感的，比如 sizes, aspect ratios 以及 anchor boxes 的数目等</li><li>由于 anchor 的 size 是 fixed 的，所以检测器在处理那些形状变化比较大的 object (尤其是小物体)时会遇到困难，此外虽然 anchor 给了检测器一个先验，但是却影响了其泛化能力。</li><li>为了提高 Recall Rate，检测器必须生成很多的 anchor，而在训练过程中，一大部分 anchor 都是负样本(背景)，这又会导致正负样本不均衡的问题。</li><li>计算 proposals 和 Ground Truth 之间的 IoU 是一个计算量比较大的过程。</li></ol><h2 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos.png"></p><p>从上面的 Pipeline 可以看到 FCOS 的结构非常简洁, backbone 后接 FPN, 然后用共享的 head 来预测 3 个 branches, 分别用于 classification, center-ness 和 regression.</p><h2 id="全卷积检测器"><a href="#全卷积检测器" class="headerlink" title="全卷积检测器"></a>全卷积检测器</h2><p>在其他方法中, 是把原图中的 $(x, y)$ 映射为 $(\lfloor \frac{x}{n} \rfloor, \lfloor \frac{y}{n} \rfloor)$, 然后预测 offsets:<br>$$<br>\boldsymbol{o}<em>{k}=\left(\frac{x</em>{k}}{n}-\left\lfloor\frac{x_{k}}{n}\right\rfloor, \frac{y_{k}}{n}-\left\lfloor\frac{y_{k}}{n}\right\rfloor\right)<br>$$</p><p>在 FCOS 中, 对于特征图上的一点 $(x, y)$, 把它映射回原图中的位置是:<br>$$<br>(\lfloor \frac{s}{2} \rfloor + xs, \lfloor \frac{s}{2} \rfloor + ys)<br>$$</p><p>作者认为这样映射到原图上的位置能够接近特征图上的点 $(x, y)$ 感受野的中心从而得到更好的结果, 然后直接预测一个 4 维向量 $t^{<em>} = (l^{</em>}, t^{<em>}, r^{</em>}, b^{<em>})$, 代表这个点在原图上分别到 bounding box 的左, 上, 右, 下边界的距离, 除此以外还预测一个类别 $c^{</em>}$, $c^{*} = 0$ 表示为背景, 否则表示为其他类别.</p><p>虽然上述方法很简明, 但是会存在一个问题, 就是当特征图上的一个点被映射回原图上时, 位于原图上的点可能在多个 bounding box 里面, 这样的点在文章中被称为 <code>&quot;ambiguous samples&quot;</code>, 作者的解决方式是简单地选择更小面积的 bounding box 来作为回归的目标.</p><p>对于 Ground Truth 的 Bounding Box $B_i = (x_o^{(i)}, y_o^{(i)}, x_1^{(i)}, y_1^{(i)})$, 特征图上的点映射到原图中为 $(x, y)$, 那么训练过程中需要回归的目标就是:<br>$$\begin{aligned}<br>l^{<em>} &amp;=x-x_{0}^{(i)}, \quad t^{</em>}=y-y_{0}^{(i)} \<br>r^{<em>} &amp;=x_{1}^{(i)}-x, \quad b^{</em>}=y_{1}^{(i)}-y<br>\end{aligned}$$</p><p>可以看到<strong>以这种方式来进行检测, 可以尽可能多的得到前景的 samples 来进行检测器的训练, 作者认为这也是 FCOS 好的原因所在</strong></p><p>网络最后得到一个 80D 的特征图用于分类, 得到一个 4D 的特征图用于回归, 和 RetinaNet 一样训练 C binary 的分类器,, 由于回归的结果总会是正数, 所以使用 $exp(x)$ 来代替回归分支中的任一真实值, 这样能让结果更加平滑, 有点类似于 CornerNet 中的预测 offsets 而不是直接预测坐标, 这样能让网络更容易学习. 最终的损失函数可以表示为:<br>$$\begin{aligned}<br>L\left(\left{\boldsymbol{p}<em>{x, y}\right},\left{\boldsymbol{t}</em>{x, y}\right}\right) &amp;=\frac{1}{N_{\mathrm{pos}}} \sum_{x, y} L_{\mathrm{cls}}\left(\boldsymbol{p}<em>{x, y}, c</em>{x, y}^{<em>}\right) \<br>&amp;+\frac{\lambda}{N_{\mathrm{pos}}} \sum_{x, y} \mathbb{1}<em>{\left{c</em>{x, y}^{</em>}&gt;0\right}} L_{\mathrm{reg}}\left(\boldsymbol{t}<em>{x, y}, \boldsymbol{t}</em>{x, y}^{*}\right)<br>\end{aligned}$$</p><p>$L_{cls}$ 就是 RetinaNet 中提出的 Focal Loss, $L_{reg}$ 是 UnitBox 中提出的 IoU Loss, $N_{pos}$ 表示正样本的数量, $\lambda=1$ 用于平衡 $L_{reg}$ 的权重, 在特征图的所有位置上计算总和, $\mathbb{1}<em>{\left{c</em>{x, y}^{<em>}&gt;0\right}} = 1$ 如果 $c^{</em>}<em>{i} &gt; 0$, 否则 $\mathbb{1}</em>{\left{c_{x, y}^{*}&gt;0\right}} = 0$.</p><p>在 Inference 阶段直接前向传播网络, 在特征图的每个位置上都能得到一个分类概率 $p_{x, y}$ 和 一个回归向量 $t_{xy}$, 选择 $p_{x, y} &gt; 0.05$ 的位置作为正样本, 然后使用下式来得到 bounding box.<br>$$\begin{aligned}<br>l^{<em>} &amp;=x-x_{0}^{(i)}, \quad t^{</em>}=y-y_{0}^{(i)} \<br>r^{<em>} &amp;=x_{1}^{(i)}-x, \quad b^{</em>}=y_{1}^{(i)}-y<br>\end{aligned}$$</p><h2 id="FCOS-中的多尺度预测"><a href="#FCOS-中的多尺度预测" class="headerlink" title="FCOS 中的多尺度预测"></a>FCOS 中的多尺度预测</h2><p>这里使用了 FPN 的结构来进行多尺度预测, 之所以使用多尺度预测的好处主要有 2 个:</p><ol><li>当 stride 很大时(e.g. 16), BPR(best possible recall, 可以理解为一个检测器能够达到的 recall rate 的上界) 会很低, 在 anchor-based 方法中可以通过降低 positive sample 所需要的 IoU 来缓解这个问题, 但在 FCOS 中即使是很大的 stride , BPR 也很高, 使用多尺度预测只是为了进一步提高 BPR.</li><li>能减少上文中的 <code>&quot;ambiguous samples&quot;</code> 现象, 从而得到更好的结果.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_fpn.png"></li></ol><p>作者直接限制 bounding box 在不同 level 的特征图上的回归范围, 具体的, 先在特征图上计算回归的 targets: $l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}$, 接着如果一个位置满足 $max(l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}) &gt; m_i$ 或者 $min(l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}) &lt; m_{i-1}$, 那么这个位置就被认为是一个负样本 (negative sample), 不进行任何一个 bounding box 的 regression, 其中 $m_i$ 是 feature level $i$ 需要回归的最大距离, 文中 $m_2, m_3, m_4, m_5, m_6, m_7$ 分别设置为 $0, 64, 128, 256, 512, \infty$, 这样就实现了多尺度的预测, 如果一个位置在经过多尺度预测后在原图上还是存在多个 bounding box, 那么也是简单地选择 bounding box 面积最小的那个.</p><p>作者和 FPN 一样, 使用了权重共享的 head, 但是作者发现不同 feature level 需要回归的尺寸范围不同, 比如 P3 的范围是 [0, 64], P4 的范围是 [64, 128], 所以使用权重共享的 head 是不合理的, 但为了减少参数和计算量, 作者还是希望使用共享权重的 head, 所以作者并没有在特征图上使用标准的 $exp(x)$, 而是加了一个标量 $exp(s_{i}x)$, 用于在特征图 $P_i$ 上进行动态调整.</p><h2 id="Center-ness-for-FCOS"><a href="#Center-ness-for-FCOS" class="headerlink" title="Center-ness for FCOS"></a>Center-ness for FCOS</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_center-ness.png"><br>作者在使用了多尺度预测后发现和 anchor-based 方法相比还是存在精度上的 gap, 作者分析了原因, 认为这是由于很多特征图上的点映射到原图时会偏离物体的中心点太远, 这样的点产生的 bounding box 的质量都很低, 从而导致精度一直上不去, 作者提出了一个无需超参的方法来解决这个问题, 直接<strong>在 head 上加一个和分类分支并行的分支</strong>用于预测一个位置上的 center-ness, center-ness 度量的是特征图上的点映射到原图上的位置距离物体中心点的距离, 所以对于一个位置上给定的回归 targets: $l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}$, center-ness target 可以被定义为:<br>$$<br>\text { centerness }^{<em>}=\sqrt{\frac{\min \left(l^{</em>}, r^{<em>}\right)}{\max \left(l^{</em>}, r^{<em>}\right)} \times \frac{\min \left(t^{</em>}, b^{<em>}\right)}{\max \left(t^{</em>}, b^{*}\right)}}$$</p><p>上式可以看到使用平方根来减缓 center-ness 的衰减, center-ness 的范围是 [0, 1], 使用 BCE Loss 训练, 这个损失会添加到上面说的损失函数中, 在 test 时每个 bounding box 的 score 是预测的 center-ness 和 classification score 相乘得到的, 最后使用 NMS 来进一步过滤低质量的 bounding box. 作者在提交后也仅用真实的中心来作为正样本(其他位置不知道是设为负样本还是用二维高斯分布来做, 具体可以看源码)来代替 center-ness 的形式, 得到了更好的结果.</p><h2 id="Results-2"><a href="#Results-2" class="headerlink" title="Results"></a>Results</h2><p>可以看到没使用多尺度预测的 FCOS 的 BPR 已经很高了, 使用之后效果更好.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_results_bpr.png"><br>使用多尺度预测后的 “ambiguous samples” 比例显著嫌少<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_results_ambiguous_samples.png"><br>进行了消融性验证, 肯定了 center-ness 的重要性<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_results_center-ness.png"><br>作者把 FPN 中的 RPN 给换成了 FCOS, 然后用 FCOS 生成的 bounding box 在 FPN 上的结果和使用 RPN 的 FPN 结果进行了对比…发现 FPN with FCOS 完虐 FPN with RPN…<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_results_fcos_as_rpn.png"><br>最后就是和其他 SOTA 方法的效果对比了, 直接上图, 结果很不错.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_results_with_sota.png"><br>上图中的 improvements 包括:</p><ol><li>把 center-ness branch 从 classification branch 移动到 regression branch</li><li>只将 ground-truth boxes 的中心位置设置为正样本</li><li>使用 GIoU Loss</li><li>标准化回归的目标 $l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}$</li></ol>]]></content>
      
      
      <categories>
          
          <category> Object Detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Anchor Free </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Two-Stage Object Detection</title>
      <link href="2020/06/20/Two-Stage-Object-Detection/"/>
      <url>2020/06/20/Two-Stage-Object-Detection/</url>
      
        <content type="html"><![CDATA[<h2 id="Two-Stage-方法简述"><a href="#Two-Stage-方法简述" class="headerlink" title="Two-Stage 方法简述"></a>Two-Stage 方法简述</h2><p>two-stage检测器包括两个步骤：</p><ol><li>使用 RPN 生成 anchor 并对 anchor 做筛选，过滤掉很多的负样本 proposals，生成稀疏的 proposals；</li><li>对得到的 proposals 进行分类和回归，对选择的 proposals，使用 roi pooling(roi align) 等操作，进一步的精细化，因此得到的框更加精准。（比如，一个 anchor 有可能只覆盖了一个目标的50%，但却作为完全的正样本，因此其预测肯定是有误差的。）</li></ol><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/rcnn.jpg"></p><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ol><li>使用 CNN 进行特征提取。</li><li>使用 bounding box regression 进行目标框的修正。</li></ol><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol><li>输入一张图片，通过指定算法 (Selective Search) 从图片中提取 2000 个类别独立的候选区域（可能目标区域）</li><li>对于每个候选区域利用卷积神经网络来获取一个特征向量，这里无视候选框的大小和形状，直接暴力 resize 后输入到 CNN 中进行特征提取。</li><li>对于每个区域相应的特征向量，利用 SVM 进行分类，这里的 SVM 是一个二分类器，每个类别都训练一个 SVM。</li><li>使用 NMS 去除一些的候选框。</li><li>使用一个回归器进行边框回归，调整目标包围框的大小，这里输入的是 CNN 提取的特征，输出为 x, y 方向上的平移和缩放。</li></ol><h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><ol><li>耗时的 Selective Search 算法</li><li>耗时的串行式 CNN 前向传播，对于每一个候选框都需要经过预训练的 CNN 提取特征</li><li>三个模块是分开训练的（CNN特征提取、SVM分类、检测框修正），不是端到端。</li></ol><h2 id="SPPNet"><a href="#SPPNet" class="headerlink" title="SPPNet"></a>SPPNet</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/spp.jpg"></p><h3 id="贡献-1"><a href="#贡献-1" class="headerlink" title="贡献"></a>贡献</h3><p>提出了 spatial pyramid pooling layer，解决了利用深度学习算法进行特征学习时输入的样本图片的尺寸必须时固定的问题。</p><h3 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h3><p>解决了利用深度学习算法进行特征学习时输入的样本图片的尺寸必须时固定的问题，这里的固定实际上就是指在网络训练过程中输入的图像的尺寸必须一样大。虽然可以采用 crop 以及 warp 的方法强制将图片转换到同一个尺寸，但是由此产生图片本身信息的形变和缺失在一定程度上限制了网络的性能。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/crop_warp_spp.jpg"><br>图中第一行对应的是一般情况下CNN网络的结构，对于一个给定的图像，需要先采取 crop 或者是 warp 操作来将图像转换为一个固定的大小，这期间就会导致原始图像视觉信息的缺失和扭曲（如图）。第二行是文中提出的网络结构，该结构可以输入任意尺寸的图像，在通过卷积操作后利用 spatial pytramid pooling 空间金字塔池化操作得到尺寸一致的特征向量，从而避免了由于输入图像尺寸不一致带来的全连接层参数不固定的情形。</p><h3 id="Spatial-Pyramid-Pooling"><a href="#Spatial-Pyramid-Pooling" class="headerlink" title="Spatial Pyramid Pooling"></a>Spatial Pyramid Pooling</h3><p>对经过 backbone 得到的特征图进行池化操作，利用不同 stride 和 size 来得到不同的池化层结果，构成最终的空间金字塔。对于 $a \times a$ 的特征图，如果想得到 $n \times n$ 的特征，那么池化层的 stride 和 kernel_size 分别为：<br>$$<br>size = \lceil \frac{a}{n} \rceil \<br>stride = \lfloor \frac{a}{n} \rfloor<br>$$</p><p>如图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/spp_example.png"><br>对于 size 为 $13 \times 13$ 的特征图，如果我们想得到一个 3 层的空间金字塔池化，即需要分别得到 $4 \times 4$、$2 \times 2$ 和 $1 \times 1$ 的特征，那么分别需要 kernel_size=$\lceil \frac{13}{4} \rceil = 4$，stride = $\lfloor \frac{13}{4} \rfloor = 3$ 的池化层用来得到 $4 \times 4$ 的特征，需要 kernel_size=$\lceil \frac{13}{2} \rceil = 8$，stride = $\lfloor \frac{13}{2} \rfloor = 7$ 的池化层用来得到 $2 \times 2$ 的特征，以及 kernel_size=$\lceil \frac{13}{1} \rceil = 13$，stride = $\lfloor \frac{13}{1} \rfloor = 13$ 的池化层用来得到 $1 \times 1$ 的特征。</p><h3 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h3><h3 id="不足-1"><a href="#不足-1" class="headerlink" title="不足"></a>不足</h3><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/fast_rcnn.jpg"></p><h3 id="贡献-2"><a href="#贡献-2" class="headerlink" title="贡献"></a>贡献</h3><ol><li>只对整个图像进行一次特征提取，避免 R-CNN 的上千次特征提取，提升速度。</li><li>提出 RoI Pooling 保证不同 size 的 proposals 的特征保持固定大小, 便于分类和回归的同时不会像直接暴力 resize 那样丢失信息</li></ol><h3 id="算法步骤-2"><a href="#算法步骤-2" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol><li>首先还是采用 selective search 提取 2000 个候选框 RoI</li><li>使用一个卷积神经网络对全图进行特征提取</li><li>使用一个 RoI Pooling Layer 在全图特征上摘取每一个 RoI 对应的特征</li><li>分别经过为21和84维的全连接层（并列的，前者是分类输出，后者是回归输出） Fast R-CNN 通过 CNN 直接获取整张图像的特征图，再使用 RoI Pooling Layer 在特征图上获取对应每个候选框的特征，避免了 R-CNN 中的对每个候选框串行进行卷积（耗时较长）。</li></ol><h3 id="RoI-Pooling"><a href="#RoI-Pooling" class="headerlink" title="RoI Pooling"></a>RoI Pooling</h3><h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><ol><li>把原图上所有的 RoIs 的坐标映射到特征图的对应位置，即为每个 RoI 提取特征。</li><li>把不同 size 的 RoI 进行处理，使其形成大小固定的特征图，便于后续接全连接层进行分类和回归。</li><li>传统的 crop 与 warp 会破坏图像原有的结构信息，导致结果不好，RoI Pooling 不存在这个问题。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/crop_and_warp.jpg"></li></ol><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><ol><li>将 RoIs 分为目标为 $H \times W$ 大小的分块</li><li>对每个分块做 Max Pooling</li><li>把所有结果组合起来便形成了 $H \times W$ 的特征图</li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_pooling.gif"><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_pooling.jpeg"></p><h3 id="不足-2"><a href="#不足-2" class="headerlink" title="不足"></a>不足</h3><ol><li>还是耗时的 Selective Search 算法</li><li>由于使用的是 Selective Search, 所以仍然无法实现真正的端到端</li></ol><h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.jpg"><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.png"></p><h3 id="贡献-3"><a href="#贡献-3" class="headerlink" title="贡献"></a>贡献</h3><ol><li>提出 RPN 来生成 proposals 而不是 Selective Search，极大提升了检测速度。</li><li>真正意义上实现了端到端的检测。</li></ol><h3 id="算法步骤-3"><a href="#算法步骤-3" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol><li>使用预训练好的 CNN 为全图进行特征提取，得到共享的特征图。</li><li>将得到的特征图送入 RPN，RPN 生成 proposals(RoIs)，并对每个 RoI 的边界框进行第一次修正。</li><li>RoI Pooling 对 RPN 得到的修正后的 RoIs 进行特征提取并固定最终特征维度。</li><li>使用全连接层对边界框进行分类和回归，这里回归就是对这些边界框进行第二次修正。</li></ol><h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/rpn.jpg"><br>可以看到 RPN 实际上有 2 条分支，上面的一条分支是分类分支，通过 softmax 来分类 anchors ，判断其是属于前景还是背景；下面的是回归分支，用于计算 anchors 的边框偏移量，从而获得更加精准的 proposals，最后的 proposal 层负责综合前景的边界框和偏移量来获取最终的 proposals 并同时剔除太小和超出边界的 proposals。</p><h4 id="共享卷积后的-3×3-卷积"><a href="#共享卷积后的-3×3-卷积" class="headerlink" title="共享卷积后的 3×3 卷积"></a>共享卷积后的 3×3 卷积</h4><p>在特征图进入2个分支之前还有一个 $3 \times 3$ 的卷积，这个卷积的作用是为了能更好的进行分类和边界框回归，因为预训练好的网络得到的特征图往往是有着较强的分类特征，使用 $3 \times 3$ 卷积对其进行二次变换能够将分类特征迁移到 分类和回归 这2个 task 所需要的分布上来。也有一种说法是进一步扩大感受野从而匹配图像中的目标的大小，可以试着去掉这个卷积或者把这个 $3 \times 3$ 的卷积只用于回归分支，分类分支直接用 $1 \times 1$ 卷积。</p><h4 id="不同分支前的-1×1-卷积"><a href="#不同分支前的-1×1-卷积" class="headerlink" title="不同分支前的 1×1 卷积"></a>不同分支前的 1×1 卷积</h4><ul><li>分类分支<br>分类分支的 $1 \times 1$ 卷积把特征图的通道数变为18, 这是由于一共设置了 9 个 anchor, 对每个 anchor 判断其为前景还是背景, 即预测每个 anchor 是否含有物体</li><li>回归分支<br>回归分支的 $1 \times 1$ 卷积把特征图的通道数变为36, 每个特征图上的点设置 9 个 anchor, 每个 anchor 的坐标是一个 4 维的向量 (x, y, w, h), 所以是 $4 \times 9 = 36$</li></ul><h4 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a>Anchor</h4><p>Anchor 可以定义为 RPN 为特征图上的每一个像素点生成 9 种预先设置好长宽比的目标框。包括 3 个 anchor scale，每个 scale 包含 3 种长宽比 (1:1, 1:2, 2:1)，以下图为例详细说明：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/generate_anchor.jpg"></p><p>如上图所示，例如原图尺寸为  $800 \times 600$，backbone 为 ResNet，那么经过 backbone 后的尺寸为  $\frac{800}{16} \times \frac{600}{16} = 50 \times 38$，这样的话就有 $50 \times 38$ 个 grid，每个 grid 都生成 9 个 anchor，总的 anchor 数为 $50 \times 38 \times 9 = 17100$。</p><p>其实 RPN 最终就是在原图尺度上，设置了密密麻麻的候选 anchor。进而去判断 anchor 到底是前景还是背景，意思就是判断这个 anchor 到底有没有覆盖目标，以及为属于前景的 anchor 进行第一次坐标修正。</p><h4 id="判断前景和背景"><a href="#判断前景和背景" class="headerlink" title="判断前景和背景"></a>判断前景和背景</h4><p>直接使用 Softmax Loss 进行训练, 在训练的时候排除超过图像边界的 anchor.</p><h4 id="边框修正"><a href="#边框修正" class="headerlink" title="边框修正"></a>边框修正</h4><p>给定 anchor 的坐标: $(A_{x}, A_{y}, A_{w}, A_{h})$ 和 ground truth 的坐标 $(G_{x}, G_{y}, G_{w}, G_{h})$，我们的目的是找到一种变换使得<br>$$<br>F\left(A_{x}, A_{y}, A_{w}, A_{h}\right)=\left(G_{x}^{\prime}, G_{y}^{\prime}, G_{w}^{\prime}, G_{h}^{\prime}\right)<br>$$</p><p>其中<br>$$<br>(G_{x}, G_{y}, G_{w}, G_{h}) \approx (G_{x}^{\prime}, G_{y}^{\prime}, G_{w}^{\prime}, G_{h}^{\prime})<br>$$</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/bbox_regression.jpg"></p><p>这里的变换可以通过平移和缩放来实现：</p><ul><li><p>平移(anchor 中心点的移动)<br>$$<br>\begin{aligned}<br>&amp;G_{x}^{\prime}=A_{w} \cdot t_x+A_{x}\<br>&amp;G_{y}^{\prime}=A_{h} \cdot t_y+A_{y}<br>\end{aligned}<br>$$</p></li><li><p>缩放(anchor 长和宽的缩放)<br>$$<br>\begin{aligned}<br>&amp;G_{w}^{\prime}=A_{w} \cdot \exp \left(t_w\right)\<br>&amp;G_{h}^{\prime}=A_{h} \cdot \exp \left(t_h\right)<br>\end{aligned}<br>$$</p></li></ul><p>由上述公式可以看出我们需要学习 4 个修正参数, 分别是:<br>$$<br>t_x, t_y, t_w, t_h<br>$$</p><p>这 4 个参数的 ground truth 值可以通过以下公式算出：</p><p>$$\begin{array}{l}<br>t_{x}=\left(G_{x}-A_{x}\right) / A_{w} \<br>t_{y}=\left(G_{y}-A_{y}\right) / A_{h} \<br>t_{w}=\log \left(G_{w} / A_{w}\right) \<br>t_{h}=\log \left(G_{h} / A_{h}\right)<br>\end{array}$$</p><p>这样在得到每一个 anchor 的修正参数后就能计算出精准的 anchor, 然后根据 score 值对 anchor 进行排序, 先剔除一些高或者宽小的 anchor (在分类分支判断前景和背景时就剔除了超过边界框的 anchor), 再进行 NMS 处理, 取先 Top-N 的 anchors 作为 proposals 输出到 RoI Pooling 层。</p><h3 id="分类和定位"><a href="#分类和定位" class="headerlink" title="分类和定位"></a>分类和定位</h3><p>这里和 Fast R-CNN 原理一样，在 RoI Pooling Layer 之后，就是 aster R-CNN 的分类器和 RoI 边框修正训练。分类器主要是分这个提取的 RoI 具体是哪个类别(人，车，马等)，一共 C+1 类(包含一类背景)。</p><p>RoI 边框修正和 RPN 中的 anchor 边框修正原理一样，同样也是 Smooth L1 Loss，值得注意的是，RoI 边框修正也是对于非背景的 RoI 进行修正，对于类别标签为背景的 RoI，则不进行 RoI 边框修正的参数训练。</p><h4 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h4><ol><li>首先通过 RPN 生成约 20000 个 anchor (40×60×9)</li><li>对20000个 anchor 进行第一次边框修正，得到修订边框后的 proposal</li><li>对超过图像边界的 proposal 的边进行 clip，使得该 proposal 不超过图像范围</li><li>忽略掉长或者宽太小的 proposal</li><li>将所有 proposal 按照前景分数从高到低排序，选取 Top-N 个 proposal</li><li>使用阈值为 0.7 的 NMS 算法排除掉重叠的 proposal</li><li>对于上一步剩下的 proposal，选取 Top-N 个 proposal 进行分类和第二次边框修正</li></ol><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>Faster R-CNN 的 loss 分两大块，第一大块是训练 RPN 的 loss (包含一个 Softmax Loss 和 Smooth L1 Loss)，第二大块是训练 Faster R-CNN 中分类器的 loss (包含一个 Softmax Loss 和 Smooth L1 Loss)。</p><ol><li>RPN 分类损失：anchor是否为前景（二分类）</li><li>RPN位置回归损失：anchor位置微调</li><li>RoI 分类损失：RoI所属类别（21分类，多了一个类作为背景）</li><li>RoI位置回归损失：继续对RoI位置微调</li></ol><p>四个损失相加作为最后的损失，反向传播，更新参数。</p><h4 id="RPN-损失函数"><a href="#RPN-损失函数" class="headerlink" title="RPN 损失函数"></a>RPN 损失函数</h4><p>RPN 的损失函数为分类损失和回归损失之和：</p><p>$$<br>\mathrm{L}\left(\left{p_{i}\right},\left{t_{i}\right}\right)=\frac{1}{N_{\mathrm{cls}}} \sum_{i} \mathrm{L}<em>{\mathrm{cls}}\left(p</em>{i}, p_{i}^{<em>}\right)+\lambda \frac{1}{N_{\mathrm{reg}}} \sum_{i} p_{i}^{</em>} \mathrm{L}<em>{\mathrm{reg}}\left(t</em>{i}, t_{i}^{*}\right)<br>$$</p><p>注意这里对每一个 anchor 计算完 $\mathrm{L}_{\mathrm{reg}}$ 后还要乘以一个 $p_i^*$，也就是说<strong>只有前景才计算回归损失，背景的 anchor 是不计算回归损失的。</strong></p><h5 id="分类损失"><a href="#分类损失" class="headerlink" title="分类损失"></a>分类损失</h5><p>分类损失为交叉熵损失即Softmax Loss：<br>$$<br>L_{c l s}\left(p_{i}, p_{i}^{<em>}\right)=-\log \left[p_{i}^{</em>} p_{i}+\left(1-p_{i}^{*}\right)\left(1-p_{i}\right)\right]<br>$$</p><p>其中 $p_i$ 为 anchor 预测为目标的概率，$p_i^*$ 为 Ground Truth 的标签, 即<br>$$<br>p_{i}^{*}=\left{\begin{array}{cl}0 &amp; \text { negative label } \ 1 &amp; \text { positive label }\end{array}\right.<br>$$</p><h5 id="回归损失"><a href="#回归损失" class="headerlink" title="回归损失"></a>回归损失</h5><p>回归损失为 Smooth L1 损失：<br>$$\begin{aligned}<br>&amp;\mathrm{L}<em>{\mathrm{reg}}\left(t</em>{i}, t_{i}^{<em>}\right)=\sum_{i \in{x, y, w, h}} \operatorname{smooth}<em>{\mathrm{L} 1}\left(t</em>{i}-t_{i}^{</em>}\right)\<br>&amp;\operatorname{soomth}_{L1}(x)=\left{\begin{array}{ll}<br>0.5 x^{2} &amp; \text { if }|x|&lt;1 \<br>|x|-0.5 &amp; \text { otherwise }<br>\end{array}\right.<br>\end{aligned}$$</p><h4 id="Faster-R-CNN-损失函数"><a href="#Faster-R-CNN-损失函数" class="headerlink" title="Faster R-CNN 损失函数"></a>Faster R-CNN 损失函数</h4><h5 id="分类损失-1"><a href="#分类损失-1" class="headerlink" title="分类损失"></a>分类损失</h5><p>RPN的分类损失时二分类的交叉熵损失，而 Fast R-CNN 是多分类的交叉熵损失</p><h5 id="回归损失-1"><a href="#回归损失-1" class="headerlink" title="回归损失"></a>回归损失</h5><p>这里和 RPN 中的回归损失一致。</p><h3 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h3><p>由于在 RPN 网络中特征提取的过成中也用到了 ZF 和VGG 网络结构，针对这样一个整体的识别任务，文中指出其实这部分网络在提取 proposal 和 目标检测识别两阶段任务中其实时可以共享的，所以文中采取四步交替迭代的策略对整个网络进行训练：</p><ol><li>在通过image-Net 初始化共享网络结构参数，训练RPN网络。</li><li>用训练好的RPN网络生成 proposal，训练 Fast-R-CNN 部分。</li><li>在刚才Fast-R-CNN 训练过程中可共享的ZF 或者VGG-16 网络部分训练模型基础上，也就是固定共享网络的参数，对RPN网络没有被共享的层进行fine-tuning。</li><li>同样固定共享网络，对 Fast-R-CNN 非共享部分进行 fine-tuning. 这个过程可以迭代执行，但是作者指出，这个过程迭代多次并没有效果，所以只执行一次。</li></ol><h2 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a>R-FCN</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/rfcn_overview.png"></p><h3 id="贡献-4"><a href="#贡献-4" class="headerlink" title="贡献"></a>贡献</h3><p>对 RoI Pooling 进行了改进，利用 Position-sensitive score maps 和 Position-sensitive RoI Pooling 使得目标检测网络应该在保证判别性的基础上具有一定的位置敏感性。并且让网络丢弃全连接层使用全卷积层，不但效果更好而且能适应不同尺度的图片。</p><h3 id="思想-2"><a href="#思想-2" class="headerlink" title="思想"></a>思想</h3><p>Fast R-CNN 发明的 RoI Pooling 中间是由全连接层存在，从而将前面的 RoI Pooling 后的 feature map 映射成两个部分（对象分类，坐标回归）；而越来越多的 backbone 网络，如 GoogleNet，ResNet 等全卷积网络证明了不要全连接层，效果不但更好，而且能适应不同尺度的图片，所以为了消灭 Fast R-CNN 的 RoI Pooling 中的全连接层，R-FCN 被提出来了。</p><h3 id="Translation-invariance-variance"><a href="#Translation-invariance-variance" class="headerlink" title="Translation invariance/variance"></a>Translation invariance/variance</h3><p>这两个次对应的中文含义可以理解为 “位置不敏感性” 以及 “位置敏感性”，文中意在指出分类任务和目标检测任务的不同，比如在分类问题中，我们并不需要模型对物体的位置敏感，如果图像中存在这个物体，无论这个物体如何移动，我们都可以将该物体分对。 但是在目标检测中，我们希望我们的模型对目标位置是非常敏感的，这样才可以去准确的定位目标。所以从模型上讲分类模型和目标检测模型不应该是完全相同的，目标检测网络应该在保证判别性的基础上具有一定的位置敏感性。也就是说 backbone CNN 模型本身是对图像分类设计的，具有图像移动不敏感性；而对象检测领域却是图像移动敏感的，所以二者之间产生了矛盾，解决的办法就是对 RoI Pooling 进行了改进。</p><h3 id="Position-sensitive-score-maps"><a href="#Position-sensitive-score-maps" class="headerlink" title="Position-sensitive score maps"></a>Position-sensitive score maps</h3><p>文中网络的改进基于 Faster R-CNN 模型，基础网络为 ResNet-101，网络的主要改进集中于对目标检测部分中全连接层的简化，称为 Region-specific Fully Convolutional Network (R-FCN)，RPN 网络保持不变。网络结构如下图所示：</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/rfcn.jpg"></p><p>这里作者设计一个位置敏感的 RoI Pooling：将 Faster R-CNN 中的 RoI 划分成 $k \times k$ 大小，即图片中本来获取的 RoI 区域，将其分成 $k \times k$ 个 区域（这里 k=3，即分成 9 个部分）。假设该数据集一共由 C 类，那么再加个背景类，一共是 C+1 类。我们希望对每个类别都有各自的位置感应，所以需要设计上图中的 <code>position-sensitive score maps</code> 。就是几个大色块并列的部分：每个色块表示对对象的特定位置进行敏感，而且每个色块大小中有 C+1 个 feature map，所以该区域一共有 $k^{2} \cdot (C+1)$ 个map，其中每个 map 的大小和之前那个 backbone 得到的 feature map 大小一致。</p><p>Position-sensitive score maps 的直观含义就是 $C+1$ 个类在 $k^2$ 个位置上的概率。</p><h3 id="Position-sensitive-RoI-Pooling"><a href="#Position-sensitive-RoI-Pooling" class="headerlink" title="Position-sensitive RoI Pooling"></a>Position-sensitive RoI Pooling</h3><p>这部分详细解释了如何利用上图中的 <code>Position-sensitive score maps</code> 得到最右边那个 $k \cdot k \cdot (c+1)$ 的特征图。</p><p>这里每个小区域都在对应的 $C+1$ 个维度上做 pooling 操作，比如 RoI 左上角的区域就在前 $C+1$ 个维度上 pooling，左中位置的区域就在 $C+2$ 到 $2(C+1)$ 间的维度上做 pooling，以此类推。pooling 后输出的是 $C+1$ 维度的 $k \times k$ 数据，每个维度上的 $k \times k$ 数据相加就得到了一个 $C+1$ 维的向量，这个向量就表示了当前的这个 proposal 属于某一个类的分数，后面接上 softmax 就能得到当前 proposal 属于每一个类的概率。</p><p>我们可以发现，在生成 positive-sensitive score map 后的计算都是固定的，没有网络参数的训练过程，这使得目标检测的速度进一步提高。回归部分类似于这个部分，只是在生成 positive-sensitive score maps 时我们需要得到总共 $4k^2(C+1)$ 分别对应于四个回归偏移量。表示９个小区域的 $[dx,dy,dw,dh]$ 4个偏移坐标。</p><h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h2><h3 id="贡献-5"><a href="#贡献-5" class="headerlink" title="贡献"></a>贡献</h3><p>提出了特征金字塔结构，在不增加计算量的同时解决了多尺度的问题。</p><h3 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h3><h4 id="Image-Pyramid"><a href="#Image-Pyramid" class="headerlink" title="Image Pyramid"></a>Image Pyramid</h4><p>传统的数字图像处理技术中为了解决多尺度问题会使用图像金字塔，就是将图片 resize 到不同的大小，然后对不同 size 的图像提取特征再进行预测，虽然这种方法可以在一定程度上解决多尺度的问题，但是带来的计算量也是非常巨大。如图 a 所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/image_pyramid.jpg"></p><h4 id="Single-Feature-Map"><a href="#Single-Feature-Map" class="headerlink" title="Single Feature Map"></a>Single Feature Map</h4><p>很多经典算法是使用单个 feature map 进行检测，这种结构在 2017 年的时候是很多人在使用的结构，比如YOLOv1、YOLOv2、Faster R-CNN 中使用的就是这种架构。直接使用这种架构导致预测层的特征尺度比较单一，对小目标检测效果比较差。ps: YOLOv2 中使用了 multi-scale training 的方式一定程度上缓解了尺度单一的问题，能够让模型适应更多输入尺度。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/single_feature_map.jpg"></p><h4 id="Pyramidal-Feature-Hierarchy"><a href="#Pyramidal-Feature-Hierarchy" class="headerlink" title="Pyramidal Feature Hierarchy"></a>Pyramidal Feature Hierarchy</h4><p>理所应当地，有方法在不同大小的 feature map 上分别进行预测，具有了多尺度预测的能力，但是特征与特征之间没有融合，遵从这种架构的经典的目标检测架构就是 SSD, SSD 用了非常多的尺度来进行检测。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/pyramid_feature_hierarchy.jpg"></p><h4 id="Feature-Pyramid-Network"><a href="#Feature-Pyramid-Network" class="headerlink" title="Feature Pyramid Network"></a>Feature Pyramid Network</h4><p>然后就是非常经典的FPN架构，FPN可以非常方便地应用到两阶段网络如 Faster R-CNN 等或者一阶段网络 YOLO、SSD 等。FPN 通过构造一种独特的特征金字塔来避免图像金字塔中计算量过高的问题，同时能够较好地处理目标检测中的多尺度变化问题，效果能够达到当时的 STOA。SSD 的一个改进版 DSSD 就是使用了 FPN，取得了比 SSD 更好的效果。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/feature_pyramid.jpg"></p><h3 id="FPN-结构"><a href="#FPN-结构" class="headerlink" title="FPN 结构"></a>FPN 结构</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/fpn.jpg"><br>整个结构可以分为左边的 Down-Top 和右边的 Top-Down，其中 Down-Top 就是 backbone 提取特征的过程，随着深度的增大，分辨率在减小，通道数在增加，以常用的 ResNet 为例，选取 conv2、conv3、conv4、conv5 层的最后一个残差 block 层特征作为FPN的特征，因为每个阶段的最深层应该具有最强的特征。，记为 {C2、C3、C4、C5}，也即是FPN网络的 4 个级别。这几个特征层相对于原图的步长分别为 4、8、16、32，就是说这几个层得到的特征图的分辨率分别是原图的 $\frac{1}{4}$、$\frac{1}{8}$、$\frac{1}{16}$ 和 $\frac{1}{32}$，没有把 conv1 考虑在内的一个原因是内存占用。</p><p>Top-Down 部分可以看成是特征融合，融合了底层低语义高分辨率的特征以及高层高语义低分辨率的特征，以上图为例的具体过程为：</p><p>C5 层先经过 1 x 1 卷积，改变特征图的通道数(文章中设置 d=256，与 Faster R-CNN 中 RPN 层的维数相同便于分类与回归)。M5 通过上采样，再加上(特征图中每一个相同位置元素直接相加) C4 经过 1 x 1 卷积后的特征图，得到 M4，即<strong>这里特征融合的方式为 element-wise add</strong>。这个过程再做两次，分别得到 M3，M2。M 层特征图再经过 3 x 3 卷积(减轻最近邻近插值带来的混叠影响，周围的数都相同)，得到最终的 P2，P3，P4，P5 层特征。和传统的图像金字塔方式一样，所有 M 层的通道数都设计成一样的，文中都用 d=256。</p><h3 id="FPN-在-RPN-中的应用"><a href="#FPN-在-RPN-中的应用" class="headerlink" title="FPN 在 RPN 中的应用"></a>FPN 在 RPN 中的应用</h3><p>Faster R-CNN 中的 RPN 是通过最后一层的特征来做的。最后一层的特征经过 3 x 3 卷积，得到 256 个 channel 的特征图，再分别经过两个 1 x 1 卷积得到类别得分和边框回归结果。这里将经过 3 x 3 卷积得到的特征图之后的 RPN 子网络称之为网络头部（network head），在 Faster R-CNN 中有 2 个头部(分支)，一个用于判断每个 anchor 是否含有目标，另一个用于对 anchor 进行坐标修正，对于特征图上的每一个点，作者用 anchor 的方式预设了 9 个框。这些框本身包含不同的尺度和不同的长宽比例。</p><p><strong>FPN 针对 RPN 的改进是将网络头部应用到每一个 P 层，可以理解为在 Faster-R-CNN 中只对 P5 添加了 head (分类分支和回归分支)，而这里是对 P2-P6 都添加了相同的 head(因为参数是共享的)。由于每个 P 层相对于原始图片具有不同的尺度信息，因此作者将原始 RPN 中的尺度信息分离，让每个 P 层只处理单一的尺度信息</strong>。具体的，对 {32 x 32、64 x 64、128 x 128、256 x 256、512 x 512} 这五种尺度的 anchor，分别对应到 {P2、P3、P4、P5、P6} 这五个特征层上。每个特征层都处理 1:1、1:2、2:1 三种长宽比例的候选框。P6 是专门为了 RPN 网络而设计的，用来处理 512 x 512 大小的候选框。它由 P5 经过下采样得到。如下图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/fpn_p6.jpg"></p><p>P2-P5 将来用于预测物体的 bbox，box-regression，mask 的，而 P2-P6 是用于训练 RPN 的，即 P6 只用于 RPN 网络中。</p><p><strong>上述 5 个网络头部的参数是共享的</strong>，作者通过实验发现，网络头部参数共享和不共享两种设置得到的结果几乎没有差别。这说明不同层级之间的特征有相似的语义层次。这和特征金字塔网络的原理一致。</p><h3 id="FPN-对-RPN-的影响"><a href="#FPN-对-RPN-的影响" class="headerlink" title="FPN 对 RPN 的影响"></a>FPN 对 RPN 的影响</h3><p>论文还做了 6 个实验来测试 FPN 对 RPN 的影响：<br>(a): 基于 conv4 的 RPN，原始的 RPN<br>(b): 基于 conv5 的 RPN，原始的 RPN<br>(c): 完整FPN<br>(d): 只用了自底向上的多层特征，没有自顶向下的特征。<br>(e): 用了自顶向下的特征，但不用侧向连接。<br>(f): 用了自顶向下的特征，也用了横向特征融合，但只用最后的 P2 做预测。（完整的预测是使用每一个 level 的特征做预测，即 P2-P6 均做预测）</p><p>得到的结果如下：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/fpn_results_on_rpn.jpg"><br>由结果可知，自顶向下的特征、横向连接、尺度分离、多个层次的预测是提升 FPN 性能的关键</p><h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><p>Mask R-CNN 可以分解为如下的3个模块：Faster-R-CNN、RoI Align 和 Mask。算法框架如下：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/mask_rcnn_overview.jpg"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/mask_rcnn.jpg"></p><h3 id="贡献-6"><a href="#贡献-6" class="headerlink" title="贡献"></a>贡献</h3><ol><li>提出了 RoI Align，解决了 RoI Pooling 中量化操作导致结果出现偏差的问题。</li><li>可以添加不同的分支完成不同的任务，模型非常的灵活。</li></ol><h3 id="算法步骤-4"><a href="#算法步骤-4" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol><li>将图像进行预处理之后输入到预训练好的神经网络中，获得对应的 feature map</li><li>为这个 feature map 上的每一个点设置若干个 anchor，得到多个候选的 RoI</li><li>将 RoIs 送入 RPN 中进行分类(前景/背景)和 bounding box 回归，过滤掉一部分 RoIs</li><li>对剩下的 RoIs 进行 RoI Align (即先将原图和 feature map 的 pixel 对应起来，然后将 feature map 和固定的 feature 对应起来)</li><li>对这些 RoI 进行分类（N类别分类）、BB回归和 Mask 生成（在每一个 ROI 里面进行 FCN 操作）。</li></ol><h3 id="RoI-Pooling-的问题"><a href="#RoI-Pooling-的问题" class="headerlink" title="RoI Pooling 的问题"></a>RoI Pooling 的问题</h3><p>RoI Pooling 在整个模型过程中使用了 2 次量化(取整)操作，这 2 次量化(取整)操作引入的误差会导致图像中的响度和 feature map 中的像素存在偏差，即将 feature map 上的 RoI 对应到原图上时会出现很大偏差。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_pooling_defect.jpg"></p><p>下面举例来具体说明：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_pooling_example.png"><br>如图所示，为了得到大小固定($7 \times 7$)的特征图，RoI Pooling 需要做 2 次量化(取整)操作：</p><ol><li>图像坐标 $\rightarrow$ feature map 坐标</li><li>feature map 坐标 $\rightarrow$ RoI feature 坐标</li></ol><p>以上图为例，输入的图像为 $800 \times 800$，其中狗的 Bounding Box 大小为 $665 \times 665$，在经过 VGG 的 backbone 后得到对应的 feature map 的大小为 $\frac{800}{32} \times \frac{800}{32} = 25 \times 25$，是个整数。但对于狗的 Bounding Box 而言，在 feature map 的 size 为 $\frac{665}{32} \times \frac{665}{32} = 20.78 \times 20.78$，是个浮点数，由于像素值没有小数所以需要进行量化(取整)操作，将结果变为 $20 \times 20$，这里就引入了第一次量化误差。</p><p>由于每个 RoI 的大小是不一致的，但后面的网络需要有固定的输入，所以需要把不同大小的 RoI 转换为固定的 RoI feature，以上图为例，为了把 $20 \times 20$ 的 RoI 映射为 $7 \times 7$ 的 RoI feature，实际上需要在 $20 \times 20$ 的 RoI 上每隔 $\frac{20}{7}$ 就进行一次 pooling，但 $\frac{20}{7}=2.86$ 同样是个浮点数，所以需要进行第二次量化(取整)操作，即上图中所示的每隔 $\frac{20}{7}=2$ 就进行一次 pooling。</p><p>在 RoI Pooling 中这 2 次量化(取整)操作大大影响了检测算法的性能，所以作者提出了 RoI Align 来解决这个问题。</p><h3 id="RoI-Align"><a href="#RoI-Align" class="headerlink" title="RoI Align"></a>RoI Align</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_align_example.jpg"></p><p>上面说到量化(取整)操作会引入误差从而影响检测效果，所以 RoI Align 中没有进行取整操作，比如 $\frac{665}{32} = 20.78$，就使用 20.78 而不是取整后使用 20，比如 $\frac{20.78}{7} = 2.97$，同样直接使用 2.97 而不是 2。对于这些浮点数，使用<code>双线性插值</code>来解决。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_align.jpg"></p><p>蓝色虚线(上图中的 feat.map )就是经过 backbone 得到的特征图，黑色实线表示特征图上的一个 RoI，如果最终需要的固定大小的 RoI feature 是 $2 \times 2$ 大小的，那么需要把 RoI 分成 $2 \times 2$ 个小区域，对每个区域进行 max pooling 或者 average pooling，最终得到 $2 \times 2$ 大小的 RoI feature。</p><p>怎么对每个小区域进行 max pooling 或者 average pooling 呢? 这里需要对每个小区域都进行采样，假设采样点为 4 的话，如上图所示，每个小区域就有 4 个蓝色的采样点，每个采样点的像素值是通过<code>双线性插值</code>得到的，每个小区域进行 max pooling 或者 average pooling 实际上就是对这 4 个采样点的像素值求最大值或者平均值。作者在文中说明采样点的个数和位置都不会对结果有比较大的影响，原因在于 RoI Align 本质上解决了 2 次量化取整操作所带来的像素区域不匹配问题，在目标检测任务中，小目标更容易受到这种问题的影响。</p><h3 id="线性插值和双线性插值"><a href="#线性插值和双线性插值" class="headerlink" title="线性插值和双线性插值"></a>线性插值和双线性插值</h3><h4 id="线性插值"><a href="#线性插值" class="headerlink" title="线性插值"></a>线性插值</h4><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/linear_interpolation.jpg"></p><p>这个很直观了，[x, y] 点的值可以通过 [x1, y1] 和 [x2, y2] 这 2 个点的值计算得到。</p><h4 id="双线性插值"><a href="#双线性插值" class="headerlink" title="双线性插值"></a>双线性插值</h4><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/bilinear_interpolation.png"></p><p>和线性插值同理，$R_2$ 的值可以通过 $Q_{12}$ 和 $Q_{22}$ 得到，$R_1$ 的值可以通过 $Q_{11}$ 和 $Q_{21}$ 得到，最终 $P$ 点的值就能由 $R_2$ 和 $R_1$ 得到。</p><h3 id="Mask-分支"><a href="#Mask-分支" class="headerlink" title="Mask 分支"></a>Mask 分支</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/mask_rcnn_mask.jpg"><br>可以看到作者在原来 Faster R-CNN 的基础上加了一个 mask 分支用于分割任务，作者没有采用 FCN 式的SoftmaxLoss，反而是输出了 K 个 Mask 预测图(为每一个类都输出一张)，并采用 average binary cross-entropy loss 训练，当然在训练 Mask branch 的时候，输出的 K 个特征图中，也只是对应 ground truth 类别的那一个特征图对 Mask Loss 有贡献。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/mask_rcnn_mask_branch.jpg"></p><p>作者实验了 2 个 mask 分支，对于每一个 RoI 的 mask 都有 80 类，因为 coco 上的数据集是 80 个类别，并且这样做的话是为了减弱类别间的竞争，从而得到更加好的结果。</p><h3 id="Classifier-分支"><a href="#Classifier-分支" class="headerlink" title="Classifier 分支"></a>Classifier 分支</h3><p>这里实际上由 2 个分支组成，就是 Faster-R-CNN 中的分类分支和回归分支。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/mask_rcnn_classifier.jpg"></p><h3 id="Mask-R-CNN-训练和预测"><a href="#Mask-R-CNN-训练和预测" class="headerlink" title="Mask R-CNN 训练和预测"></a>Mask R-CNN 训练和预测</h3><p>训练和预测是分开的，不是套用同一个流程。在训练的时候，classifier 和 mask 都是同时进行的；在预测的时候，先得到 classifier 的结果，然后再把此结果传入到 mask 预测中得到 mask，有一定的先后顺序。</p><h2 id="Cascade-R-CNN"><a href="#Cascade-R-CNN" class="headerlink" title="Cascade R-CNN"></a>Cascade R-CNN</h2><h3 id="贡献-7"><a href="#贡献-7" class="headerlink" title="贡献"></a>贡献</h3><p>提出了 cascade 结构，解决了单纯提高 IoU 阈值反而导致检测精度下降的问题，通过设置不同 IoU 阈值的 Head 来逐渐提高 proposals 和 GT 的 IoU，从而逐步提升 proposals 的质量，最终解决 Training 过程中的过拟合问题(负例过多正例过少)以及 Inference 阶段的 proposals 存在的 mismatch 问题。</p><p>在 Training 阶段通过设置级联的 head 能得到更多的正例，一定程度上避免了过拟合(其实 Focal Loss 也能解决这个问题)，在 Inference 阶段每个 proposal 通过 multi-stage 的 head，其实也是得到了更多正例(有更多的 proposals 和 GT 的 IoU 更大)，这样就保证了 training 和 inference 阶段的 proposals 分布大致相同，解决 mismatch 问题。</p><h3 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h3><p>在目标检测中，IoU 阈值用于定义正例和反例，一个使用低阈值的检测器通常会产生不太好的结果(误检过多)，但是增加 IoU 阈值也会使得检测精度下降，作者认为主要有 2 个原因：① 在训练过程中由于增大 IoU 阈值导致的正例数量呈指数形式减少，会导致对负例过拟合。② 推理阶段的 proposals 存在 mismatch 的问题(分布不一致)。</p><p>下图给出了随着 IoU 阈值的增大导致的检测结果改变以及回归精度和检测精度的变化曲线：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/results_with_diff_iou.png"></p><ul><li>Figure1(a) 展示了当 IoU 阈值等于 0.5 时的检测结果，可以看到结果图中存在较多误检，因为的 0.5 阈值会使得正样本中有较多的背景，这是产生误检的主要原因。</li><li>Figure1(b) 展示了当 IoU 阈值等于 0.7 时的检测结果，可以看到误检变少了，但是并不一定误检变少了结果就一定更好，因为 IoU 阈值更大导致正样本越少，那么过拟合的风险就越大。</li><li>Figure1(c) 展示了模型定位表现，其中横坐标表示 RPN 得到的 proposals 和 GT 框的 IoU 值，纵坐标表示经过 bbox reg 之后的 proposals 和 GT 框的 IoU 值。然后，红，绿，蓝三条曲线分别代表训练检测模型时用的正负样本标签的阈值分别是0.5，0.6，0.7。可以<strong>从图中看到当 proposal 和 GT 的 IoU 比较小的时候，更小 IoU 阈值能得到更好的结果，当 proposal 和 GT 的 IoU 比较大的时候，更大的 IoU 阈值能得到更好的结果，也就是说单一的 IoU 阈值训练的结果非常有限，不能对所有的 proposals 都有很好的效果。所以后面的 cascade 结构会把 IoU 阈值设置的越来越大，因为每经过一次级联结构，proposal 和 GT 的 IoU 也会越大，所以需要更大的 IoU 阈值。</strong></li><li>Figure1(d) 横轴表示在 Training 阶段判定一个 box 是否为 TP 的 IoU 阈值，纵轴为 Inference 阶段的 mAP，可以看出当 IoU 阈值从 0.5 上升到 0.7 的时候效果下降比较明显，原因是 IoU 阈值过高导致正样本数量更少，这样样本会更加不平衡并且容易导致过拟合。</li></ul><h3 id="Mismatch-问题"><a href="#Mismatch-问题" class="headerlink" title="Mismatch 问题"></a>Mismatch 问题</h3><p>这里详细说下第二点，即训练阶段和推理阶段的 proposals 存在不匹配的问题，先看下 Faster R-CNN 的结构。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn_two_stage.jpg"></p><p>如上图所示，整个网络其实可以分为 2 个阶段，训练阶段和推理阶段。</p><ol><li><p>Training 阶段<br>RPN 网络提出了 2000 左右的 proposals，这些 proposals 被送入到 Faster R-CNN 结构中，在 Faster R-CNN 结构中，首先计算每个 proposal 和 gt 之间的 IoU，通过人为的设定一个 IoU 阈值（通常为0.5），把这些 Proposals 分为正样本（前景）和负样本（背景），并对这些正负样本采样，使得他们之间的比例尽量满足（1:3，二者总数量通常为128），之后这些 proposals（128个）被送入到 Roi Pooling，最后进行类别分类和 box 回归。</p></li><li><p>Inference 阶段<br>RPN 网络提出了 300 左右的 proposals，这些 proposals 被送入到 Faster R-CNN 结构中，<strong>和 training 阶段不同的是，inference 阶段没有办法对这些 proposals 采样（inference 阶段肯定不知道 GT 的，也就没法计算 IoU）</strong>，所以他们直接进入 Roi Pooling，之后进行类别分类和 bbox 回归。</p></li></ol><p><strong>可以明显的看到 training 阶段和 inference 阶段，bbox 回归器的输入分布是不一样的，这就是论文中提到 mismatch 问题</strong>，training 阶段的输入 proposals 质量更高(被采样过，IoU &gt; threshold )，inference 阶段的输入 proposals 质量相对较差（没有被采样过，可能包括很多 IoU &lt; threshold 的），这个问题是固有存在的，通常 threshold 取 0.5 时，mismatch 问题还不会很严重。</p><h3 id="Cascade-结构"><a href="#Cascade-结构" class="headerlink" title="Cascade 结构"></a>Cascade 结构</h3><p>前面已经说明了<code>单一的 IoU 阈值训练的结果非常有限，不能对所有的 proposals 都有很好的效果</code>，所以作者提出了 multi-stage 的结构，每个 stage 都有不同的 IoU 阈值。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/cascade_structure.png"></p><p>可以看到不止作者一个人想到了这种结构，作者讨论了 cascade 结构和另外 2 种结构的不同之处以及 cascade 结构为什么更优秀的原因。</p><p>为什么 Cascade R-CNN 要这样来设计呢？这和上面的出发点密切相关，从Figure1(c)中我们看出使用不同的 IoU 阈值训练得到的检测模型对有不同 IoU 阈值的输入候选框的结果差别较大，因此我们希望训练每个检测模型用的 IoU 阈值要尽可能和输入候选框的 IoU 接近。并且从 Figure1(c) 中可以看出三条彩色曲线都在灰度曲线上方，这说明对于这几个阈值来说，输出的 IoU 阈值都大于输入的 IoU 阈值。根据这一特点，我们就可以拿上一个阶段的输出作为下一个阶段的输入，这样就可以得到越来越高的 IoU。</p><p>总结就是我们很难让一个在指定 IoU 阈值界定的训练集上训练得到的检测模型对 IoU 跨度较大的输入候选框都达到最佳，因此采取级联的方式能够让每一个阶段的检测器都专注于检测 IoU 在某一范围内的候选框，因为输出 IoU 普遍大于输入 IoU，因此检测效果会越来越好。</p><h4 id="和-Iterative-BBox-比较"><a href="#和-Iterative-BBox-比较" class="headerlink" title="和 Iterative BBox 比较"></a>和 Iterative BBox 比较</h4><p>Iterative BBox 的 H 位置都是共享的，而且 3 个分支的 IoU 阈值都取 0.5。Iterative BBox 存在的问题：</p><ul><li>三个分支的 IoU 阈值都是 0.5，单一阈值无法对所有 proposals 取得良好效果</li><li>detector 会改变样本的分布，这时候每个分支使用一个共享的头 H(head) 对最终检测结果有影响，作者做了下面的实验证明样本分布在各个 stage 的变化。第一行是 x, y 的变化图，第二行是 w, h 的变化图。</li></ul><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/sample_distribution_in_diff_stage.jpg"></p><p>可以看到每经过一次回归，样本点都会更加靠近 GT 一点，质量也就更高一点，样本的分布也在逐渐发生变化，如果仍然使用 0.5 的阈值，在后面 2 个 stage 就会有较多的离群点，</p><h4 id="和-Integral-Loss-比较"><a href="#和-Integral-Loss-比较" class="headerlink" title="和 Integral Loss 比较"></a>和 Integral Loss 比较</h4><p>Integral Loss 共用 pooling，只有一个 stage，但有 3 个不共享的 H，每个 H 处都对应不同的 IoU 阈值。Integral Loss 存在的问题：</p><ul><li>第一个 stage 的输入 IoU 的分布很不均匀，高阈值 proposals 数量很少，导致负责高阈值的 detector 很容易过拟合。</li><li>在 inference 时，3 个 detector 的结果要进行 ensemble，但是它们的输入的 IoU 大部分都比较低，这时高阈值的 detector 也需要处理低 IoU 的 proposals，它就存在较严重的 mismatch 问题，它的 detector 效果就很差了。</li></ul><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/iou_histogram_of_training_sample.png"></p><p>由上图可以看到每经过一个 stage，即每经过一个检测器进行坐标回归，候选框就越准确，候选框和 GT 的 IoU 也就如图所示的越来越大。这就说明了 Cascade R-CNN 是可行的，因为它不会因为后面的检测器提高阈值导致正样本过少而过拟合。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>RPN 提出的 proposals 大部分质量不高，导致没办法直接使用高阈值的 detector，Cascade R-CNN 使用 cascade 回归作为一种重采样的机制，逐 stage 提高 proposal 和 GT 的 IoU 值，从而使得前一个 stage 重新采样过的 proposals 能够适应下一个有更高阈值的 stage。</p><ul><li>每一个 stage 的 detector 都不会过拟合，都有足够满足阈值条件的样本。</li><li>更深层的 detector 也就可以优化更大阈值的 proposals。</li><li>每个 stage 的 H 不相同，意味着可以适应多级的分布。</li><li>在 inference 时，虽然最开始 RPN 提出的 proposals 质量依然不高，但在每经过一个 stage 后质量都会提高，从而和有更高 IoU 阈值的 detector 之间不会有很严重的 mismatch。</li></ul><!-- ## TridentNetTODO -->]]></content>
      
      
      <categories>
          
          <category> Object Detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Two-Stage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>One-Stage Object Detection</title>
      <link href="2020/06/20/One-Stage-Object-Detection/"/>
      <url>2020/06/20/One-Stage-Object-Detection/</url>
      
        <content type="html"><![CDATA[<h1 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h1><p><a href="https://blog.csdn.net/c20081052/article/details/80236015">CSDN的这篇文章写的非常详细, 可以瞻仰下</a></p><h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><p>One-Stage 算法的开山之作, 直接在输出层回归 bounding box 的位置以及所属类别。</p><h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>回顾 YOLO 之前的目标检测算法, 都是基于产生大量可能包含物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体, 以及物体所属类别的概率或者置信度, 同时需要后处理修正边界框, 最后基于一些准则过滤掉置信度不高和重叠度较高的边界框, 进而得到检测结果。这种基于先产生候选区域再进行检测的方法虽然有较高的精度, 但速度非常慢。YOLO 直接将目标检测堪称一个回归问题进行处理, 将候选区和检测两个阶段合二为一。 YOLO 的检测过程如下所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov1_system.png"></p><p>可以看到 YOLO v1 用于检测只有非常简单的三个步骤:</p><ol><li>resize 图像</li><li>CNN 提取特征并预测</li><li>利用 NMS 过滤 bbox</li></ol><h2 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h2><p>最终会得到一个 $S \times S$ 的特征图, <strong>如果某个 object 的中心落在这个网格中, 则这个网格就负责预测这个 object</strong>。每个 grid Cell 预测 $B$ 个 bounding box 的 4 个坐标以及每个 bounding box 的 confidence, 这里的 confidence 表示一个 bounding box 是否含有目标以及含有目标的准确度, 如果一个 bounding box 没有目标, 那么它的 confidence 应该为 0, 否则它的 confidence 应该为 bounding box 和 Ground Truth 的 IoU大小, 即定义为以下公式：<br>$$<br>confidence = Pr(Object) \cdot IoU_{pred}^{trutth}<br>$$</p><p>坐标 x, y 代表了预测的 bounding box 的中心与栅格边界的相对值(offset)。坐标 w, h 代表了预测的 bounding box 的 width、height 相对于整幅图像 width, height 的比例。每一个栅格还要预测 C 个 conditional class probability（条件类别概率）：$Pr(Class_i | Object)$。即在一个栅格包含一个 Object 的前提下, 它属于某个类的概率。我们只为每个栅格预测一组（C个）类概率, 而不考虑框 B 的数量。即不管一个 grid 预测多少个 bounding box, 每个 bounding box 的类别就是那个 grid 预测的类别。也就是说 class 信息是针对每个网格的, confidence 信息是针对每个 bounding box 的, 如图所示：</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolo_model.png"></p><h2 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h2><p>在 test 的时候, 将每个 bounding box 的 confidence 和每个 grid 预测的条件概率相乘得到每个 bounding box 的 class-specific confidence score：<br>$$<br>Pr(Class_{i} | Object) \cdot Pr(Object) *  IoU_{pred}^{trutth} = Pr(Class_{i}) \cdot IoU_{pred}^{trutth}<br>$$</p><p>得到每个 bbox 的 class-specific confidence score 以后, 设置阈值, 滤掉得分低的 boxes, 对保留的 boxes 进行 NMS 处理, 就得到最终的检测结果。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>这里有 3 个损失, 分别是坐标误差, 置信度误差以及分类误差。</p><p>这里简单的使用平方误差和 (sum-squared error loss) 来优化模型, 因为平方误差和容易优化, 但是这样会有 2 个不足之处:</p><ol><li>简单地将 localization error 和 classification error 设为同等权重是明显不合理的</li><li>有很多的 grid Cells 并不会含有 object, 这就导致这些 Cells 的 预测的 bounding box 的 confidence 为 0, 相比于较少的有object的栅格, 这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献, 这会导致网络不稳定甚至发散。</li></ol><p>为了解决上面的问题作者设置了 2 个参数, 分别用于增大 bounding box 回归的损失权重以及减少那些不含 object 的 bounding box 的分类损失权重, 在原文中分别设置为 $\lambda_{coord} = 5$ and  $\lambda_{noobj} = 0.5$</p><p>在对不同大小的 bbox 预测中, 相比于大 bbox 预测偏一点, 小 bbox 预测偏一点肯定更不能被忍受的。而 sum-square error loss 中对同样的偏移 loss 是一样。为了解决这个问题, 在计算 loss 时使用 bbo x的 width 和 height 取平方根代替原本的 height 和 width, 原因是小 bbox 的横轴值较小, 发生偏移时, 反应到 y 轴上相比大 box 要大。</p><p>所以最终的损失函数如下所示：</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov1_loss_function.png"></p><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Copy LaTex formula</span></div>    <div class="hide-content"><p>$$<br>\begin{aligned}<br>{\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left[\left(x</em>{i}-\hat{x}<em>{i}\right)^{2}+\left(y</em>{i}-\hat{y}<em>{i}\right)^{2}\right]} \<br>{+\lambda</em>{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left[(\sqrt{w</em>{i}}-\sqrt{\hat{w}<em>{i}})^{2}+(\sqrt{h</em>{i}}-\sqrt{\hat{h}<em>{i}})^{2}\right]} \<br>+\sum</em>{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\lambda</em>{\text {noobj }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {noobj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\sum</em>{i=0}^{S^{2}} \mathbb{1}<em>{i}^{\text {obj }} \sum</em>{c \in \text { classes }}\left(p_{i}(c)-\hat{p}_{i}(c)\right)^{2}<br>\end{aligned}<br>$$</p></div></div><p>在这个损失函数中：</p><ol><li>只有当某个网格中有 object 的时候才对 classification error 进行惩罚。</li><li>只有当某个 bbox predictor 对某个 ground truth box 负责的时候, 才会对 bbox 的 coordinate error 进行惩罚, 而对哪个 ground truth bbox 负责就看其预测值和 ground truth bbox 的 IoU 是不是在那个 Cell 的所有 box 中最大。</li></ol><p>总的来说就是与 ground truth 匹配的先验框计算坐标误差、置信度误差（此时 target 为 1）以及分类误差, 而其它的边界框只计算置信度误差（此时 target 为 0）。</p><h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><ol><li>每个 grid Cell 只预测一个 类别的 Bounding Boxes, 而且最后只取置信度最大的那个 Box。这就导致如果多个不同物体(或者同类物体的不同实体)的中心落在同一个网格中, 会造成漏检。</li><li>预测的 bbox 对于尺度的变化比较敏感, 在尺度上的泛化能力比较差。</li><li>相对 Two-Stage 算法而言检测精度低。</li><li>Recall 很低。</li></ol><h1 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h1><h2 id="贡献-1"><a href="#贡献-1" class="headerlink" title="贡献"></a>贡献</h2><ol><li>通过在特征图上使用 $3 \times 3$ 的小卷积来直接预测类别以及边界框的偏移值 (YOLO v1 和 Faster R-CNN 的 FPN 中都是预测是否含有物体, 而不是直接预测类别)。</li><li>在不同尺度的特征图上使用 $3 \times 3$ 的小卷积预测, 提高了小目标的检测精度, 一定程度上解决了多尺度的问题。</li></ol><h2 id="多尺度特征图检测"><a href="#多尺度特征图检测" class="headerlink" title="多尺度特征图检测"></a>多尺度特征图检测</h2><p>在 YOLO v1 中只在最后的那个 $7 \times 7$ 特征图来进行检测(生成 anchor), 在 SSD 中是在 backbone 中多个不同尺度的特征图上都进行检测。如图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/ssd_and_yolov1.png"></p><p>原文对于 300x300 的输入, 分别在 conv4_3,  conv7, conv8_2, conv9_2, conv10_2, conv11_2 的特征图上的每个单元取 4, 6, 6, 6, 4, 4 个 default box。</p><h2 id="卷积检测器"><a href="#卷积检测器" class="headerlink" title="卷积检测器"></a>卷积检测器</h2><p>由上图可以看到, 对于 backbone 中每个用于检测的特征图, 都使用一个 $3 \times 3 \times (k \times (classes + 4))$ 的卷积核来进行, 其中 $k$ 是每个特征图上的 grid 生成 anchor 的数目。这里为每个 anchor 都预测了其类别和坐标的 offsets。</p><h2 id="Default-boxes-and-aspect-ratio"><a href="#Default-boxes-and-aspect-ratio" class="headerlink" title="Default boxes and aspect ratio"></a>Default boxes and aspect ratio</h2><p>这里其实就是相当于预设好的 anchors, 和 Faster R-CNN 中一致, 只是在多个特征层上都提生成了 anchor 而已, 一般宽高比在{ 1, 2, 3, 1/2, 1/3} 中选取, 以上图为例, 由于以上特征图的大小分别是 38x38, 19x19,10x10, 5x5, 3x3, 1x1, 所以一共得到 38 x 38 x 4 + 19 x 19 x 6 + 10 x 10 x 6 + 5 x 5 x 6 +  3 x 3 x 4 + 1 x 1 x 4 = 8732 个 default box。即对一张 300 x 300 的图片输入网络将会针对这 8732 个 default box 预测 8732 个边界框。</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h3 id="匹配策略"><a href="#匹配策略" class="headerlink" title="匹配策略"></a>匹配策略</h3><p>在训练过程中需要决定哪个 default box(anchor) 和 Ground Truth 对应。对于所有的 anchors 而言(不限位置、纵横比还是尺度), 只要其和 Ground Truth 的 jaccard overlap &gt; 0.5, 作者就将这个 anchor 和 Ground Truth 对应, 这样简化了学习问题, 这样网络可以对多个重叠的 anchor 都预测高得分而不是只对重叠度最高的预测得分。</p><h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>设定 $x^{p}<em>{ij} = \lbrace 1, 0 \rbrace$ 表示第 $i$ 个 anchor 是否和第 $j$ 个 Ground Truth 在类别 $p$ 下对应。对于一张图片, 至少有一个 anchor 会和 Ground Truth 对应, 所以有 $\sum</em>{i} x_{i j}^{p} \geq 1$, 这样总的 loss function 就是 localization loss 和 confidence loss 的加权和(交叉验证时 $\alpha=1$)：</p><p>$$<br>L(x, c, l, g)=\frac{1}{N}\left(L_{c o n f}(x, c)+\alpha L_{l o c}(x, l, g)\right)<br>$$</p><p>其中 $N$ 是和 Ground Truth 匹配的 anchors 的数目, 如果 $N=0$ 那么 loss = 0, localization loss 是 anchors 预测的 offsets 和实际的 offsets 之间的 Smooth L1 Loss：<br>$$<br>\begin{array}{rl}<br>L_{l o c}(x, l, g)=\sum_{i \in \text {Pos }}^{N} \sum_{m\in{c x, c y, w, h}} &amp; x_{i j}^{k} \operatorname{smooth}<em>{\mathrm{L} 1}\left(l</em>{i}^{m}-\hat{g}<em>{j}^{m}\right) \<br>\hat{g}</em>{j}^{c x}=\left(g_{j}^{c x}-d_{i}^{c x}\right) / d_{i}^{w} &amp; \hat{g}<em>{j}^{c y}=\left(g</em>{j}^{c y}-d_{i}^{c y}\right) / d_{i}^{h} \<br>\hat{g}<em>{j}^{w}=\log \left(\frac{g</em>{j}^{w}}{d_{i}^{w}}\right) &amp; \hat{g}<em>{j}^{h}=\log \left(\frac{g</em>{j}^{h}}{d_{i}^{h}}\right)<br>\end{array}<br>$$</p><p>confidence loss 是一个多分类的 softmax loss：<br>$$<br>L_{c o n f}(x, c)=-\sum_{i \in P o s}^{N} x_{i j}^{p} \log \left(\hat{c}<em>{i}^{p}\right)-\sum</em>{i \in N e g} \log \left(\hat{c}<em>{i}^{0}\right) \quad \text { where } \quad \hat{c}</em>{i}^{p}=\frac{\exp \left(c_{i}^{p}\right)}{\sum_{p} \exp \left(c_{i}^{p}\right)}<br>$$</p><h3 id="设置-scales-和-ratios"><a href="#设置-scales-和-ratios" class="headerlink" title="设置 scales 和 ratios"></a>设置 scales 和 ratios</h3><p>网络中不同层级的特征图有着不同的感受野, 在 SSD 中设定的 anchors 不一定要和每层中的实际感受野对应, 假设一共使用 $m$ 个特征图来做预测, 那么每个特征图的 anchor 的 scale 可以定义为：<br>$$<br>s_{k}=s_{\min }+\frac{s_{\max }-s_{\min }}{m-1}(k-1), \quad k \in[1, m]<br>$$</p><p>原文中 $s_{max} = 0.9, s_{min} = 0.2$, 意味着最低层的尺度是 0.2, 最高层的尺度为 0.9, 对于 anchor, 使用 5 种不同的宽高比 $\alpha_r = \lbrace 1, 2, 3, \frac{1}{2}, \frac{1}{3} \rbrace$, 这样就能计算出每个 anchor 的宽和高：<br>$$<br>\begin{array}{rl}<br>w_{k}^{a}=s_{k} \sqrt{a}<em>{r} \<br>h</em>{k}^{a}=s_{k} \sqrt{a}_{r}<br>\end{array}<br>$$</p><p>对于宽高比为 1 的 anchors, 额外增加一个 scale 为 $s’_k = \sqrt{s_k s_{k+1}}$ 的 anchor, 这样一共就能得到 6 个 anchors, 每个 anchor 的中心坐标为 $(\frac{i+0.5}{|f_k|}, \frac{j+0.5}{|f_k|})$ 其中 $f_k$ 是第 $k$ 个特征图的大小, 例如 $k = 2$ 时 $|f_k| = 4$, 这样就得到了所有的 default boxes 即 anchors。</p><h3 id="Hard-negative-mining"><a href="#Hard-negative-mining" class="headerlink" title="Hard negative mining"></a>Hard negative mining</h3><p>匹配步骤后, 默认框中的大多数都是 negative, 尤其是候选框个数众多的时候。这就导致 positive 和 negative 训练样本不平衡。这些 negative 样本不全用, 而是对于每一个默认框, 通过最高置信度损失来对它们进行排序, 选择最高的几个, 这样 negative 和 positive 的比例最多是 3:1。这样训练更稳定也更快。</p><h1 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h1><h2 id="贡献-2"><a href="#贡献-2" class="headerlink" title="贡献"></a>贡献</h2><p>加了很多的改进方法使得 mAP 从 63.4 提高到了 78.6。</p><h2 id="改进方法"><a href="#改进方法" class="headerlink" title="改进方法"></a>改进方法</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov2_tricks.png"></p><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>在所有的建基层后面加上 BN 后在 mAP 上提高了 2 个点, 同时由于 BN 能够正则化模型, 避免过拟合, 所以作者把 YOLO v1 中的 Dropout 去掉了。</p><h3 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h3><p>几乎所有的目标检测方法都使用在 ImageNet 上预训练的 backbone, 而训练图像的大小肯定会影响最终结果, YOLO v1 是在 $224 \times 224$ 上训练的, 对于 YOLO v2 而言, 先在 ImageNet 上用 $448 \times 448$ 的分辨率对 classification network 做 10 个 epoch 的 fine tune, 再用 $448 \times 448$ 的图片来训练, 这使得最终 mAP 提高了 4 个点。</p><h3 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes."></a>Convolutional With Anchor Boxes.</h3><ol><li>在 YOLO v1 中直接使用 FC 层来预测 bounding box 的坐标, 这回导致丢失一些空间信息, 在 YOLO v2 里面直接删除了 FC 层, 并且使用 anchor 机制来预测 bounding box。</li><li>为了得到更高分辨率的特征图, 去掉了 YOLO v1 中的一个池化层</li><li>把 $448 \times 448$ 的分辨率 shrink 到 $416 \times 416$, 这样经过下采样之后的特征图 size 就会是一个奇数, 由于图片中的物体都倾向于出现在图片的中心位置, 若特征图恰好有一个中心位置, 利用这个中心位置预测中心点落入该位置的物体会对这些物体的检测会更容易。</li></ol><p>这样对于 $416 \times 416$ 的输入就能得到 $13 \times 13$ 的特征图, 特征图上每个 grid 预测 5 个 anchor, 虽然使用 anchor 机制让 mAP 从 69.5 下降到了 69.2, 但这让 Recall 从 81% 提高到了 87%。</p><h3 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h3><p>此前 anchor 的 size 都是动手设置好的, 可以把 anchor 看成是一个先验 prior, 虽然网络可以通过学习参数来对最终的 bounding box 坐标进行调整, 但如果一开始就给模型一个更好的 prior, 即更好的 anchor, 网络就能更容易训练也能获得更好的效果。</p><p>这里使用了 k-means 聚类算法来对所有的 bounding box 进行聚类, 但问题是如果使用标准的欧式距离的话, 更大的 bounding box 会产生更大的 loss, 我们当然希望 loss 值和 bounding box 的 size 没有关系, 所以利用 IoU 定义了以下的距离函数, 使得误差和 bounding box 的大小无关：<br>$$<br>d(box, centroid) = 1 - IoU(box, centroid)<br>$$</p><h3 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a>Direct location prediction</h3><p>作者在引入 anchor 机制后发现模型在早期的训练是很不稳定的, 主要原因在于 bounding box 的 x 和 y 坐标, 通常 anchor 机制中预测的是 offset, 即预测下式中的 $t_x$ 和 $t_y$：</p><p>$$<br>\begin{array}{l}<br>x=\left(t_{x} * w_{a}\right)-x_{a} \<br>y=\left(t_{y} * h_{a}\right)-y_{a}<br>\end{array}<br>$$</p><p>其中 $(x_a, y_a, w_a, h_a)$ 分别是 anchor 的 4 个坐标。可以看到这个公式没有任何的限制, 所以任何一个 anchor 都可以在图像中的任意一点结束, 这也就导致了模型在随机初始化的情况下, 需要花费很长一段时间来预测合理的 offset。</p><p>所以作者继续沿用了 YOLO v1 中直接预测相对于 grid Cell 的坐标位置的方式来得到每个 anchor 的坐标, 对于最终的特征图, 每个 grid Cell 预测 5 个 bounding box, 每个 bounding box 预测 5 个参数, 分别是 $t_x, t_y, t_w, t_h, t_o$, 如果这个 Cell 的左上角点坐标为 $(c_x, c_y)$, 且这个 bounding box 的先验为 $p_w, p_h$, 那么最终得到的 bounding box 为：</p><p>$$<br>\begin{aligned}<br>b_{x} &amp;=\sigma\left(t_{x}\right)+c_{x} \<br>b_{y} &amp;=\sigma\left(t_{y}\right)+c_{y} \<br>b_{w} &amp;=p_{w} e^{t_{w}} \<br>b_{h} &amp;=p_{h} e^{t_{h}} \<br>\operatorname{Pr}(\text { object }) * I O U(b, \text { object }) &amp;=\sigma\left(t_{o}\right)<br>\end{aligned}<br>$$</p><p>其中 $t_x$ 和 $t_y$ 都经过 $sigmoid$ 函数处理为 [0, 1] 之间的数, 这样归一化处理能使模型训练更加稳定, </p><p>可以根据下图来理解：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov2_coordinate_predict.png"></p><p>蓝色框为最终预测的 bounding box, 黑色的虚线为先验框, $c_x$ 和 $c_y$ 表示 grid Cell 和图像左上角点的横纵坐标距离,  $p_w$ 和 $p_h$ 是先验框的 size。</p><h3 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h3><p>Faster R-CNN(RPN) 和 SSD 都在多个特征图上提取 proposals 以得到不同分辨率, 而作者使用一个 passthrough 层来做这件事, passthrough 层抽取原特征图每个 2 x 2 的局部区域组成新的 channel, 即原特征图大小降低 4 倍, channel 增加 4 倍, 这使得最终 mAP 上升了 1 个点。</p><h3 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h3><p>由于 YOLO v2 中只有卷积层和池化层, 所以可以处理任意大小的图片, 为了模型能够在不同的分辨率图像上都取得好结果, 作者采用了多尺度训练的方式, 在训练过程中每隔 10 个 batches 重新选择输入图片的尺寸, 由于 DarkNet-19 的下采样总步长为 32, 所以输入图片的尺寸一般为 32 的倍数{320,352, …。608}, 这种机制使得网络可以更好地预测不同尺寸的图片, 意味着同一个网络可以进行不同分辨率的检测任务</p><h2 id="训练的-3-个阶段"><a href="#训练的-3-个阶段" class="headerlink" title="训练的 3 个阶段"></a>训练的 3 个阶段</h2><p>YOLO v2 的训练主要包括三个阶段。第一阶段就是先在 ImageNet 分类数据集上预训练 Darknet-19, 此时模型输入为 $224 \times 224$ , 共训练 160 个 epochs。然后第二阶段将网络的输入调整为 $448 \times 448$ , 继续在 ImageNet 数据集上 finetune 分类模型, 训练 10 个 epochs, 此时分类模型的 top-1 准确度为 76.5%, 而 top-5 准确度为 93.3%。第三个阶段就是修改 Darknet-19 分类模型为检测模型, 并在检测数据集上继续 finetune 检测网络。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov2_training_stage.jpg"></p><h2 id="损失函数-2"><a href="#损失函数-2" class="headerlink" title="损失函数"></a>损失函数</h2><p>首先回顾下 YOLO v1 的损失函数：</p><p>$$<br>\begin{aligned}<br>{\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left[\left(x</em>{i}-\hat{x}<em>{i}\right)^{2}+\left(y</em>{i}-\hat{y}<em>{i}\right)^{2}\right]} \<br>{+\lambda</em>{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left[(\sqrt{w</em>{i}}-\sqrt{\hat{w}<em>{i}})^{2}+(\sqrt{h</em>{i}}-\sqrt{\hat{h}<em>{i}})^{2}\right]} \<br>+\sum</em>{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\lambda</em>{\text {noobj }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {noobj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\sum</em>{i=0}^{S^{2}} \mathbb{1}<em>{i}^{\text {obj }} \sum</em>{c \in \text { classes }}\left(p_{i}(c)-\hat{p}_{i}(c)\right)^{2}<br>\end{aligned}<br>$$</p><p>第一行和第二行是坐标误差, 第三行和第四行是置信度误差, 第五行是分类误差, 第二行之所以用 $\sqrt{w}, \sqrt{h}$ 来代替 $w, h$ 是因为更小的 bbox 在偏移相同的尺寸下对 IoU 影响更大。</p><p>YOLO v2 的损失函数和 YOLO v1 其实相差并不大：<br>$$<br>\begin{aligned}<br>{\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left[\left(x</em>{i}-\hat{x}<em>{i}\right)^{2}+\left(y</em>{i}-\hat{y}<em>{i}\right)^{2}\right]} \<br>{+\lambda</em>{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left(2-w_i * h_i)[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2+(w_i-\hat{w}_i)^2+(h_i-\hat{h}_i)^2\right]} \<br>+\sum</em>{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\lambda</em>{\text {noobj }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {noobj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\sum</em>{i=0}^{S^{2}} \mathbb{1}<em>{i}^{\text {obj }} \sum</em>{c \in \text { classes }}\left(p_{i}(c)-\hat{p}<em>{i}(c)\right)^{2} \<br>+0.01 *  \sum</em>{i=0}^{S^{2}} \sum_{j=0}^{B} 1^{noobj}_{ij}[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2+(w_i-\hat{w}_i)^2+(h_i-\hat{h}_i)^2]<br>\end{aligned}<br>$$</p><p>可以看到和 YOLO v1 的损失函数相比, 第 2 项改变了, 多了第 6 项, 其中第 2 项的作用和 YOLO v1 中第 2 项作用一致, 只是实现思路不同, 都是为了解决更小的 bbox 在偏移相同的尺寸下对 IoU 影响更大的问题。第 6 项只在训练初期使用, 只在 12800 样本之前计算未预测到 target (不含 object) 的 anchors 的坐标损失。</p><h1 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h1><p>YOLO v3 相比于 YOLO v2 来说可以简单理解为加入了残差结构和 FPN 架构。</p><h2 id="引入残差结构"><a href="#引入残差结构" class="headerlink" title="引入残差结构"></a>引入残差结构</h2><p>主要是把 backbone 换成了 DarkNet-53, 网络结构如下所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/darknet53.png"></p><h2 id="FPN-结构的引入"><a href="#FPN-结构的引入" class="headerlink" title="FPN 结构的引入"></a>FPN 结构的引入</h2><p>这里和 FPN 还是有点区别的, 详细可以看以下图解：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov3.jpg"></p><p>先把最后一层的 feature map (上图中通道数为 1024)做上采样并利用卷积减少通道数, 再和前一层的 feature map 直接 concatenate 起来(在 FPN 中特征融合的方式是 element-wise add)。对 concatenate 在一起的特征图在经过 CNN 分别得到下一层需要的特征图(上图中的 Conv2D Block 5L 256 ) 以及最终用于生成 anchor 的最终 feature (上图中第 2 个红色的)。</p><p>这里之所以出现了 $3 \times 3$ 的卷积是为了能更好的进行分类和边界框回归, 因为预训练好的网络得到的特征图往往是有着较强的分类特征, 使用 $3 \times 3$ 卷积对其进行二次变换能够将分类特征迁移到 分类和回归 这2个 task 所需要的分布上来。这个最早在 Faster R-CNN 中就已经有体现。</p><h2 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a>Anchor</h2><p>从上面的 YOLO v3 结构图可以看到一共有 3 个输出(红色的特征图), 其维度分别是 $(batchsize, 52, 52, 75)$、$(batchsize, 26, 26, 75)$ 和 $(batchsize, 13, 13, 75)$, 在这三个输出的特征图上设置 anchor, 原文中使用 k-means 聚类后一共得到 9 个 anchor, 把这 9 个 anchor 平均分到这 3 个不同 scale 的特征图上即可, 例如在 COCO 上聚类得到的 9 个 anchor size 分别为 (10×13,  (16×30), (33×23),  (30×61),  (62×45),  (59×119), (116 × 90), (156 × 198), (373 × 326)。</p><p>这样前 3 个 anchor size 在  $(batchsize, 52, 52, 75)$ 的特征图上生成, 即在  $(batchsize, 52, 52, 75)$ 这个特征图上一共生成 $52 \times 52 \times 3 = 8112$ 个 anchor, 所有 anchor 的 size 都是 (10×13);, (16×30),  (33×23) 中的一个, 以此类推, $(batchsize, 26, 26, 75)$ 特征图用于生成 (30×61),  (62×45),  (59×119) 尺寸的 anchors, $(batchsize, 13, 13, 75)$ 特征图用于生成 (116 × 90), (156 × 198), (373 × 326) 大小的 anchors。</p><h1 id="YOLO-v4"><a href="#YOLO-v4" class="headerlink" title="YOLO v4"></a>YOLO v4</h1><p>TODO</p><h1 id="RefineDet"><a href="#RefineDet" class="headerlink" title="RefineDet"></a>RefineDet</h1><p>TODO</p><h1 id="Gaussian-YOLO-v3"><a href="#Gaussian-YOLO-v3" class="headerlink" title="Gaussian YOLO v3"></a>Gaussian YOLO v3</h1><p>TODO</p><h1 id="Guided-Anchoring"><a href="#Guided-Anchoring" class="headerlink" title="Guided Anchoring"></a>Guided Anchoring</h1><p>TODO</p><h1 id="M2Det"><a href="#M2Det" class="headerlink" title="M2Det"></a>M2Det</h1><p>TODO</p><h1 id="EfficientDet"><a href="#EfficientDet" class="headerlink" title="EfficientDet"></a>EfficientDet</h1><p>TODO</p>]]></content>
      
      
      <categories>
          
          <category> Object Detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> One-Stage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Object Detection Review</title>
      <link href="2020/06/20/Object-Detection-Review/"/>
      <url>2020/06/20/Object-Detection-Review/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是目标检测"><a href="#什么是目标检测" class="headerlink" title="什么是目标检测"></a>什么是目标检测</h2><p>计算机视觉领域有三个基本任务，分别是分类、检测和分割。如图所示。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Object-Detection-Review/classify_detection_segmentation.jpg"></p><ol><li>分类（Classification），即是将图像结构化为某一类别的信息，用事先确定好的类别(string)或实例ID来描述图片。这一任务是最简单、最基础的图像理解任务，也是深度学习模型最先取得突破和实现大规模应用的任务。其中，ImageNet是最权威的评测集，每年的ILSVRC催生了大量的优秀深度网络结构，为其他任务提供了基础。在应用领域，人脸、场景的识别等都可以归为分类任务。</li><li>检测（Detection）。分类任务关心整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标，要求同时获得这一目标的类别信息和位置信息。相比分类，检测给出的是对图片前景和背景的理解，我们需要从背景中分离出感兴趣的目标，并确定这一目标的描述（类别和位置），因而，检测模型的输出是一个列表，列表的每一项使用一个数据组给出检出目标的类别和位置（常用矩形检测框的坐标表示）。</li><li>分割（Segmentation）。分割包括语义分割（semantic segmentation）和实例分割（instance segmentation），前者是对前背景分离的拓展，要求分离开具有不同语义的图像部分，而后者是检测任务的拓展，要求描述出目标的轮廓（相比检测框更为精细）。分割是对图像的像素级描述，它赋予每个像素类别（实例）意义，适用于理解要求较高的场景，如无人驾驶中对道路和非道路的分割。</li></ol><h2 id="目标检测现有方法"><a href="#目标检测现有方法" class="headerlink" title="目标检测现有方法"></a>目标检测现有方法</h2><h3 id="深度目标检测方法"><a href="#深度目标检测方法" class="headerlink" title="深度目标检测方法"></a>深度目标检测方法</h3><h4 id="综述-博客内容"><a href="#综述-博客内容" class="headerlink" title="综述(博客内容)"></a>综述(博客内容)</h4><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Object-Detection-Review/review.png"></p><h4 id="分类概括"><a href="#分类概括" class="headerlink" title="分类概括"></a>分类概括</h4><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Object-Detection-Review/methods_category.jpg"></p><h4 id="关键技术发展"><a href="#关键技术发展" class="headerlink" title="关键技术发展"></a>关键技术发展</h4><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Object-Detection-Review/methods.png"></p><h3 id="传统目标检测"><a href="#传统目标检测" class="headerlink" title="传统目标检测"></a>传统目标检测</h3><p>传统检测方法一般分三个步骤：</p><ol><li>首先在给定图像上采用不同大小的滑动窗口(多尺度)对整幅图像进行遍历选择候选区域(ROI)，使用不同大小的滑动窗口框住待测图像中的某一部分作为候选区域，然后提取该候选区域相关的视觉特征。</li><li>特征提取，如人检测和普通目标检测常用的 HOG 和 SIFT 特征等，然后对这些区域提取特征。</li><li>分类器分类，即使用训练完成的分类器进行分类，如常用的SVM，最后使用分类器进行分类。</li></ol><p>传统方法的瓶颈在于：</p><ol><li>大量冗余的 proposal 生成，导致学习效率低下，容易在分类出现大量的假正样本。</li><li>特征描述子都是基于低级特征进行手工设计的，难以捕捉高级语义特征和复杂内容。</li><li>检测的每个步骤是独立的，缺乏一种全局的优化方案进行控制。</li></ol><h3 id="深度目标检测"><a href="#深度目标检测" class="headerlink" title="深度目标检测"></a>深度目标检测</h3><p>主要可以分为 One-Stage、Two-Stage 和 Anchor-Free 三类。</p><ol><li>Two-Stage：需要先提取 proposals/ROI，然后再对这些 ROI 进行分类和回归。</li><li>One-Stage：直接预测不同目标的类别与位置，不需要首先提取 ROI。</li><li>Anchor-Free：目前是预测图中每个物体的一个或多个关键点，利用这些关键点来预测最后的 Bounding Box。</li></ol><p>two-stage检测器包括两个步骤：</p><ol><li>使用 RPN 生成 anchor 并对 anchor 做筛选，过滤掉很多的负样本proposals，生成稀疏的proposals；</li><li>对得到的 proposals 进行分类和回归，对选择的 proposals，使用 roi pooling 等操作，进一步的精细化，因此得到的框更加精准。（比如，一个 anchor 有可能只覆盖了一个目标的50%，但却作为完全的正样本，因此其预测肯定是有误差的。）</li></ol><p>One-stage检测器步骤：<br>在指定特征图上，对每个位置，使用不同 scale、不同长宽比密集采样生成 anchor（没有对这些 anchor 进行筛选），直接进行分类和回归。主要优点是计算效率高，但是，检测精度通常落后于 Two-stage 方法。</p><p>【One-stage精度低】：<br>原因：<br>主要原因是类别不均衡问题（因为没有对负类的anchor进行删除）。<br>解决方案：<br>为了改善类别不均衡问题，RetinaNet 提出了 Focal Loss 重建标准交叉熵损失，降低 easy sample 的权重，增加 hard sample 的权重。</p><p>【One-stage对小物体检测不好（没有Two-stage好）】：<br>原因：<br>如果所有的 anchor 都没有覆盖到这个目标，那么这个目标就会漏检。如果一个比较大的 anchor 覆盖了这个目标，那么较大的感受野会弱化目标的真实特征，得分也不会高。two-stage 算法中的 roi pooling 会对目标做 resize, 小目标的特征被放大，其特征轮廓也更为清晰，因此检测也更为准确。</p><p>解决方案：</p><ol><li>最直接的提升就是增大input size（但是不能一味的增大，因为会让后面的特征量增大，时间增多，失去One-stage的速度优势）</li><li>借鉴FPN，把深层特征通过反卷积，然后通过skip pooling来结合底层的特征层。（结合的方式上有： concat, pixel-wise sum/ add等）（这样做的好处是，只用 SSD 的话虽然用了底层特征图，但是底层语义特征比较弱，处理小物体时效果表现的不好）</li><li>空洞卷积增加感受野（TridentNet）</li><li>attention机制（One-stage中目前用的好像比较少）</li></ol><h2 id="数据集和评价指标"><a href="#数据集和评价指标" class="headerlink" title="数据集和评价指标"></a>数据集和评价指标</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul><li><p><a href="http://host.robots.ox.ac.uk/pascal/VOC/">Pascal VOC</a></p><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看介绍</span></div>    <div class="hide-content"><p>VOC数据集是目标检测经常用的一个数据集，自2005年起每年举办一次比赛，最开始只有4类，到2007年扩充为20个类，共有两个常用的版本：2007和2012。学术界常用5k的train/val 2007和16k的train/val 2012作为训练集，test 2007作为测试集，用10k的train/val 2007+test 2007和16k的train/val 2012作为训练集，test2012作为测试集，分别汇报结果。</p></div></div></li><li><p><a href="http://cocodataset.org/">MS COCO</a></p><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看介绍</span></div>    <div class="hide-content"><p>COCO数据集是微软团队发布的一个可以用来图像recognition+segmentation+captioning的数据集，该数据集收集了大量包含常见物体的日常场景图片，并提供像素级的实例标注以更精确地评估检测和分割算法的效果，致力于推动场景理解的研究进展。依托这一数据集，每年举办一次比赛，现已涵盖检测、分割、关键点识别、注释等机器视觉的中心任务，是继ImageNet Chanllenge以来最有影响力的学术竞赛之一。相比ImageNet，COCO更加偏好目标与其场景共同出现的图片，即non-iconic images。这样的图片能够反映视觉上的语义，更符合图像理解的任务要求。而相对的iconic images则更适合浅语义的图像分类等任务。COCO的检测任务共含有80个类，在2014年发布的数据规模分train/val/test分别为80k/40k/40k，学术界较为通用的划分是使用train和35k的val子集作为训练集（trainval35k），使用剩余的val作为测试集（minival），同时向官方的evaluation server提交结果（test-dev）。除此之外，COCO官方也保留一部分test数据作为比赛的评测集。</p></div></div></li><li><p><a href="https://storage.googleapis.com/openimages/web/index.html">Google Open Image</a></p><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看介绍</span></div>    <div class="hide-content"><p>pen Image是谷歌团队发布的数据集。最新发布的Open Images V4包含190万图像、600个种类，1540万个bounding-box标注，是当前最大的带物体位置标注信息的数据集。这些边界框大部分都是由专业注释人员手动绘制的，确保了它们的准确性和一致性。另外，这些图像是非常多样化的，并且通常包含有多个对象的复杂场景（平均每个图像 8 个）。</p></div></div></li><li><p><a href="http://www.image-net.org/">ImageNet</a></p><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看介绍</span></div>    <div class="hide-content"><p>ImageNet是一个计算机视觉系统识别项目， 是目前世界上图像识别最大的数据库。ImageNet是美国斯坦福的计算机科学家，模拟人类的识别系统建立的。能够从图片识别物体。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。Imagenet数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注。</p></div></div></li><li><p><a href="https://captain-whu.github.io/DOTA/dataset.html">DOTA数据集</a></p><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看介绍</span></div>    <div class="hide-content"><p>DOTA是遥感航空图像检测的常用数据集，包含2806张航空图像，尺寸大约为4kx4k，包含15个类别共计188282个实例，其中14个主类，small vehicle 和 large vehicle都是vehicle的子类。其标注方式为四点确定的任意形状和方向的四边形。航空图像区别于传统数据集，有其自己的特点，如：尺度变化性更大；密集的小物体检测；检测目标的不确定性。数据划分为1/6验证集，1/3测试集，1/2训练集。目前发布了训练集和验证集，图像尺寸从800x800到4000x4000不等。</p></div></div></li></ul><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p>目标检测的主要评价指标就是 mAP (mean Average Precision), 即各类别 AP 的平均值</p><h4 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h4><p>交并比（IOU）是度量两个检测框（对于目标检测来说）的交叠程度，公式如下：<br>$$<br>\mathrm{IOU}=\frac{\operatorname{area}\left(B_{p} \cap B_{g t}\right)}{\operatorname{area}\left(B_{p} \cup B_{g t}\right)}<br>$$<br>其中 $B_{p}$ 是预测的检测框，而 $B_{g t}$ 是真实的检测框(Ground Truth)。</p><h4 id="TP、FP、FN、TN"><a href="#TP、FP、FN、TN" class="headerlink" title="TP、FP、FN、TN"></a>TP、FP、FN、TN</h4><ol><li>True Positive (TP): IoU&gt;Threshold ( 一般取 0.5 ) 的检测框数量（同一 Ground Truth 只计算一次）</li><li>False Positive (FP): IoU&lt;= 的检测框数量，<strong>或者是检测到同一个 GT 的多余检测框的数量</strong></li><li>False Negative (FN): 没有检测到的 GT 的数量</li><li>True Negative (TN): 在 mAP 评价指标中不会使用到</li></ol><h4 id="准确率-Precision-和召回率-Recall"><a href="#准确率-Precision-和召回率-Recall" class="headerlink" title="准确率(Precision)和召回率(Recall)"></a>准确率(Precision)和召回率(Recall)</h4><ol><li>$Precision = \frac{TP}{TP+FP}$</li><li>$Recall = \frac{TP}{TP+FN}$</li></ol><h4 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h4><p>由于 Precision 和 Recall 都是相对 Threshold 而改变的东西， 这是一场 Precision 与 Recall 之间的trade off， 用一组固定值表述不够全面， 因为我们根据不同的 threshold， 可以取到不同（也可能相同）的 Precision Recall 值。 这样想的话对于每个 threshold，我们都有 (Precision， Recall) 的 pair， 也就有了 Precision 和 Recall 之间的 curve 关系。</p><p><strong>对于每一个类</strong>, 每个预测框都有一个 score/confidence, 从大到小排序, 以此选择其 score 作为 threshold 来得到一组不同的 Precision 和 Recall, 这样如果一个类在所有图片中的预测框数量之和为 100 个, 那么就能得到 100 组 Precision 和 Recall, 根据这 100 组数据来画出 PR 曲线。</p><h4 id="AP-Average-Precision"><a href="#AP-Average-Precision" class="headerlink" title="AP(Average Precision)"></a>AP(Average Precision)</h4><p>对于一个类，其 AP 值为 PR 曲线下的面积，通常来说一个越好的分类器，AP 值越高。AP 可以充分的表示在这个模型中， Precision 和 Recall 的总体优劣。</p><h4 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h4><p>mAP 是多个类别 AP 的平均值。这个 mean 的意思是对每个类的 AP 值再求平均，得到的就是 mAP 的值，mAP 的大小一定在 [0,1] 区间，越大越好。该指标是目标检测算法中最重要的一个。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://www.zhihu.com/question/53405779/answer/429585383">知乎-陳子豪</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&mid=2247484121&idx=1&sn=d29803cf9d76f8ff3b781b1d898eb6f3&chksm=9f80b84fa8f731590e35ae64800866deba3a7383c7668f4b0f68078b6d5a2954e923b7b4f465&scene=21#wechat_redirect">微信公众号- GiantPandaCV</a></p>]]></content>
      
      
      <categories>
          
          <category> Object Detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lightweight Network</title>
      <link href="2020/06/16/Lightweight-Network/"/>
      <url>2020/06/16/Lightweight-Network/</url>
      
        <content type="html"><![CDATA[<h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a><a href="https://arxiv.org/abs/1602.07360">SqueezeNet</a></h2><p>核心思路就是先用 1x1 卷积降低通道数，再用 1x1 和 3x3 卷积得到新的通道数更多的新特征图， 每一个 Fire Module 的输入和输出的尺寸不变， 只是改变通道数。主要就是提出了 Fire module， 即 squeeze 层 + expand 层。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/squeeze_expand.png"></p><p>具体的操作如下</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/layers.png"></p><p>Fire module输入的feature map为$H<em>W</em>M$的，输出的feature map为$H<em>W</em>(e1+e3)$，可以看到feature map的分辨率是不变的，变的仅是维数，也就是通道数，这一点和VGG的思想一致。<br>首先，$H<em>W</em>M$的feature map经过Squeeze层，得到S1个feature map，这里的S1均是小于M的，以达到“压缩”的目的，详细思想可参考Google的Inception系列。<br>其次，$H<em>W</em>S1$的特征图输入到Expand层，分别经过1x1卷积层和3x3卷积层进行卷积，再将结果进行concat，得到Fire module的输出，为$H<em>W</em>(e1+e3)$的feature map。<br>fire模块有三个可调参数：S1，e1，e3，分别代表卷积核的个数，同时也表示对应输出feature map的维数，在原论文提出的SqueezeNet结构中，e1=e3=4s1 。<br>网络结构设计思想，同样与VGG的类似，堆叠的使用卷积操作，只不过这里堆叠的使用Fire module(图中用红框部分)</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/SqueezeNet.png"></p><h2 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a><a href="https://arxiv.org/abs/1707.01083">ShuffleNet</a></h2><p><strong>采用depth-wise convolution 会有一个问题，就是导致“信息流通不畅”，即输出的feature map仅包含输入的feature map的一部分，MobileNet采用了point-wise来解决问题， 而ShuffleNet通过将特征图的不同通道进行有序打乱来解决这个问题</strong></p><p>shuffle不是什么网络都需要用的，是有一个前提，就是采用了group convolution，才有可能需要shuffle！！ 为什么说是有可能呢？因为可以用point-wise convolution 来解决这个问题。</p><p>具体做法是把各组的channel平均分为g（下图g=3）份，然后依次序的重新构成feature map.</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/shuffle.png"></p><p>Channel shuffle 的操作非常简单，接下来看看ShuffleNet，ShuffleNet借鉴了Resnet的思想，从基本的resnet 的bottleneck unit 逐步演变得到 ShuffleNet 的bottleneck unit，然后堆叠的使用ShuffleNet bottleneck unit获得ShuffleNet；<br>下图展示了ShuffleNet unit的演化过程<br>图(a)：是一个带有depthwise convolution的bottleneck unit；<br>图(b)：作者在(a)的基础上进行变化，对1x1 conv 换成 1x1 Gconv，并在第一个1x1 Gconv之后增加一个channel shuffle 操作；<br>图(c)： 在旁路增加了AVG pool，目的是为了减小feature map的分辨率；因为分辨率小了，于是乎最后不采用Add，而是concat，从而“弥补”了分辨率减小而带来的信息损失。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/shufflenet.png"></p><h2 id="ShuffleNet-v2"><a href="#ShuffleNet-v2" class="headerlink" title="ShuffleNet v2"></a><a href="https://arxiv.org/abs/1807.11164">ShuffleNet v2</a></h2><p>目前衡量模型复杂度的一个通用指标是FLOPs，具体指的是multiply-add数量，主要衡量的就是卷积层的乘法操作. 但是这却是一个间接指标，因为它不完全等同于速度, 相同MFLOPs的网络实际速度差别却很大，因此以FLOPs作为衡量模型速度的指标是有问题的。</p><p>相同FLOPs的两个模型存在速度上的差异的主要原因有:</p><ul><li>内存使用量(MAC)</li><li>模型的并行程度也影响速度，并行度高的模型速度相对更快</li><li>模型在不同平台上的运行速度是有差异的，如GPU和ARM，而且采用不同的库也会有影响</li></ul><h3 id="四个实验"><a href="#四个实验" class="headerlink" title="四个实验"></a>四个实验</h3><ul><li>第一个实验是关于卷积层的输入输出特征通道数对MAC指标的影响。结论是卷积层的输入和输出特征通道数相等时MAC最小，此时模型速度最快。</li><li>第二个实验是关于卷积的group操作对MAC的影响。结论是过多的group操作会增大MAC，从而使模型速度变慢。</li><li>第三个实验是关于模型设计的分支数量对模型速度的影响。结论是模型中的分支数量越少，模型速度越快。</li><li>第四个实验是关于element-wise操作对模型速度的影响。结论是element-wise操作所带来的时间消耗远比在FLOPs上的体现的数值要多，因此要尽可能减少element-wise操作。</li></ul><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>根据前面的4条准则，作者分析了ShuffleNetv1设计的不足，并在此基础上改进得到了ShuffleNetv2，两者模块上的对比如下:</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/shufflenet-v2.jpg"></p><p>在ShuffleNetv1的模块中，大量使用了1x1组卷积，这违背了G2原则，另外v1采用了类似ResNet中的瓶颈层（bottleneck layer），输入和输出通道数不同，这违背了G1原则。同时使用过多的组，也违背了G3原则。短路连接中存在大量的元素级Add运算，这违背了G4原则。(G4代表Guide 4)</p><h3 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h3><p>为了改善v1的缺陷，v2版本引入了一种新的运算：channel split。具体来说，在开始时先将输入特征图在通道维度分成两个分支：$c$ 和 $c - c’$, 原文中 $c’=\frac{c}{2}$, 左边分支用来做恒等映射, 右边分支包含3个连续的卷积, 并且输入和输出通道相同.</p><p>而且两个1x1卷积不再是分组卷积，这符合G2，另外两个分支相当于已经分成两组。两个分支的输出不再是Add元素，而是concat在一起，紧接着是对两个分支concat结果进行channle shuffle，以保证两个分支信息交流。其实concat和channel shuffle可以和下一个模块单元的channel split合成一个元素级运算，这符合原则G4。</p><p>对于下采样模块，不再有channel split，而是每个分支都是直接copy一份输入，每个分支都有stride=2的下采样，最后concat在一起后，特征图空间大小减半，但是通道数翻倍。</p><h3 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a><a href="https://arxiv.org/abs/1610.02357">Xception</a></h3><p><strong>思想是卷积的时候要将通道的卷积与空间的卷积进行分离，这样会比较好(这是Inception)的假设, 没有理论上的证明</strong><br>先回顾下 Inception v3</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/inceptionv3.png"></p><p>下一步作者简化了 inception module（就是只保留1*1的那条“路”，其他的 3x3 conv, 5x5 conv 以及a avg pool 都删除)</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/simply_inception.png"></p><p>假设出一个简化版inception module之后，再进一步假设，把第一部分的3个1x1卷积核统一起来，变成一个1x1的，后面的3个3x3的分别“负责”一部分通道，如图3所示； 最后提出“extreme” version of an Inception ，module Xception登场，先用1x1卷积核对各通道之间（cross-channel）进行卷积</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/cross-channel.png"></p><p>总的来说就是先用 1x1 的卷积改变输入特征图的通道数, 再用若干个 3x3 卷积核来处理不同通道, 这个和 MobileNet 中的顺序是相反的, 这个真的能吐槽一下…顺序反一下就又是一篇论文…</p><p>参考了<a href="blog.csdn.net/u011995719/article/details/79100582">TensorSense的CSDN博客</a></p><h2 id="EfficientNet系列"><a href="#EfficientNet系列" class="headerlink" title="EfficientNet系列"></a><a href="https://arxiv.org/abs/1905.11946">EfficientNet系列</a></h2><p>NAS直接搜出来的…我们没卡就算了, 了解下吧</p><h3 id="构建步骤"><a href="#构建步骤" class="headerlink" title="构建步骤"></a>构建步骤</h3><ol><li>使用强化学习算法实现的MnasNet模型生成基线模型EfficientNet-B0.</li><li>采用复合缩放的方法，在预先设定的内存和计算量大小的限制条件下，对EfficientNet-B0模型的深度、宽度（特征图的通道数）、图片大小这三个维度都同时进行缩放，这三个维度的缩放比例由网格搜索得到。最终输出了EfficientNet模型.</li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/efficient-search.jpg"></p><h3 id="其他信息"><a href="#其他信息" class="headerlink" title="其他信息"></a>其他信息</h3><p>和 MobileNet v2 一样采用了 Inverted Residual Block.</p><h2 id="GhostNet"><a href="#GhostNet" class="headerlink" title="GhostNet"></a><a href="https://arxiv.org/pdf/1911.11907.pdf">GhostNet</a></h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>使用简单的线性变换生成特征图, 从而用更少的参数来生成更多特征, <a href="https://github.com/huawei-noah/ghostnet">代码链接</a></p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>通常情况下，为了对输入数据有更好的理解，使用普通卷积训练的神经网络，在训练完成后会生成很多冗余的特征图，下图中颜色相同的框代表着相似的”特征图对”.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/feature_map_pair.png"></p><p>这样的操作，虽然能获得很好的性能，但是却需要驱动大量的卷积层计算，耗费大量的计算资源. 由上图可以看到, 我们可以把一个特征图看成是另一个特征图的”幻影”, 即 ghost, 一个特征图的”幻影”通过简单的线性变换来得到, 这样就能降低计算成本.</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/ghost_net.png"></p><p>这里的 cheap operation 就是一个简单的卷积层, 核心代码如下:</p><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看代码</span></div>    <div class="hide-content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GhostModule</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, inp, oup, kernel_size=<span class="number">1</span>, ratio=<span class="number">2</span>, dw_size=<span class="number">3</span>, stride=<span class="number">1</span>, relu=True</span>):</span></span><br><span class="line">        super(GhostModule, self).__init__()</span><br><span class="line">        self.oup = oup</span><br><span class="line">        init_channels = math.ceil(oup / ratio)</span><br><span class="line">        new_channels = init_channels*(ratio<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        self.primary_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//<span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(init_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>) <span class="keyword">if</span> relu <span class="keyword">else</span> nn.Sequential(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.cheap_operation = nn.Sequential(</span><br><span class="line">            nn.Conv2d(init_channels, new_channels, dw_size, <span class="number">1</span>, dw_size//<span class="number">2</span>, groups=init_channels, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(new_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>) <span class="keyword">if</span> relu <span class="keyword">else</span> nn.Sequential(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.primary_conv(x)</span><br><span class="line">        x2 = self.cheap_operation(x1)</span><br><span class="line">        out = torch.cat([x1,x2], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out[:,:self.oup,:,:]</span><br></pre></td></tr></table></figure></div></div><p>Ghost bottleneck<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Lightweight-Network/ghost_bottleneck.png"></p><h3 id="压缩程度"><a href="#压缩程度" class="headerlink" title="压缩程度"></a>压缩程度</h3><p>假设 Ghost Module 输入特征图的大小为 $c_{in} \cdot h \cdot w$, 输出特征图的大小为 $c_{out} \cdot h’ \cdot w’$. 可以看到 Ghost Module 中有2个卷积, 第二个卷积的输入是第一个卷积的结果.<br>第一个卷积的卷积核大小为 $k \times k$, 第一个卷积得到的特征图的通道数为 $m$, 对有 $m$ 个通道的特征图进行线性变换, 得到通道数为 $n=m \times s$ 的特征图, 其中的 $s$ 表示线性操作的方式(例如3x3, 5x5卷积), 假设线性变换的卷积核大小为 $d \times d$, 那么 Ghost Module 相比普通模型的加速比例为:</p><p>$$<br>\begin{aligned}<br>r_{s} &amp;=\frac{n \cdot h^{\prime} \cdot w^{\prime} \cdot c \cdot k \cdot k}{\frac{n}{s} \cdot h^{\prime} \cdot w^{\prime} \cdot c \cdot k \cdot k+(s-1) \cdot \frac{n}{s} \cdot h^{\prime} \cdot w^{\prime} \cdot d \cdot d} \<br>&amp;=\frac{c \cdot k \cdot k}{\frac{1}{s} \cdot c \cdot k \cdot k+\frac{s-1}{s} \cdot d \cdot d} \approx \frac{s \cdot c}{s+c-1} \approx s<br>\end{aligned}<br>$$</p><p>其中的 $\frac{n}{s} = m$, 即第一个卷积层之后的特征图通道数, 公式中的 $(s-1)$ 是因为第一个卷积层之后的特征图会直接堆叠到后面的特征图上, 请注意看上面的网络结构图.</p><h3 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h3><ol><li>这里基本遵循 MobileNetV3 的基本体系结构, 第一个 Ghost Module 用于升维, 第二个 Ghost Module 用于降维使得通道数和 shortcut 匹配, 类似于 Inverted Residual Block</li><li>利用全局平均池和卷积层将特征图转换为1280维特征向量以进行最终分类</li><li>SE模块也用在了某些Ghost bottleneck中</li></ol>]]></content>
      
      
      <categories>
          
          <category> Backbone </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Lightweight Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu software installation and beautification</title>
      <link href="2020/06/15/Ubuntu-software-installation-and-beautification/"/>
      <url>2020/06/15/Ubuntu-software-installation-and-beautification/</url>
      
        <content type="html"><![CDATA[<h2 id="解决双系统时差问题"><a href="#解决双系统时差问题" class="headerlink" title="解决双系统时差问题"></a>解决双系统时差问题</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timedatectl set-local-rtc 1 --adjust-system-clock</span><br></pre></td></tr></table></figure><h2 id="替换-Shell"><a href="#替换-Shell" class="headerlink" title="替换 Shell"></a>替换 Shell</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install git</span><br><span class="line">sudo apt install zsh</span><br><span class="line">sh -c &quot;$(wget -O- https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;</span><br><span class="line">chsh -s /usr/bin/zsh</span><br></pre></td></tr></table></figure><h2 id="主题美化"><a href="#主题美化" class="headerlink" title="主题美化"></a>主题美化</h2><h3 id="系统主题-Sieera"><a href="#系统主题-Sieera" class="headerlink" title="系统主题 Sieera"></a>系统主题 Sieera</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:dyatlov-igor/sierra-theme</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install sierra-gtk-theme # point releases</span><br><span class="line">sudo apt install sierra-gtk-theme-git # git master branch, 好像是可忽略</span><br></pre></td></tr></table></figure><h3 id="应用图标-suru-plus"><a href="#应用图标-suru-plus" class="headerlink" title="应用图标 suru-plus"></a>应用图标 suru-plus</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -qO- https://raw.githubusercontent.com/gusbemacbe/suru-plus/master/install.sh | env DESTDIR=&quot;$HOME/.icons&quot; sh</span><br></pre></td></tr></table></figure><h3 id="Grub-引导主题-Fallout"><a href="#Grub-引导主题-Fallout" class="headerlink" title="Grub 引导主题 Fallout"></a>Grub 引导主题 Fallout</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wge`t -O - https://github.com/shvchk/fallout-grub-theme/raw/master/install.sh | bash</span><br></pre></td></tr></table></figure><h3 id="文泉驿字体"><a href="#文泉驿字体" class="headerlink" title="文泉驿字体"></a>文泉驿字体</h3><h4 id="文泉驿字体-微米黑-正黑"><a href="#文泉驿字体-微米黑-正黑" class="headerlink" title="文泉驿字体 微米黑/正黑"></a>文泉驿字体 微米黑/正黑</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install fonts-wqy-microhei fonts-wqy-zenhei</span><br></pre></td></tr></table></figure><h4 id="终端字体-powerline"><a href="#终端字体-powerline" class="headerlink" title="终端字体 powerline"></a>终端字体 powerline</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install fonts-powerline</span><br></pre></td></tr></table></figure><h3 id="优化工具-Gnome-tweak-tool"><a href="#优化工具-Gnome-tweak-tool" class="headerlink" title="优化工具(Gnome-tweak-tool)"></a>优化工具(Gnome-tweak-tool)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install gnome-tweak-tool</span><br><span class="line">sudo apt install gnome-shell-extensions</span><br></pre></td></tr></table></figure><h3 id="主题自定义"><a href="#主题自定义" class="headerlink" title="主题自定义"></a>主题自定义</h3><h4 id="Gnome-tweak-tool"><a href="#Gnome-tweak-tool" class="headerlink" title="Gnome-tweak-tool"></a>Gnome-tweak-tool</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install gnome-tweak-tool</span><br></pre></td></tr></table></figure><p>移步到 所有软件→Ubuntu 软件→附加组件 ，在此处安装相应的 Shell 组件。为了自定义 Shell 主题（加载本地文件），需要安装、启用插件：User Themes 。</p><h4 id="主题-图标-字体-通过”优化”软件更改"><a href="#主题-图标-字体-通过”优化”软件更改" class="headerlink" title="主题 图标 字体(通过”优化”软件更改)"></a>主题 图标 字体(通过”优化”软件更改)</h4><p>安装目录有两种，区别上类似于 Windows 环境变量里的个人和系统<br>主题存放目录：/usr/share/themes 或 ~/.themes<br>图标存放目录：/usr/share/icons 或 ~/.icons<br>字体存放目录：/usr/share/fonts 或 ~/.fonts<br>其中 /usr/share 目录需要 root 权限才能修改，可以对文件管理提权后打开：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nautilus</span><br></pre></td></tr></table></figure><h4 id="GDM-登录背景图修改"><a href="#GDM-登录背景图修改" class="headerlink" title="GDM 登录背景图修改"></a>GDM 登录背景图修改</h4><p>更换登录界面的背景图需要修改 ubuntu.css，它位于 /usr/share/gnome-shell/theme 。<br>在终端输入以下命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /usr/share/gnome-shell/theme/ubuntu.css</span><br></pre></td></tr></table></figure><p>找到下面的内容(一般在 1815 行)</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#lockDialogGroup</span> &#123;</span><br><span class="line"><span class="attribute">background</span>: <span class="number">#2c001e</span> <span class="built_in">url</span>(resource:///org/gnome/shell/theme/noise-texture.png);</span><br><span class="line"><span class="attribute">background-repeat</span>: repeat; &#125;</span><br></pre></td></tr></table></figure><p>修改图片路径即可，样例如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#lockDialogGroup</span> &#123;</span><br><span class="line"><span class="attribute">background</span>: <span class="number">#2c001e</span> <span class="built_in">url</span>(file:///home/inkss/APP/ink_img/img.jpg);</span><br><span class="line"><span class="attribute">background-repeat</span>: no-repeat;</span><br><span class="line"><span class="attribute">background-size</span>: cover;</span><br><span class="line"><span class="attribute">background-position</span>: center; &#125;</span><br></pre></td></tr></table></figure><h2 id="Gnome-扩展"><a href="#Gnome-扩展" class="headerlink" title="Gnome 扩展"></a>Gnome 扩展</h2><h3 id="Gnome-Shell-安装"><a href="#Gnome-Shell-安装" class="headerlink" title="Gnome Shell 安装"></a>Gnome Shell 安装</h3><p>获取扩展的方法很多：终端命令，软件中心下载，浏览器下载等，这里主要介绍浏览器下载。首先安装 Gnome Shell ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install chrome-gnome-shell</span><br></pre></td></tr></table></figure><p>然后安装浏览器插件（谷歌浏览器）：<a href="https://chrome.google.com/webstore/detail/gnome-shell-integration/gphhapmejobijbbhgpjhcjognlahblep">Chrome 网上应用商店</a> 。</p><p>无条件的同学可以使用火狐浏览器安装扩展：<a href="https://addons.mozilla.org/zh-CN/firefox/addon/gnome-shell-integration/?src=search">GNOME Shell integration</a>。</p><p>浏览器插件安装完成后点击 插件图标 就能进入：<a href="https://extensions.gnome.org/">Shell 扩展商店</a> 。</p><h3 id="Gnome-扩展推荐"><a href="#Gnome-扩展推荐" class="headerlink" title="Gnome 扩展推荐"></a>Gnome 扩展推荐</h3><table><thead><tr><th>拓展</th><th>简要描述</th></tr></thead><tbody><tr><td><a href="https://extensions.gnome.org/extension/1112/screenshot-tool/">Screenshot Tool</a></td><td>截图工具</td></tr><tr><td><a href="https://extensions.gnome.org/extension/1166/extension-update-notifier/">Extension update notifier</a></td><td>自动推送所有扩展的更新信息</td></tr><tr><td><a href="https://extensions.gnome.org/extension/97/coverflow-alt-tab/">Coverflow Alt-Tab</a></td><td>Alt Tab 切换应用（更酷炫的界面, 可能导致界面卡死）</td></tr><tr><td><a href="https://extensions.gnome.org/extension/307/dash-to-dock/">Dash to Dock</a></td><td>Dock管理</td></tr><tr><td><a href="https://extensions.gnome.org/extension/750/openweather/">OpenWeather</a></td><td>顶栏显示天气情况（支持中文）</td></tr><tr><td><a href="https://extensions.gnome.org/extension/19/user-themes/">User Themes</a></td><td>允许本地安装使用 Shell 主题</td></tr></tbody></table><h2 id="常用软件下载"><a href="#常用软件下载" class="headerlink" title="常用软件下载"></a>常用软件下载</h2><h3 id="搜狗输入法"><a href="#搜狗输入法" class="headerlink" title="搜狗输入法"></a>搜狗输入法</h3><p>先<a href="https://pinyin.sogou.com/linux/?r=pinyin">下载搜狗输入法的deb文件</a><br>安装Fctix输入框架</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install fcitx</span><br></pre></td></tr></table></figure><p>之后安装搜狗的安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gdebi xxxxxx.deb</span><br></pre></td></tr></table></figure><p>然后移步到 <strong>设置→区域和语言</strong> ，删除一部分输入源，只保留汉语，接着选择 <strong>管理已安装的语言</strong> ，修改 键盘输入法系统为 <strong>fcitx</strong> 。关闭窗口，打开所有程序，选择软件 <strong>Fcitx 配置</strong> ，选择加号添加搜狗输入法</p><h3 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a>Chrome</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -</span><br><span class="line">sudo sh -c &#x27;echo &quot;deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main&quot; &gt;&gt; /etc/apt/sources.list.d/google.list&#x27;</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install google-chrome-stable</span><br></pre></td></tr></table></figure><h3 id="Typora"><a href="#Typora" class="headerlink" title="Typora"></a>Typora</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Typora</span><br></pre></td></tr></table></figure><h3 id="Albert"><a href="#Albert" class="headerlink" title="Albert"></a>Albert</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install curl</span><br><span class="line">curl https://build.opensuse.org/projects/home:manuelschneid3r/public_key | sudo apt-key add -</span><br><span class="line">sudo sh -c &quot;echo &#x27;deb http://download.opensuse.org/repositories/home:/manuelschneid3r/xUbuntu_18.04/ /&#x27; &gt; /etc/apt/sources.list.d/home:manuelschneid3r.list&quot;</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install albert</span><br></pre></td></tr></table></figure><h3 id="网易云音乐"><a href="#网易云音乐" class="headerlink" title="网易云音乐"></a>网易云音乐</h3><p><a href="https://music.163.com/#/download">下载deb包</a><br>然后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 这里的 <span class="string">&#x27;1.2.1_amd64_ubuntu_20190428&#x27;</span> 是版本信息, 按照你下载的安装包来</span></span><br><span class="line">sudo dpkg -i netease-cloud-music_1.2.1_amd64_ubuntu_20190428.deb</span><br></pre></td></tr></table></figure><h3 id="WPS-2019下载"><a href="#WPS-2019下载" class="headerlink" title="WPS 2019下载"></a>WPS 2019下载</h3><p><a href="https://www.wps.cn/product/wpslinux">下载deb包</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i wps-office_11.1.0.8865_amd64.deb</span><br></pre></td></tr></table></figure><h3 id="DeepinWine-QQ-WeChat"><a href="#DeepinWine-QQ-WeChat" class="headerlink" title="DeepinWine QQ WeChat"></a>DeepinWine QQ WeChat</h3><p>首先需要在本机下载 Deepin-Wine 环境：<a href="https://github.com/wszqkzqk/deepin-wine-ubuntu">deepin-wine-ubuntu</a><br>克隆或下载压缩包到本机，<strong>解压后</strong> 在终端目录下执行命令：./install.sh 安装环境<br>容器下载地址：<a href="http://mirrors.aliyun.com/deepin/pool/non-free/d/">Index of /deepin/pool/non-free/d/</a> ，使用方法见仓库中的 <a href="https://github.com/wszqkzqk/deepin-wine-ubuntu/blob/master/README.md">README 文件</a>。</p><h2 id="顶部栏透明"><a href="#顶部栏透明" class="headerlink" title="顶部栏透明"></a>顶部栏透明</h2><ol><li><p>打开”优化(Gnome-tweak-tool)”软件, 查看自己正在使用的主题<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Ubuntu-software-installation-and-beautification/1.png"></p></li><li><p>在 <code>/usr/share/themes</code> 下找到主题文件夹<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Ubuntu-software-installation-and-beautification/2.png"></p></li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Ubuntu-software-installation-and-beautification/3.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Ubuntu-software-installation-and-beautification/4.png"></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Ubuntu-software-installation-and-beautification/5.png"></p><ol start="3"><li>找到自己正在应用的主题文件夹并打开位于该文件夹下的 <code>gnome-shell</code> 并进入<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Ubuntu-software-installation-and-beautification/6.png"></li></ol><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Ubuntu-software-installation-and-beautification/7.png"></p><p>右键空白处, 打开终端使用如下命令打开CSS文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit gnome-shell.css</span><br></pre></td></tr></table></figure><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Ubuntu-software-installation-and-beautification/8.png"></p><p>找到如下代码(该主题在1095行)</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* TOP BAR */</span></span><br><span class="line"><span class="selector-id">#Panel</span> &#123;</span><br><span class="line">    <span class="attribute">background-color</span>: <span class="built_in">rgba</span>(<span class="number">42</span>, <span class="number">42</span>, <span class="number">42</span>, <span class="number">0.5</span>)</span><br><span class="line">    ...</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>将第一行中的代码改为</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* TOP BAR */</span></span><br><span class="line"><span class="selector-id">#Panel</span> &#123;</span><br><span class="line">    <span class="attribute">background-color</span>: <span class="built_in">rgba</span>(<span class="number">42</span>, <span class="number">42</span>, <span class="number">42</span>, <span class="number">0</span>)</span><br><span class="line">    ...</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>如图所示<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Ubuntu-software-installation-and-beautification/9.png"></p><p>其中rgba中的a表示模糊程度, 0位完全透明, 修改并保存CSS文件后重启即可</p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>本文除了顶部完全透明的教程以外, 其他内容均来自<a href="https://www.coolapk.com/">酷安</a>的<a href="https://inkss.cn/">枋柚梓的猫会发光</a>, 超级感谢大佬的帮助以及教程~</p>]]></content>
      
      
      <categories>
          
          <category> Ubuntu </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ResNeXt</title>
      <link href="2020/06/15/ResNeXt/"/>
      <url>2020/06/15/ResNeXt/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在深度和宽度外找到了第3个维度：基数。模型也因找到了下一个调参维度，而被称为“NeXt”。<br>论文: <a href="https://arxiv.org/abs/1611.05431">Aggregated Residual Transformations for Deep Neural Networks</a></p><p>PyTorch代码: <a href="https://github.com/miraclewkf/ResNeXt-PyTorch/blob/master/resnext.py">ResNeXt-PyTorch</a>, <a href="https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/backbones/resnext.py">mmdetection_resnext</a></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/ResNet-ResNeXt/resnext.png"></p><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ul><li>网络结构简明, 模块化</li><li>由于分支同构所以需要调节的参数少</li><li>和ResNet比相同参数有更好效果</li></ul><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>$$<br>\mathcal{F}(\mathbf{x})=\sum_{i=1}^{C} \mathcal{T}_{i}(\mathbf{x})<br>$$</p><p>其中是 ${T}<em>{i}$ 是能把 $x$ 投影到低维子空间并进行任意变换的函数,在ResNeXt中, ${T}</em>{i}$ 是一个自带瓶颈的网络结构(1 $\times$ 1conv降维 $\rightarrow$ 3 $\times$ 3conv $\rightarrow$  1 $\times$ 1conv升维), $C$就是基数, 是出了平常的宽度和深度以外的第三个超参数, 最后加上恒等映射就得到了ResNeXt的基本结构。<br>$$<br>\mathbf{y}=\mathbf{x}+\sum_{i=1}^{C} T_{i}(\mathbf{x})<br>$$</p><p><strong>ResNeXt v1</strong>的图如下所示</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/ResNet-ResNeXt/down_trans_up.jpg"></p><h2 id="模型理解"><a href="#模型理解" class="headerlink" title="模型理解"></a>模型理解</h2><p>和MobileNet中的DW卷积一样的思想, 通过使用分组卷积来处理输入中的不同特征. 处理完之后再进行聚合, 这个和Inception思想一样, 即 分割→变换→聚合, 也融合了 ResNet 中的 降维→变换→升维. <strong>我的理解是把输入特征图的每个特征(通道)用不同的方式(卷积核)处理之后再融合, 而不是每个卷积核都去处理所有通道, 感觉就是incepton和resnet思想的融合</strong></p><h2 id="三种形式的理解"><a href="#三种形式的理解" class="headerlink" title="三种形式的理解"></a>三种形式的理解</h2><p>如果只看每个Block中单独的支路（Branch），则其只是一个常见的“降维→变换→升维”的Bottleneck结构。但是，若以“分割-变换-聚合”的角度考虑，那么第一个1*1conv的“降维”，实际上也是把输入分给基数（这里C=32）个低维嵌入的过程。但是分割/汇聚不一定是个显式过程，必要时完全可以等加成更简单的表达, 也就是<strong>可以把负责升维的若干个conv结合起来从而得到 ResNeXt v2:</strong></p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/ResNet-ResNeXt/v2.jpg"></p><p>也就是说ResNeXt v1可以变换为ResNeXt v2这种形式, 而且 v1 和 v2 的参数数量是一致的。</p><p>可以看到第一个分开的conv其实都接受了一样的输入，各分支又有着相同的拓扑结构。类比乘法结合律，这其实就是把一个conv的输出拆开成32个再分掉。（相同输入，不同输出） 而最后一个conv又只对同一个输出负责，因此就可以并起来用一个conv处理。（不同输入，相同输出）</p><p>唯一一个输入和输出都不同的，就是中间的3*3conv了。它们的输入，参数，负责的输出都不同，无法合并，因此也相互独立。这才是模型的关键所在。</p><p>简单来说就是 ResNeXt v1 到 ResNeXt v2 的变换是把不同输入相同输出的conv结合起来, 所以很自然地就能想到还可以把相同输入不同输出的conv结合起来, 即把 ResNeXt v2 中低维嵌入的那一部分给结合起来。这样就得到了ResNeXt v3。</p><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/ResNet-ResNeXt/v3.png"></p>]]></content>
      
      
      <categories>
          
          <category> Backbone </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Backbone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MobileNet v1-v3</title>
      <link href="2020/06/14/MobileNet-v1-v3/"/>
      <url>2020/06/14/MobileNet-v1-v3/</url>
      
        <content type="html"><![CDATA[<h2 id="MobileNet-v1"><a href="#MobileNet-v1" class="headerlink" title="MobileNet v1"></a>MobileNet v1</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/MobileNet-v1-v3/mobilev1.png"></p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul><li>使用深度可分离卷积(depthwise + pointwise)来代替传统卷积</li><li>添加2个收缩超参数: 宽度乘子和分辨率乘子</li></ul><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>单模型效率不高, 需要加入ResNet, DenseNet等模型</li><li><strong>Depthwise Convolution存在潜在问题，训练后部分kernel的权值为0</strong></li></ul><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>Depthwise Separable Convolution实质上是将标准卷积分成了两步：depthwise卷积和pointwise卷积，其输入与输出都是相同的</p><ul><li>depthwise卷积：对每个输入通道单独使用一个卷积核处理</li><li>pointwise卷积：1×1卷积，用于将depthwise卷积的输出组合起来</li></ul><p>实际上就是先用3 $\times$ 3, padding=1的depthwise卷积对不同通道上的特征图做分组卷积, 再利用1 $\times$ 1的pointwise卷积来让通道数改变, 从而得到和普通卷积一样的效果。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/MobileNet-v1-v3/standard_vs_DSC.png"><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/MobileNet-v1-v3/compare.png"></p><h3 id="为什么需要-Pointwise-Convolution"><a href="#为什么需要-Pointwise-Convolution" class="headerlink" title="为什么需要 Pointwise Convolution"></a>为什么需要 Pointwise Convolution</h3><p>采用 Depthwise Convolution 会有一个问题，就是导致“信息流通不畅”，即输出的feature map仅包含输入的feature map的一部分，MobileNet采用了point-wise来解决问题</p><h3 id="深度可分离卷积和标准卷积的计算量"><a href="#深度可分离卷积和标准卷积的计算量" class="headerlink" title="深度可分离卷积和标准卷积的计算量"></a>深度可分离卷积和标准卷积的计算量</h3><p>输入为 $D_k \times D_k \times M$, 其中$D_k$为长和宽, $M$为通道数, 输出为 $D_f \times D_f \times N$, 其中$D_f$为长和宽, $N$为通道数.</p><ul><li>标准卷积的卷积核个数为$D_k \times D_k \times M \times N$, 所以标准卷积的计算量为 $D_k \times D_k \times M \times N \times D_f \times D_f$</li><li>DW卷积的计算量为$D_k \times D_k \times M \times D_f \times D_f$, PW卷积的计算量为$D_f \times D_f \times M \times N$, 所以总的计算量为 $D_k \times D_k \times M \times D_f \times D_f + D_f \times D_f \times M \times N$<br>即$D_f \times D_f \times M \times (N + D_k \times D_k)$</li><li>所以深度可分离卷积和标准卷积的计算量之比为 $\frac{D_f \times D_f \times M \times (N + D_k \times D_k)}{D_f \times D_f \times M \times N \times D_k \times D_k} = \frac{N + D_k^2}{N \times D_k^2} = \frac{1}{D_k^2} + \frac{1}{N}$</li></ul><h3 id="主要超参数"><a href="#主要超参数" class="headerlink" title="主要超参数"></a>主要超参数</h3><ul><li><strong>宽度乘子(Width Multiplier): $\alpha$</strong><br>该参数用于控制特征图的维数，即通道数, 作用是在整体上对网络的每一层维度（特征数量）进行瘦身</li><li><strong>分辨率乘子(Resolution Multiplier): $p$</strong><br>用于控制特征图的宽/高，即分辨率</li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看代码</span></div>    <div class="hide-content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MobileNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(MobileNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">conv_bn</span>(<span class="params">inp, oup, stride</span>):</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(inp, oup, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(oup),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">conv_dw</span>(<span class="params">inp, oup, stride</span>):</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(inp, inp, <span class="number">3</span>, stride, <span class="number">1</span>, groups=inp, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(inp),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                nn.Conv2d(inp, oup, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(oup),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            conv_bn(  <span class="number">3</span>,  <span class="number">32</span>, <span class="number">2</span>), </span><br><span class="line">            conv_dw( <span class="number">32</span>,  <span class="number">64</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw( <span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>),</span><br><span class="line">            conv_dw(<span class="number">128</span>, <span class="number">128</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>),</span><br><span class="line">            conv_dw(<span class="number">256</span>, <span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">1024</span>, <span class="number">2</span>),</span><br><span class="line">            conv_dw(<span class="number">1024</span>, <span class="number">1024</span>, <span class="number">1</span>),</span><br><span class="line">            nn.AvgPool2d(<span class="number">7</span>),</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1024</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.model(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">1024</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></div></div><h2 id="MobileNet-v2"><a href="#MobileNet-v2" class="headerlink" title="MobileNet v2"></a>MobileNet v2</h2><p>一句话总结就是：使用中间比两端更多通道的Depthwise的Residual Block，并且去掉最后的ReLU。</p><h3 id="主要改动"><a href="#主要改动" class="headerlink" title="主要改动"></a>主要改动</h3><ul><li>逆向残差块(Inverted Residual Blocks)<br>MobileNetV2 结构基于 inverted residual。其本质是一个残差网络设计，传统 Residual block 是 block 的两端 channel 通道数多，中间少，而本文设计的 inverted residual 是 block 的两端 channel 通道数少，block 内 channel 多，类似于沙漏和梭子形态的区别。另外保留 Depthwise Separable Convolutions。</li><li>Linear Bottlenecks<br>感兴趣区域在 ReLU 之后保持非零，近似认为是线性变换。<br>ReLU 能够保持输入信息的完整性，但仅限于输入特征位于输入空间的低维子空间中。<br>对于低纬度空间处理，论文中把 ReLU 近似为线性转换。</li></ul><h3 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h3><p><a href="https://arxiv.org/abs/1610.02357">Xception</a>论文指出，这种卷积背后的假设是跨channel相关性和跨spatial相关性的解耦。另一方面，网络中的feature map被认为在channel维度存在冗余，常利用1x1卷积进行channel数的压缩后再进行3x3可分离卷积的运算。演化过程如下：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/MobileNet-v1-v3/evolution.jpg"></p><p>图a中普通卷积将channel和spatial的信息同时进行映射，参数量较大；图b为可分离卷积，解耦了channel和spatial，化乘法为加法，有一定比例的参数节省；图c中进行可分离卷积后又添加了bottleneck，映射到低维空间中；图d则是从低维空间开始，进行可分离卷积时扩张到较高的维度（前后维度之比被称为expansion factor，扩张系数），之后再通过1x1卷积降到原始维度。</p><p>这里要指出的一点是，观察和设计一个网络时，隐含的会有“状态层”和“变换层”的划分。当上图中c和d的结构堆叠起来时，其事实上是等价的。在这个视角下，我们可以把channel数少的张量看做状态层，而channel数多的张量看做变换层。这种视角下，在网络中传递的特征描述是压缩的，进行新一轮的变换时被映射到channel数相对高的空间上进行运算（文中称这一扩张比例为扩张系数，实际采用的数值为6），之后再压缩回原来的容量。</p><h3 id="逆向残差块-Inverted-Residual-Blocks"><a href="#逆向残差块-Inverted-Residual-Blocks" class="headerlink" title="逆向残差块(Inverted Residual Blocks)"></a>逆向残差块(Inverted Residual Blocks)</h3><p><strong>注意b图中最前面和最后面的特征图的形状, 这是由于在卷积后没有使用ReLU6的原因</strong><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/MobileNet-v1-v3/inverted_res.jpg"></p><p>原始的ResNet block先用1x1降通道过ReLU，再3x3空间卷积过ReLU，再用1x1卷积过ReLU恢复通道，并和输入相加。之所以要1x1卷积降通道，是为了减少计算量，不然中间的3x3空间卷积计算量太大。所以Residual block是沙漏形，两边宽中间窄。</p><p>但现在中间的3x3卷积变为了Depthwise的了，计算量很少了，所以通道可以多一点，效果更好，所以通过1x1卷积先提升通道数，再Depthwise的3x3空间卷积，再用1x1卷积降低维度。两端的通道数都很小，所以1x1卷积升通道或降通道计算量都并不大，而中间通道数虽然多，但是是Depthwise的卷积，计算量也不大。</p><p>V2 在 DW 卷积之前新加了一个 PW 卷积。这么做的原因，是因为 DW 卷积由于本身的计算特性决定它自己没有改变通道数的能力，上一层给它多少通道，它就只能输出多少通道。所以如果上一层给的通道数本身很少的话，DW 也只能很委屈的在低维空间提特征，因此效果不够好。现在 V2 为了改善这个问题，给每个 DW 之前都配备了一个 PW，专门用来升维，定义升维系数 t = 6，这样不管输入通道数是多是少，经过第一个 PW 升维之后，DW 都是在相对的更高维 进行着辛勤工作的。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/MobileNet-v1-v3/v1_v2_res_compare.jpg"></p><h3 id="线性瓶颈-Linear-Bottlenecks"><a href="#线性瓶颈-Linear-Bottlenecks" class="headerlink" title="线性瓶颈(Linear Bottlenecks)"></a>线性瓶颈(Linear Bottlenecks)</h3><p>文中，经过激活层后的张量被称为兴趣流形，具有$C \times H \times W$维，其中C即为通常意义的channel数，部分文章也将其称为网络的宽度（width）。根据之前的研究，兴趣流形可能仅分布在激活空间的一个低维子空间里，利用这一点很容易使用1x1卷积将张量降维（即MobileNet V1的工作），但由于ReLU的存在，这种降维实际上会损失较多的信息。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/MobileNet-v1-v3/linear_bottlenecks.png"></p><p>首选看看RELU的功能。RELU可以将负值全部映射为0，具有高度非线性。上图为论文的测试。在维度比较低2,3的时候，使用RELU对信息的损失是比较严重的。而单维度比较高15,30时，信息的损失是比较少的。MobileNet v2中为了保证信息不被大量损失，应此在残差模块中去掉最后一个的RELU。因此，也称为线性模块单元。</p><p>简单来说就是作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，不如线性的效果好。由于第二个 PW 的主要功能就是降维，因此按照上面的理论，降维之后就不宜再使用 ReLU6 了。</p><p>注: ReLU6，卷积之后通常会接一个ReLU非线性激活，在Mobile v2里面使用ReLU6，ReLU6就是普通的ReLU但是限制最大输出值为6（对输出值做clip），这是为了在移动端设备float16的低精度的时候，也能有很好的数值分辨率，如果对ReLU的激活范围不加限制，输出范围为0到正无穷，如果激活值非常大，分布在一个很大的范围内，则低精度的float16无法很好地精确描述如此大范围的数值，带来精度损失。</p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看代码</span></div>    <div class="hide-content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearBottleNeck</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, stride, t=<span class="number">6</span>, class_num=<span class="number">100</span></span>):</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.residual = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, in_channels * t, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(in_channels * t),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(in_channels * t, in_channels * t, <span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, groups=in_channels * t),</span><br><span class="line">            nn.BatchNorm2d(in_channels * t),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(in_channels * t, out_channels, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line"></span><br><span class="line">        residual = self.residual(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.stride == <span class="number">1</span> <span class="keyword">and</span> self.in_channels == self.out_channels:</span><br><span class="line">            residual += x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> residual</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MobileNetV2</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, class_num=<span class="number">100</span></span>):</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.pre = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.stage1 = LinearBottleNeck(<span class="number">32</span>, <span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.stage2 = self._make_stage(<span class="number">2</span>, <span class="number">16</span>, <span class="number">24</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line">        self.stage3 = self._make_stage(<span class="number">3</span>, <span class="number">24</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line">        self.stage4 = self._make_stage(<span class="number">4</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line">        self.stage5 = self._make_stage(<span class="number">3</span>, <span class="number">64</span>, <span class="number">96</span>, <span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line">        self.stage6 = self._make_stage(<span class="number">3</span>, <span class="number">96</span>, <span class="number">160</span>, <span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line">        self.stage7 = LinearBottleNeck(<span class="number">160</span>, <span class="number">320</span>, <span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">320</span>, <span class="number">1280</span>, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">1280</span>),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">1280</span>, class_num, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pre(x)</span><br><span class="line">        x = self.stage1(x)</span><br><span class="line">        x = self.stage2(x)</span><br><span class="line">        x = self.stage3(x)</span><br><span class="line">        x = self.stage4(x)</span><br><span class="line">        x = self.stage5(x)</span><br><span class="line">        x = self.stage6(x)</span><br><span class="line">        x = self.stage7(x)</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.adaptive_avg_pool2d(x, <span class="number">1</span>)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_stage</span>(<span class="params">self, repeat, in_channels, out_channels, stride, t</span>):</span></span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> repeat - <span class="number">1</span>:</span><br><span class="line">            layers.append(LinearBottleNeck(out_channels, out_channels, <span class="number">1</span>, t))</span><br><span class="line">            repeat -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mobilenetv2</span>():</span></span><br><span class="line">    <span class="keyword">return</span> MobileNetV2()</span><br></pre></td></tr></table></figure></div></div><h2 id="MobileNet-v3"><a href="#MobileNet-v3" class="headerlink" title="MobileNet v3"></a>MobileNet v3</h2><p>NAS + depthwise separable convolutions + inverted residual with linear bottleneck + SEModule + hard-swish</p><h3 id="已有的相关工作"><a href="#已有的相关工作" class="headerlink" title="已有的相关工作"></a>已有的相关工作</h3><p><code>MobileNet v1中的深度可分离卷积</code><br>将传统的卷积分为depthwise conv + pointwise conv</p><p><code>MobileNet v2中的反转残差线性瓶颈</code></p><ul><li>扩张（1x1 conv without ReLU6） $ \rightarrow $ 抽取特征（3x3 depthwise with ReLU6）$ \rightarrow $ 压缩（1x1 conv without ReLU6）</li><li>当且仅当输入输出具有相同的通道数时，才进行残差连接</li><li>在最后“压缩”完以后，没有接ReLU激活，作者认为这样会引起较大的信息损失</li><li>该结构在输入和输出处保持紧凑的表示，同时在内部扩展到更高维的特征空间，以增加非线性每通道变换的表现力</li></ul><p><code>融入SENet的思想</code><br>与SE-ResBlock相比，不同点在于SE-ResBlock的SE layer加在最后一个1x1卷积后，而MnasNet的SE layer加在depthwise卷积之后，也就是在通道数最多的feature map上做Attention。</p><h3 id="NAS-NetAdapt"><a href="#NAS-NetAdapt" class="headerlink" title="NAS-NetAdapt"></a>NAS-NetAdapt</h3><p>首先使用了神经网络搜索功能来构建全局的网络结构，随后利用了NetAdapt算法来对每层的核数量进行优化。对于全局的网络结构搜索，研究人员使用了与Mnasnet中相同的，基于RNN的控制器和分级的搜索空间，并针对特定的硬件平台进行精度-延时平衡优化，在目标延时(~80ms)范围内进行搜索。随后利用NetAdapt方法来对每一层按照序列的方式进行调优。在尽量优化模型延时的同时保持精度，减小扩充层和每一层中瓶颈的大小。</p><h3 id="swish激活函数"><a href="#swish激活函数" class="headerlink" title="swish激活函数"></a>swish激活函数</h3><p>$$<br>\text{swish} (x)=x \cdot \sigma(x)<br>$$</p><p>swish激活函数已经被证明是一种ReLU更佳的非线性激活，但是相比ReLU，它的计算更复杂，因为有sigmoid函数。为了能够在移动设备上应用swish，并降低它的计算开销，作者做了两个改进, 第一个改进是使用 hard-swish, 即用分段函数来模拟sigmoid函数</p><p>$$<br>\mathrm{h}-\mathrm{swish}[x]=x \frac{\operatorname{ReLU} 6(x+3)}{6}<br>$$</p><p>第二个改进是<strong>延迟使用h-swish</strong>, 随着网络的加深，feature map分辨率逐渐变小，在此上面应用非线性激活的成本逐渐降低。所以在MobileNet-v3模型设计上，作者刻意将h-swish使用在网络靠后的部分上。</p><p>尽管使用h-swish相比ReLU还是会带来一定的延迟，但相对于其带来的精度的提升是积极的。而且量化版的h-swish还可以进一步通过软件优化：一旦将Sigmoid转化为分段线性函数，那么实际计算的时候大部分的开销都转变成对内存的访问开销，这是可以通过与前一层进行融合来消除的。</p><h3 id="SE-Module"><a href="#SE-Module" class="headerlink" title="SE Module"></a>SE Module</h3><p>具体的SE Module 会在另一篇文章中给出, 这里与MnasNet相同，在depthwise卷积后进行SE操作, 目的是为了在通道数最多的feature map上做Attention，不同点在于使用了新的非线性。</p><h3 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h3><div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>查看代码</span></div>    <div class="hide-content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">hswish</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = x * F.relu6(x + <span class="number">3</span>, inplace=<span class="literal">True</span>) / <span class="number">6</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">hsigmoid</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = F.relu6(x + <span class="number">3</span>, inplace=<span class="literal">True</span>) / <span class="number">6</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeModule</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_size, reduction=<span class="number">4</span></span>):</span></span><br><span class="line">        super(SeModule, self).__init__()</span><br><span class="line">        self.se = nn.Sequential(</span><br><span class="line">            nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(in_size, in_size // reduction, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(in_size // reduction),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_size // reduction, in_size, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(in_size),</span><br><span class="line">            hsigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x * self.se(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;expand + depthwise + pointwise&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, kernel_size, in_size, expand_size, out_size, nolinear, semodule, stride</span>):</span></span><br><span class="line">        super(Block, self).__init__()</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.se = semodule</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(expand_size)</span><br><span class="line">        self.nolinear1 = nolinear</span><br><span class="line">        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride, padding=kernel_size//<span class="number">2</span>, groups=expand_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(expand_size)</span><br><span class="line">        self.nolinear2 = nolinear</span><br><span class="line">        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(out_size)</span><br><span class="line"></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> stride == <span class="number">1</span> <span class="keyword">and</span> in_size != out_size:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_size, out_size, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(out_size),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.nolinear1(self.bn1(self.conv1(x)))</span><br><span class="line">        out = self.nolinear2(self.bn2(self.conv2(out)))</span><br><span class="line">        out = self.bn3(self.conv3(out))</span><br><span class="line">        <span class="keyword">if</span> self.se != <span class="literal">None</span>:</span><br><span class="line">            out = self.se(out)</span><br><span class="line">        out = out + self.shortcut(x) <span class="keyword">if</span> self.stride==<span class="number">1</span> <span class="keyword">else</span> out</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MobileNetV3_Large</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span></span>):</span></span><br><span class="line">        super(MobileNetV3_Large, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        self.hs1 = hswish()</span><br><span class="line"></span><br><span class="line">        self.bneck = nn.Sequential(</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, nn.ReLU(inplace=<span class="literal">True</span>), <span class="literal">None</span>, <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">24</span>, nn.ReLU(inplace=<span class="literal">True</span>), <span class="literal">None</span>, <span class="number">2</span>),</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">24</span>, <span class="number">72</span>, <span class="number">24</span>, nn.ReLU(inplace=<span class="literal">True</span>), <span class="literal">None</span>, <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">24</span>, <span class="number">72</span>, <span class="number">40</span>, nn.ReLU(inplace=<span class="literal">True</span>), SeModule(<span class="number">40</span>), <span class="number">2</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">40</span>, <span class="number">120</span>, <span class="number">40</span>, nn.ReLU(inplace=<span class="literal">True</span>), SeModule(<span class="number">40</span>), <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">40</span>, <span class="number">120</span>, <span class="number">40</span>, nn.ReLU(inplace=<span class="literal">True</span>), SeModule(<span class="number">40</span>), <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">40</span>, <span class="number">240</span>, <span class="number">80</span>, hswish(), <span class="literal">None</span>, <span class="number">2</span>),</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">80</span>, <span class="number">200</span>, <span class="number">80</span>, hswish(), <span class="literal">None</span>, <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">80</span>, <span class="number">184</span>, <span class="number">80</span>, hswish(), <span class="literal">None</span>, <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">80</span>, <span class="number">184</span>, <span class="number">80</span>, hswish(), <span class="literal">None</span>, <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">80</span>, <span class="number">480</span>, <span class="number">112</span>, hswish(), SeModule(<span class="number">112</span>), <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">112</span>, <span class="number">672</span>, <span class="number">112</span>, hswish(), SeModule(<span class="number">112</span>), <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">112</span>, <span class="number">672</span>, <span class="number">160</span>, hswish(), SeModule(<span class="number">160</span>), <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">160</span>, <span class="number">672</span>, <span class="number">160</span>, hswish(), SeModule(<span class="number">160</span>), <span class="number">2</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">160</span>, <span class="number">960</span>, <span class="number">160</span>, hswish(), SeModule(<span class="number">160</span>), <span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">160</span>, <span class="number">960</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">960</span>)</span><br><span class="line">        self.hs2 = hswish()</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">960</span>, <span class="number">1280</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(<span class="number">1280</span>)</span><br><span class="line">        self.hs3 = hswish()</span><br><span class="line">        self.linear4 = nn.Linear(<span class="number">1280</span>, num_classes)</span><br><span class="line">        self.init_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</span><br><span class="line">                init.normal_(m.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.hs1(self.bn1(self.conv1(x)))</span><br><span class="line">        out = self.bneck(out)</span><br><span class="line">        out = self.hs2(self.bn2(self.conv2(out)))</span><br><span class="line">        out = F.avg_pool2d(out, <span class="number">7</span>)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = self.hs3(self.bn3(self.linear3(out)))</span><br><span class="line">        out = self.linear4(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MobileNetV3_Small</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span></span>):</span></span><br><span class="line">        super(MobileNetV3_Small, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        self.hs1 = hswish()</span><br><span class="line"></span><br><span class="line">        self.bneck = nn.Sequential(</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, nn.ReLU(inplace=<span class="literal">True</span>), SeModule(<span class="number">16</span>), <span class="number">2</span>),</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">16</span>, <span class="number">72</span>, <span class="number">24</span>, nn.ReLU(inplace=<span class="literal">True</span>), <span class="literal">None</span>, <span class="number">2</span>),</span><br><span class="line">            Block(<span class="number">3</span>, <span class="number">24</span>, <span class="number">88</span>, <span class="number">24</span>, nn.ReLU(inplace=<span class="literal">True</span>), <span class="literal">None</span>, <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">24</span>, <span class="number">96</span>, <span class="number">40</span>, hswish(), SeModule(<span class="number">40</span>), <span class="number">2</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">40</span>, <span class="number">240</span>, <span class="number">40</span>, hswish(), SeModule(<span class="number">40</span>), <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">40</span>, <span class="number">240</span>, <span class="number">40</span>, hswish(), SeModule(<span class="number">40</span>), <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">40</span>, <span class="number">120</span>, <span class="number">48</span>, hswish(), SeModule(<span class="number">48</span>), <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">48</span>, <span class="number">144</span>, <span class="number">48</span>, hswish(), SeModule(<span class="number">48</span>), <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">48</span>, <span class="number">288</span>, <span class="number">96</span>, hswish(), SeModule(<span class="number">96</span>), <span class="number">2</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">96</span>, <span class="number">576</span>, <span class="number">96</span>, hswish(), SeModule(<span class="number">96</span>), <span class="number">1</span>),</span><br><span class="line">            Block(<span class="number">5</span>, <span class="number">96</span>, <span class="number">576</span>, <span class="number">96</span>, hswish(), SeModule(<span class="number">96</span>), <span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">96</span>, <span class="number">576</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">576</span>)</span><br><span class="line">        self.hs2 = hswish()</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">576</span>, <span class="number">1280</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(<span class="number">1280</span>)</span><br><span class="line">        self.hs3 = hswish()</span><br><span class="line">        self.linear4 = nn.Linear(<span class="number">1280</span>, num_classes)</span><br><span class="line">        self.init_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</span><br><span class="line">                init.normal_(m.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.hs1(self.bn1(self.conv1(x)))</span><br><span class="line">        out = self.bneck(out)</span><br><span class="line">        out = self.hs2(self.bn2(self.conv2(out)))</span><br><span class="line">        out = F.avg_pool2d(out, <span class="number">7</span>)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = self.hs3(self.bn3(self.linear3(out)))</span><br><span class="line">        out = self.linear4(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    net = MobileNetV3_Small()</span><br><span class="line">    x = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">    y = net(x)</span><br><span class="line">    print(y.size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># test()</span></span><br></pre></td></tr></table></figure></div></div><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ul><li><a href="https://blog.csdn.net/weixin_44474718/article/details/91045521">https://blog.csdn.net/weixin_44474718/article/details/91045521</a></li><li><a href="https://blog.csdn.net/weixin_39875161/article/details/90052644">https://blog.csdn.net/weixin_39875161/article/details/90052644</a></li><li><a href="https://blog.csdn.net/u010712012/article/details/94888053">https://blog.csdn.net/u010712012/article/details/94888053</a></li><li><a href="https://www.zhihu.com/question/265709710">https://www.zhihu.com/question/265709710</a></li><li><a href="https://blog.csdn.net/weixin_44474718/article/details/91045521">https://blog.csdn.net/weixin_44474718/article/details/91045521</a></li><li><a href="https://blog.csdn.net/DL_wly/article/details/90168883">https://blog.csdn.net/DL_wly/article/details/90168883</a></li><li><a href="https://blog.csdn.net/sinat_37532065/article/details/90813655">https://blog.csdn.net/sinat_37532065/article/details/90813655</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Backbone </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Lightweight Network </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
