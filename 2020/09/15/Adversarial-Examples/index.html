<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Paper List About Adversarial Examples | Ming</title><meta name="keywords" content="Adversarial"><meta name="author" content="Ming Li"><meta name="copyright" content="Ming Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Paper List and AbstractYou can click on each paper title to access the detailed information of the paper. CVPR2019(26 papers)A Kernelized Manifold Mapping to Diminish the Effect of Adversarial Perturb">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper List About Adversarial Examples">
<meta property="og:url" content="https://coderming.cn/2020/09/15/Adversarial-Examples/index.html">
<meta property="og:site_name" content="Ming">
<meta property="og:description" content="Paper List and AbstractYou can click on each paper title to access the detailed information of the paper. CVPR2019(26 papers)A Kernelized Manifold Mapping to Diminish the Effect of Adversarial Perturb">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Adversarial-Examples/avatar.png">
<meta property="article:published_time" content="2020-09-15T07:46:57.000Z">
<meta property="article:modified_time" content="2020-09-23T04:16:22.031Z">
<meta property="article:author" content="Ming Li">
<meta property="article:tag" content="Adversarial">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Adversarial-Examples/avatar.png"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://coderming.cn/2020/09/15/Adversarial-Examples/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-09-23 12:16:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">19</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/images/"><i class="fa-fw fas fa-images"></i><span> Images</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Paper-List-and-Abstract"><span class="toc-number">1.</span> <span class="toc-text">Paper List and Abstract</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CVPR"><span class="toc-number">1.1.</span> <span class="toc-text">CVPR</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2019-26-papers"><span class="toc-number">1.1.1.</span> <span class="toc-text">2019(26 papers)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2020-34-papers"><span class="toc-number">1.1.2.</span> <span class="toc-text">2020(34 papers)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ICCV"><span class="toc-number">1.2.</span> <span class="toc-text">ICCV</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2019-24-papers"><span class="toc-number">1.2.1.</span> <span class="toc-text">2019(24 papers)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ECCV"><span class="toc-number">1.3.</span> <span class="toc-text">ECCV</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2020-29-papers"><span class="toc-number">1.3.1.</span> <span class="toc-text">2020(29 papers)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ICLR"><span class="toc-number">1.4.</span> <span class="toc-text">ICLR</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2019-10-papers"><span class="toc-number">1.4.1.</span> <span class="toc-text">2019(10 papers)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2020-26-papers"><span class="toc-number">1.4.2.</span> <span class="toc-text">2020(26 papers)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ICML"><span class="toc-number">1.5.</span> <span class="toc-text">ICML</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2019-17-papers"><span class="toc-number">1.5.1.</span> <span class="toc-text">2019(17 papers)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2020-24-papers"><span class="toc-number">1.5.2.</span> <span class="toc-text">2020(24 papers)</span></a></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Adversarial-Examples/avatar.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ming</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/images/"><i class="fa-fw fas fa-images"></i><span> Images</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Paper List About Adversarial Examples</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-09-15T07:46:57.000Z" title="Created 2020-09-15 15:46:57">2020-09-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-09-23T04:16:22.031Z" title="Updated 2020-09-23 12:16:22">2020-09-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Adversarial/">Adversarial</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">21.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>128min</span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="Paper-List-and-Abstract"><a href="#Paper-List-and-Abstract" class="headerlink" title="Paper List and Abstract"></a>Paper List and Abstract</h1><p>You can click on each paper title to access the detailed information of the paper.</p>
<h2 id="CVPR"><a href="#CVPR" class="headerlink" title="CVPR"></a>CVPR</h2><h3 id="2019-26-papers"><a href="#2019-26-papers" class="headerlink" title="2019(26 papers)"></a>2019(26 papers)</h3><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Taghanaki_A_Kernelized_Manifold_Mapping_to_Diminish_the_Effect_of_Adversarial_CVPR_2019_paper.html"><strong><code>A Kernelized Manifold Mapping to Diminish the Effect of Adversarial Perturbations</code></strong></a></p>
<p>Propose a non-linear radial basis convolutional feature mapping by <strong>learning a Mahalanobis-like distance function</strong>, then maps the convolutional features onto a linearly well-separated manifold, which prevents small adversarial perturbations from forcing a sample to cross the decision boundary.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_Adversarial_Attacks_Beyond_the_Image_Space_CVPR_2019_paper.html"><strong><code>Adversarial Attacks Beyond the Image Space</code></strong></a></p>
<p>In the contexts of classification and visual question answering, we augment CNNs receive 2D input images with a rendering module (either differentiable or not) in front, so that  a 3D scene (in physical space) is rendered to a 2D image (in the image space), and then mapped to a prediction (in the output space).</p>
<p><strong>This paper demonstrates that the adversarial perturbations can now go beyond the image space and have clear meanings in the 3D physical world</strong>. Though the image-space adversaries can be interpreted as per-pixel albedo change, we verify that they cannot be well explained along these physically meaningful dimensions, which often have a non-local effect. But it is still possible to successfully attack beyond the image space on the physical space, thought it is more difficult than image-space attack, reflected in lower success rate and heavier perturbations required.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Adversarial_Defense_by_Stratified_Convolutional_Sparse_Coding_CVPR_2019_paper.html"><strong><code>Adversarial Defense by Stratified Convolutional Sparse Coding</code></strong></a></p>
<p>Based on convolutional sparse coding, this paper <strong>constructs a stratified low-dimensional quasi-natural image space</strong>, the quasi-natural space can not only faithfully approximates the nature image space but also removing adversarial perturbations. The method shows SOTA performance compared to other  attack-agnostic adversarial defense methods.</p>
<p>Introduce a novel Sparse Transformation Layer (STL) between the input image and the first layer of the neural network to efficiently project images into our quasi-natural image space.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Qiu_Adversarial_Defense_Through_Network_Profiling_Based_Path_Extraction_CVPR_2019_paper.html"><strong><code>Adversarial Defense Through Network Profiling Based Path Extraction</code></strong></a></p>
<p>Recent work has shown the effectiveness of decomposing DNN models to decomposed functional blocks for defending adversarial attacks.</p>
<p>This work <strong>proposes a profiling-based method to decompose the DNN models to different functional blocks</strong>, which lead to the effective path as a new approach to explore DNNs’ internal organizations.</p>
<p>The per-image effective path can be aggregated to the class-level effective path, through which we observe that adversarial images activate effective path different from normal images. So proposing an effective path similarity-based method to detect adversarial images with an interpretable model, which achieve better accuracy and broader applicability.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Ho_Catastrophic_Childs_Play_Easy_to_Perform_Hard_to_Defend_Adversarial_CVPR_2019_paper.html"><strong><code>Catastrophic Child’s Play: Easy to Perform, Hard to Defend Adversarial Attacks</code></strong></a></p>
<p><strong>This paper discusses the perceptibility of attack perturbations and confirm the hypothesis that none of the existing defenses is found effective against the attacks.</strong></p>
<p>It is argued that perceptibility depends on context, and a distinction is made between imperceptible and semantically imperceptible perturbations. While the former survives in image comparisons, the latter are perceptible but have no impact on human object recognition.</p>
<p>A procedure is proposed to determine the perceptibility of perturbations using Turk experiments, and a dataset of both perturbation classes which enables replicable studies of object manipulation attacks, is assembled.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Jia_ComDefend_An_Efficient_Image_Compression_Model_to_Defend_Adversarial_Examples_CVPR_2019_paper.html"><strong><code>ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples</code></strong></a></p>
<p>This paper <strong>proposes an end-to-end image compression model to defend adversarial examples</strong>. <strong>The proposed model consists of a compression module (ComCNN) and a reconstruction module (RecCNN)</strong>. The ComCNN is used to maintain the structure information of the original image and purity adversarial perturbations. The RecCNN is used to reconstruct the original image with high quality.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Curls__Whey_Boosting_Black-Box_Adversarial_Attacks_CVPR_2019_paper.html"><strong><code>Curls &amp; Whey: Boosting Black-Box Adversarial Attacks</code></strong></a></p>
<p>Existing black-box iterative attacks have two defects:</p>
<ol>
<li>These attacks add noises monotonically along the direction of gradient ascent, resulting in a lack of diversity and adaptability of the generated iterative trajectories.</li>
<li>It is trivial to perform adversarial attack by adding excessive noises, but currently there is no refinement mechanism to squeeze redundant noises.</li>
</ol>
<p>So this paper <strong>proposes Curl &amp; Whey black-box attack to fix the above two defects.</strong></p>
<p>During Curl iteration, by combining gradient descent and ascent, we ‘curl’ up iterative trajectories to integrate more diversity and transferability into adversarial examples. Curls iteration also alleviates the diminishing marginal effect in existing iterative attacks.</p>
<p>The Whey optimization further squeezes the ‘whey’ of noises by exploiting the robustness of adversarial perturbation</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Rony_Decoupling_Direction_and_Norm_for_Efficient_Gradient-Based_L2_Adversarial_Attacks_CVPR_2019_paper.html"><strong><code>Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses</code></strong></a></p>
<p>Considering L2 norm distortions, the Carlini and Wagner attack is presently the most effective white-box attack in the literature. However, this method is slow since it performs a line-search for one of the optimization terms, and often requires thousands of iterations.</p>
<p>In this paper, <strong>an <code>efficient</code> approach is proposed to generate gradient-based attacks that induce misclassifications with low L2 norm</strong>, by decoupling the direction and the norm of the adversarial perturbation that is added to the image.Models trained with our attack achieve state-of-the-art robustness against white box gradient-based L2 attacks.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Taran_Defending_Against_Adversarial_Attacks_by_Randomized_Diversification_CVPR_2019_paper.html"><strong><code>Defending against adversarial attacks by randomized diversification</code></strong></a></p>
<p>We propose a randomized diversification as a defense strategy. <em>This strategy is used to gray-box scenario(situation), which assumes that the architecture of the classifier and the training data set are known to the attacker. What’s more, attacker have access to the internal system and to a secret key at the test time.</em>  <code>That&#39;s totally bullshit! If attacker can get all of this, you should call police right now rather than training a robust model......</code></p>
<p><strong>The defender processes an input in multiple channels.Each channel introduces its own randomization in a special transform domain based on a secret key shared between the training and testing stages.</strong></p>
<p>Such a transform based randomization with a shared key preserves the gradients in key-defined sub-spaces for the defender but it prevents gradient back propagation and the creation of various bypass systems for the attacker. An additional benefit of multi-channel randomization is the aggregation that fuses soft-outputs from all channels, thus increasing the reliability of the final score. The sharing of a secret key creates an information advantage to the defender.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Dubey_Defense_Against_Adversarial_Images_Using_Web-Scale_Nearest-Neighbor_Search_CVPR_2019_paper.html"><strong><code>Defense Against Adversarial Images using Web-Scale Nearest-Neighbor Search</code></strong></a></p>
<p>We hypothesize that adversarial perturbations fool the DNNs by moving the image away from the image manifold, because there exist no physical process that could have produced the adversarial image. This hypothesis suggest that a good defense mechanism should aim to project the adversarial images back onto the image manifold.</p>
<p>This paper <strong>study such defense mechanism, which approximate the projection onto the unknown image manifold by a nearest neighbor search against a web-scale image database containing tens of billions of images</strong>.(通过对含数百亿个图像的数据库进行最邻近搜索来预测未知图像的 image manifold)</p>
<p>It is very effective in attack settings in which the adversary does not have access to the image database. This paper propose two attack methods to break nearest neighbor defenses, and demonstrate conditions under which nearest-neighbor defense fails. With a series of experiments, there a tradeoff between robustness and accuracy in our defense.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Detection_Based_Defense_Against_Adversarial_Examples_From_the_Steganalysis_Point_CVPR_2019_paper.html"><strong><code>Detection based Defense against Adversarial Examples from the Steganalysis Point of View</code></strong></a></p>
<p><strong>This paper point out that steganalysis  can be applied to adversarial examples detection, and propose a method to enhance steganalysis features by estimating the probability of modifications</strong> caused by adversity attacks. Due to this method is not based on  DNN but based on high-dimensional manual-crafted feature, secondary attacks are hard to be directly performed to the model.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Stutz_Disentangling_Adversarial_Robustness_and_Generalization_CVPR_2019_paper.html"><strong><code>Disentangling Adversarial Robustness and Generalization</code></strong></a></p>
<p><strong>Study the relationship between model’s robustness and generalization on an underline, low-dimensional data manifold</strong>, results shows that:</p>
<ol>
<li>regular adversarial examples leave the manifold</li>
<li>adversarial examples constrained to the manifold (i.e. on-manifold adversarial examples) exist</li>
<li>on–manifold adversarial examples are generalization errors, and on-manifold adversarial training boosts generalization</li>
<li>regular robustness and generalization are not necessarily contradicting goals</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Efficient_Decision-Based_Black-Box_Adversarial_Attacks_on_Face_Recognition_CVPR_2019_paper.html"><strong><code>Efficient Decision-based Black-box Adversarial Attacks on Face Recognition --Jun Zhu</code></strong></a></p>
<p>Evaluate the robustness of SOTA face recognition models in the decision-based black-box attack setting. (the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model).</p>
<p>To improve the efficiency of previous methods, <strong>we propose an evolutionary attack algorithm, which can model the local geometry of the search directions and reduce the dimension of the search space.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Evading_Defenses_to_Transferable_Adversarial_Examples_by_Translation-Invariant_Attacks_CVPR_2019_paper.html"><strong><code>Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks --Jun Zhu</code></strong></a></p>
<p>We <strong>propose a translation-invariant attack method to generate more transferable adversarial examples</strong> against the defense models.</p>
<p>By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability.</p>
<p>To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Exact_Adversarial_Attack_to_Image_Captioning_via_Structured_Output_Learning_CVPR_2019_paper.html"><strong><code>Exact Adversarial Attack to Image Captioning via Structured Output Learning with Latent Variables</code></strong></a></p>
<p><strong>Study the robustness of a CNN+RNN based image captioning system</strong>, specifically, propose to fool an image captioning system to generate some targeted partial captions for an adversarial example.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Feature_Denoising_for_Improving_Adversarial_Robustness_CVPR_2019_paper.html"><strong><code>Feature Denoising for Improving Adversarial Robustness --Kaiming He</code></strong></a></p>
<p>Why model cannot correctly recognize the image is that adversarial perturbations lead to noise in the feature constructed by the networks, so denoising the noise in feature is crucial to increase adversarial robustness.</p>
<p><strong>This paper propose new architectures to denoising the noises in feature.</strong> Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end.</p>
<p>The proposed feature denoising networks can improve the SOTA robustness in both white-box and black-box attack settings combined with adversarial training.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Feature_Distillation_DNN-Oriented_JPEG_Compression_Against_Adversarial_Examples_CVPR_2019_paper.html"><strong><code>Feature Distillation DNN-Oriented JPEG Compression Against Adversarial Examples</code></strong></a></p>
<p>Prior compression-based works mainly rely on directly tuning parameters like compression rate, to blindly reduce image features, thereby lacking guarantee on both efficiency and accuracy of benign images after applying defense methods.</p>
<p><strong>We propose a JPEG-based defensive compression framework to effectively rectify adversarial examples without impacting classification accuracy on benign data.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.html"><strong><code>Feature Space Perturbations Yield More Transferable Adversarial Examples</code></strong></a></p>
<p>This work <strong>describes a transfer-based black-box targeted attack of deep deep feature space representations</strong>, the attack also provides insights into cross-model class representations of deep CNNs.</p>
<p>The attack is explicitly designed for transferability, driving feature space representation of a source image at layer $L$ towards the representation of a target image at layer $L$.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Improving_Transferability_of_Adversarial_Examples_With_Input_Diversity_CVPR_2019_paper.html"><strong><code>Improving Transferability of Adversarial Examples with Input Diversity</code></strong></a></p>
<p>Most of the existing attacks only achieve low success rates under the black-box setting. To this end, we <strong>propose to improve the transferability of adversarial examples by creating diverse input patterns.</strong></p>
<p>Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Xiao_MeshAdv_Adversarial_Meshes_for_Visual_Recognition_CVPR_2019_paper.html"><strong><code>MeshAdv: Adversarial Meshes for Visual Recognition --Jia Deng</code></strong></a></p>
<p>Currently, the majority of these studies have focused on perturbation added to image pixels, while such manipulation is not physically realistic. So this work propose to <strong>add adversarial shape/texture on 3D object to achieve attack.</strong></p>
<p><strong>This work proposes meshAdv to generate “adversarial 3D meshes” from objects that have rich shape features but minimal textural variation.</strong></p>
<p>To manipulate the shape or texture of the objects, we make use of a differentiable renderer to compute accurate shading on the shape and propagate the gradient.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/He_Parametric_Noise_Injection_Trainable_Randomness_to_Improve_Deep_Neural_Network_CVPR_2019_paper.html"><strong><code>Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial Attack</code></strong></a></p>
<p>Training the network with Gaussian noise is an effective technique to perform model regularization, thus improving model robustness against input variation. Inspired by this idea, we explore to <strong>utilize the regularization characteristic of noise injection to improve DNN’s  robustness against adversarial attack.</strong></p>
<p>we propose ParametricNoise-Injection (PNI) which involves trainable Gaussian noise injection at each layer on either activation or weights through solving the Min-Max optimization problem, embedded with adversarial training. These parameters are trained explicitly to achieve improved robustness.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Retrieval-Augmented_Convolutional_Neural_Networks_Against_Adversarial_Examples_CVPR_2019_paper.html"><strong><code>Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples</code></strong></a></p>
<p><strong>Propose a retrieval-augmented convolutional network (RaCNN) and propose to train it with local mixup, using them to deal with <code>off-manifold</code> and <code>on-manifold</code> adversarial examples respectively</strong>, local mixup is a novel variant of the recently proposed mixup algorithm.</p>
<p>The proposed hybrid architecture combining a convolutional network and an off-the-shelf retrieval engine was designed to mitigate the adverse effect of <code>off-manifold</code> adversarial examples.</p>
<p>The proposed local mixup addresses <code>on-manifold</code> ones by explicitly encouraging the classifier to locally behave linearly on the data manifold.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Wicker_Robustness_of_3D_Deep_Learning_in_an_Adversarial_Setting_CVPR_2019_paper.html"><strong><code>Robustness of 3D Deep Learning in an Adversarial Setting</code></strong></a></p>
<p>Another work about the robustness of 3D deep learning…(3rd in CVPR 2019)</p>
<p><strong>we develop an algorithm for analysis of point-wise robustness of neural networks that operate on 3D data.</strong> We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness.</p>
<p>Then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Theagarajan_ShieldNets_Defending_Against_Adversarial_Attacks_Using_Probabilistic_Adversarial_Robustness_CVPR_2019_paper.html"><strong><code>ShieldNets: Defending Against Adversarial Attacks Using Probabilistic Adversarial Robustness</code></strong></a></p>
<p>Probabilistic adversarial robustness (PAR), as a theoretical framework, is introduced to neutralize adversarial attacks by concentrating sample probability to adversarial-free zones.</p>
<p><strong>Unlike the most of the existing defense mechanisms that require modifying the architecture/training of the target classifier.PAR is designed in the first place to provide proactive protection to an existing fixed model.</strong></p>
<p>ShieldNet is implemented as a demonstration of PAR in this work by using PixelCNN.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Yao_Trust_Region_Based_Adversarial_Attack_on_Neural_Networks_CVPR_2019_paper.html"><strong><code>Trust Region Based Adversarial Attack on Neural Networks</code></strong></a></p>
<p>Current state-of-the-art adversarial attack methods typically require very time consuming hyper-parameter tuning, or require many iterations to solve an optimization based adversarial attack.</p>
<p>To address this problem, <strong>we present a new family of trust region based adversarial attacks, with the goal of computing adversarial perturbations efficiently. We propose several attacks based on variants of the trust region optimization method.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Corneanu_What_Does_It_Mean_to_Learn_in_Deep_Networks_And_CVPR_2019_paper.html"><strong><code>What does it mean to learn in deep networks? And, how does one detect adversarial attacks?</code></strong></a></p>
<p><strong>we derive a novel approach to define what it means to learn in deep networks, and how to use this knowledge to detect adversarial attacks.</strong></p>
<p>Study how to define the ability of a network to generalize to unseen testing samples and, most importantly, why this is the case.</p>
<h3 id="2020-34-papers"><a href="#2020-34-papers" class="headerlink" title="2020(34 papers)"></a>2020(34 papers)</h3><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Naseer_A_Self-supervised_Approach_for_Adversarial_Robustness_CVPR_2020_paper.html"><strong><code>A Self-supervised Approach for Adversarial Robustness</code></strong></a></p>
<p>Transferability of adversarial examples demand generalizable defenses that can provide cross-task protection, but adversarial training lack such generalization because adversarial training enhances robustness by only modifying target model’s parameters. On the of hand, different input processing based defenses cannot defend continuously evolving attacks.</p>
<p><strong>We take the first step to combine the benefits of both approaches and propose a self-supervised adversarial training mechanism in the input space.</strong> our defense is a generalizable approach and provides significant robustness against the unseen adversarial attacks.</p>
<p>The results show that  our defense is a generalizable approach and provides significant robustness against the unseen adversarial attacks.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Gowal_Achieving_Robustness_in_the_Wild_via_Adversarial_Mixing_With_Disentangled_CVPR_2020_paper.html"><strong><code>Achieving Robustness in the Wild via Adversarial Mixing with Disentangled Representations --DeepMind</code></strong></a></p>
<p>Adversarial training’s application has been limited to enforcing invariance to analytically defined transformations like $ℓ_p$ p-norm bounded perturbations. Such perturbations do not necessarily cover plausible real-world variations that preserve the semantics of the input.(对抗训练的应用场景仅限于对定义好的变换实施不变性, 但这样的扰动变换不一定必须包括真实世界中能保留语义的变换, 例如化妆或者改变光照条件之后的图片不应该被视为对抗样本.)</p>
<p><strong>In this paper, we propose a novel approach to express and formalize robustness to these kinds of real-world transformations of the input.</strong>(我们提出了一种方法用于表达和形式化这些真实世界中存在的变换的鲁棒性)</p>
<p>The two key ideas underlying our formulation are (1) leveraging disentangled representations of the input to define different factors of variations(利用输入的解耦表示来定义变换的不同因素), and (2) generating new input images by adversarially composing the representations of different images(通过对抗地组合不同图像的表示来生成新的输入图像)</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Duan_Adversarial_Camouflage_Hiding_Physical-World_Attacks_With_Natural_Styles_CVPR_2020_paper.html"><strong><code>Adversarial Camouflage: Hiding Physical-World Attacks with Natural Styles</code></strong></a></p>
<p>Existing works have mostly focused on either digital adversarial examples created via small and imperceptible perturbations, or physical-world adversarial examples created with large and less realistic distortions that are easily identified by human observers.</p>
<p><strong>We propose a novel approach to craft and camouflage physical world adversarial examples into natural styles that appear legitimate to human observers. AdvCam transfers large adversarial perturbations into customized styles, which are then “hidden” on-target object or off-target background.</strong></p>
<p>The results show that in both digital and physical-world scenarios, adversarial examples crafted by AdvCam are well camouflaged and highly stealthy, while remaining effective in fooling state-of-the-art DNN image classifiers.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Xie_Adversarial_Examples_Improve_Image_Recognition_CVPR_2020_paper.html"><strong><code>Adversarial Examples Improve Image Recognition</code></strong></a></p>
<p>Here we present an opposite perspective: <strong>adversarial examples can be used to improve image recognition models if harnessed in the right manner.</strong></p>
<p>We propose AdvProp, an enhanced adversarial training scheme which <strong>treats adversarial examples as additional examples, to prevent overfitting to improve performance in image recognition.</strong></p>
<p><strong>Key to our method is the usage of a separate auxiliary(辅助的) batch norm for adversarial examples, as they have different underlying distributions to normal examples.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Adversarial_Robustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_paper.html"><strong><code>Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning</code></strong></a></p>
<p>Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy, but <strong>gaining robustness from pretraining is left unexplored.</strong></p>
<p><strong>We introduce adversarial training into selfsupervision, to provide general-purpose robust pretrained models for the first time.</strong></p>
<p>We find these robust pretrained models can benefit the subsequent(随后的) fine-tuning in two ways: (1) boosting final model robustness. (2) saving the computation cost, if proceeding towards adversarial fine-tuning.</p>
<p><strong>We find that different self-supervised pretrained models have diverse adversarial vulnerability.</strong> It inspires us to ensemble several pretraining tasks, which boosts robustness more.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Lee_Adversarial_Vertex_Mixup_Toward_Better_Adversarially_Robust_Generalization_CVPR_2020_paper.html"><strong><code>Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization</code></strong></a></p>
<p>There is a large gap exists between test accuracy and training accuracy in adversarial training. <strong>we identify Adversarial Feature Overfitting (AFO), which may cause poor adversarially robust generalization</strong>, and <strong>we show that adversarial training can overshoot(超过) the optimal point in terms of robust generalization, leading to AFO in our simple Gaussian model.</strong></p>
<p><strong>We present soft labeling as a solution to the AFO problem.</strong> Furthermore, <strong>we propose Adversarial Vertex mixup (AVmixup), a soft-labeled data augmentation approach for improving adversarially robust generalization.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Dong_Benchmarking_Adversarial_Robustness_on_Image_Classification_CVPR_2020_paper.html"><strong><code>Benchmarking Adversarial Robustness on Image Classification --Jun Zhu</code></strong></a></p>
<p>In this paper, <strong>we establish a comprehensive, rigorous, and coherent <code>benchmark</code> to evaluate adversarial robustness on image classification tasks.</strong></p>
<p>We perform large-scale experiments with two robustness curves as the fair-minded evaluation criteria to fully understand the performance ofthese methods(我们以两条稳健性曲线作为公正的评估标准进行大规模实验，以充分了解这些方法的性能)</p>
<p>we draw several important findings that can provide insights for future research, including:</p>
<ol>
<li><strong>The relative robustness between models can change across different attack configurations, thus it is encouraged to adopt the robustness curves to evaluate adversarial robustness.</strong></li>
<li><strong>As one of the most effective defense techniques, adversarial training can generalize across different threat models.</strong></li>
<li><strong>Randomization-based defenses are more robust to query-based black-box attacks.</strong></li>
</ol>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Wu_Boosting_the_Transferability_of_Adversarial_Samples_via_Attention_CVPR_2020_paper.html"><strong><code>Boosting the Transferability of Adversarial Samples via Attention</code></strong></a></p>
<p>This paper aims to improve the transferability of adversarial examples. <strong>The synthesized adversarial samples often achieve limited success due to overfitting to the local model employed.</strong></p>
<p><strong>We propose a novel mechanism to alleviate(缓和) the overfitting issue. It computes model attention over extracted features to regularize the search of adversarial examples(通过提取到的特征来计算模型的attention, 以此来调整对抗样本的生成), which prioritizes the corruption of critical features that are likely to be adopted by diverse architectures(这种做法保证了关键特征中的对抗信息能够被不同的网络结构所适应.)</strong></p>
<p>It can promote the transferability of resultant adversarial instances, its superiority to state-of-the-art benchmarks in both white-box and black-box settings.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Shamsabadi_ColorFool_Semantic_Adversarial_Colorization_CVPR_2020_paper.html"><strong><code>ColorFool: Semantic Adversarial Colorization</code></strong></a></p>
<p>Adversarial attacks that generate small Lp-norm perturbations to mislead classifiers have limited success in black-box settings and with unseen classifiers. Instead, adversarial attacks that generate unrestricted perturbations are more robust to defenses, are generally more successful in black-box settings and are more transferable to unseen classifiers. However, unrestricted perturbations may be noticeable to humans.</p>
<p><strong>we propose a content-based black-box adversarial attack that generates unrestricted perturbations by exploiting image semantics to selectively modify colors within chosen ranges that are perceived as natural by humans.</strong>(可以理解为生成一个人类看起来没有语义问题, 但实际上不受限制的扰动, 这里的限制指类似 $l_2$ 范数这种)</p>
<p>The proposed ColorFool, outperforms in terms of success rate, robustness to defense frameworks and transferability, five state-of-the-art adversarial attacks.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_DaST_Data-Free_Substitute_Training_for_Adversarial_Attacks_CVPR_2020_paper.html"><strong><code>DaST: Data-Free Substitute Training for Adversarial Attacks</code></strong></a></p>
<p>For the black-box setting, current substitute attacks need pre-trained (classification) models to generate adversarial examples. <strong>This paper propose a data-free substitute training method (DaST) to obtain substitute models</strong> for adversarial black-box attacks <strong>without the requirement of any real data.</strong></p>
<p>DaST utilizes specially designed generative adversarial networks (GANs) to train the substitute models. In particular, <strong>we design a multi-branch architecture and label-control loss for the generative model to deal with the uneven distribution of synthetic samples. The substitute model is then trained by the synthetic samples generated by the generative model, which are labeled by the attacked model subsequently</strong></p>
<p><strong>We are the first to train a substitute model for adversarial attacks without any real data.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/He_Defending_and_Harnessing_the_Bit-Flip_Based_Adversarial_Weight_Attack_CVPR_2020_paper.html"><strong><code>Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack</code></strong></a></p>
<p>Bit-Flip Attack (BFA). BFA has shown extraordinary attacking ability, where the adversary can malfunction a quantized Deep Neural Network (DNN) as a random guess, through malicious bit-flips on a small set of vulnerable weight bits.However, <strong>there are no effective defensive methods to enhance the fault-tolerance capability of DNN against such BFA.</strong></p>
<p><strong>We conduct comprehensive investigations on BFA and propose to leverage binarizationaware training and its relaxation – piece-wise clustering as simple and effective countermeasures to BFA.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Cohen_Detecting_Adversarial_Samples_Using_Influence_Functions_and_Nearest_Neighbors_CVPR_2020_paper.html"><strong><code>Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors</code></strong></a></p>
<p><strong>We present a method for detecting adversarial attacks, which is suitable for any pre-trained neural network classifier</strong></p>
<p>We <strong>use influence functions to measure the impact of every training sample on the validation set data</strong>. From the influence scores, <strong>we find the most supportive training samples for any given validation example.</strong></p>
<p><strong>A k-nearest neighbor (k-NN) model fitted on the DNN’s activation layers is  employed to search for the ranking of these supporting training samples.</strong></p>
<p>We observe that <strong>these samples are highly correlated with the nearest neighbors of the normal inputs, while this correlation is much weaker for adversarial inputs.</strong></p>
<p>We train an adversarial detector using the k-NN ranks and distances and show that it successfully distinguishes adversarial examples, getting state-of-the-art results.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zheng_Efficient_Adversarial_Training_With_Transferable_Adversarial_Examples_CVPR_2020_paper.html"><strong><code>Efficient Adversarial Training with Transferable Adversarial Examples</code></strong></a></p>
<p>This paper shows that  <strong>there is high transferability between models from neighboring epochs in the same training process.</strong>(i.e. adversarial examples from one epoch continue to be adversarial in subsequent epochs)</p>
<p>we propose a novel method, Adversarial Training with Transferable Adversarial Examples (ATTA), that <strong>can enhance the robustness of trained models and greatly improve the training efficiency <code>by accumulating adversarial perturbations through epochs</code>.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Enhancing_Intrinsic_Adversarial_Robustness_via_Feature_Pyramid_Decoder_CVPR_2020_paper.html"><strong><code>Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder</code></strong></a></p>
<p>In this paper, we propose an attack-agnostic defence framework to enhance the intrinsic robustness of neural networks, without jeopardizing(危害) the ability of generalizing clean samples.</p>
<p>This feature pyramid decoder(FPD) framework <strong>applies to all block-based CNNs.</strong></p>
<p><strong>It implants denoising and image restoration modules into a targeted CNN, and it also constraints the Lipschitz constant of the classification layer</strong></p>
<p>Moreover, we propose a two-phase strategy to train the FPD-enhanced CNN, utilizing $\epsilon$ neighborhood noisy images with multi-task and self-supervised learning.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Yuan_Ensemble_Generative_Cleaning_With_Feedback_Loops_for_Defending_Adversarial_Attacks_CVPR_2020_paper.html"><strong><code>Ensemble Generative Cleaning With Feedback Loops for Defending Adversarial Attacks</code></strong></a></p>
<p>We develop a new method called ensemble generative cleaning with feedback loops (EGC-FL) <strong>for effective defense</strong> of deep neural networks.</p>
<p>The proposed EGC-FL method is based on two central ideas:</p>
<ol>
<li>We introduce a transformed deadzone layer into the defense network, which consists of an orthonormal transform and a deadzone-based activation function, to destroy the sophisticated noise pattern of adversarial attacks.</li>
<li>By constructing a generative cleaning network with a feedback loop, we are able to generate an ensemble of diverse estimations of the original clean image.</li>
</ol>
<p>Then learn a network to fuse this set of diverse estimations together to restore the original image.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.html"><strong><code>Exploiting Joint Robustness to Adversarial Perturbations</code></strong></a></p>
<p>This paper <strong>exploits first-order interactions within ensembles to formalize a reliable and practical defense.</strong></p>
<p><strong>We introduce a scenario of interactions that certifiably improves the robustness</strong> according to the size of the ensemble, the diversity of the gradient directions, and the balance of the member’s contribution to the robustness.</p>
<p>We <strong>present a joint gradient phase and magnitude regularization (GPMR) as a vigorous approach to impose the desired scenario of interactions among members of the ensemble.</strong></p>
<p>we demonstrate that GPMR is orthogonal to other defense strategies developed for single classifiers and their combination can further improve the robustness of ensembles.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Rahmati_GeoDA_A_Geometric_Framework_for_Black-Box_Adversarial_Attacks_CVPR_2020_paper.html"><strong><code>GeoDA: a geometric framework for black-box adversarial attacks</code></strong></a></p>
<p><strong>We also obtain the optimal distribution of the queries over the iterations of the algorithm</strong>.(the adversary can only generate a small number of queries, each of them returning the top-1 label of the classifier)</p>
<p><strong>Our framework is based on the observation that the decision boundary of deep networks usually has a small mean curvature in the vicinity of data samples</strong></p>
<p><strong>We propose an effective iterative algorithm to generate query-efficient black-box perturbations with small $ℓ_p$ norms for p ≥ 1</strong>, which is confirmed via experimental evaluations on state-of-the-art natural image classifiers</p>
<p>Moreover, <strong>for p = 2, we theoretically show that our algorithm actually converges to the minimal $ℓ_2$ perturbation when the curvature of the decision boundary is bounded.</strong></p>
<p>We also obtain the optimal distribution of the queries over the iterations of the algorithm.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Haque_ILFO_Adversarial_Attack_on_Adaptive_Neural_Networks_CVPR_2020_paper.html"><strong><code>ILFO: Adversarial Attack on Adaptive Neural Networks</code></strong></a></p>
<p><strong>The first attempt to attack the energy consumption of an AdNN.</strong>(继攻击模型之后开始攻击模型的能耗…真能灌水)</p>
<p>We investigate the robustness of neural networks against energy-oriented attacks.(研究了模型对针对能耗进行攻击的鲁棒性)</p>
<p>Specifically, we propose ILFO (Intermediate Output-Based Loss Function Optimization) attack against a common type of energy-saving neural networks, Adaptive Neural Networks <strong>AdNNs save energy consumption by dynamically deactivating part of its model based on the need of the inputs.</strong></p>
<p>ILFO leverages intermediate output as a proxy to infer the relation between input and its corresponding energy consumption.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Jeddi_Learn2Perturb_An_End-to-End_Feature_Perturbation_Learning_to_Improve_Adversarial_Robustness_CVPR_2020_paper.html"><strong><code>Learn2Perturb: an End-to-end Feature Perturbation Learning to Improve Adversarial Robustness</code></strong></a></p>
<p>Adversarial training methods leverage fixed, pre-defined perturbations and require significant hyperparameter tuning that makes them very difficult to leverage in a general fashion.</p>
<p><strong>We introduce Learn2Perturb, an end-to-end feature perturbation learning approach for improving the adversarial robustness of deep neural networks.</strong></p>
<p>We introduce novel perturbation-injection modules that are incorporated at each layer to perturb the feature space and increase uncertainty in the network. <strong>This feature perturbation is performed at both the training and the inference stages.</strong>(就是在训练和 inference 的时候为每一层都添加一个扰动模块, 用这个模块来产生网络进行对抗训练所需要的不确定性)</p>
<p>Furthermore, inspired by the Expectation-Maximization, <strong>an alternating back-propagation training algorithm is introduced to train the network and noise parameters consecutively.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Kim_Modeling_Biological_Immunity_to_Adversarial_Examples_CVPR_2020_paper.html"><strong><code>Modeling Biological Immunity to Adversarial Examples</code></strong></a></p>
<p>The attack takes advantage of statistical irregularities(统计不规则性) within the training data, where the added perturbations can “move” the image across deep learning decision boundaries.</p>
<p><strong>In a general sense, adversarial attack through perturbations is not a machine learning vulnerability</strong>, because human can be fooled by various methods too.</p>
<p>There is a gap between human perception and machine perception, which means in order to change biological perception, we need to add greater amount and magnitude perturbations to the image than changing machine perception.</p>
<p><strong>This paper explored this gap through the lens of biology and neuroscience in order to understand the robustness exhibited in human perception.</strong>(这篇文章通过生物学和神经科学的视角探索了为什么对于模型只需要一点扰动就能改变其决策认知而需要对生物进行更大数量和幅度的扰动)</p>
<p>Our experiments show that by leveraging sparsity and modeling the biological mechanisms at a cellular level, we are able to mitigate the effect of adversarial alterations to the signal that have no perceptible meaning. Furthermore, we present and illustrate the effects of top-down functional processes that contribute to the inherent immunity in human perception in the context of exploiting these properties to make a more robust machine vision system.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_On_Isometry_Robustness_of_Deep_3D_Point_Cloud_Models_Under_CVPR_2020_paper.html"><strong><code>On Isometry Robustness of Deep 3D Point Cloud Models under Adversarial Attacks</code></strong></a></p>
<p>Regarding the 3D adversarial samples, most existing works focus on manipulation of local points, which may fail to invoke the global geometry properties, like robustness under linear projection that preserves the Euclidean distance, i.e., isometry.</p>
<p><strong>We show that existing state-of-the-art deep 3D models are extremely vulnerable to isometry transformations</strong>(等轴测图转换).</p>
<p>Armed with the Thompson Sampling, we develop a black-box attack with success rate over 95% on ModelNet40 dataset Incorporating with the Restricted Isometry Property, we <strong>propose a novel framework of white-box attack</strong> on top of spectral norm based perturbation.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Xiao_One_Mans_Trash_Is_Another_Mans_Treasure_Resisting_Adversarial_Examples_CVPR_2020_paper.html"><strong><code>Resisting Adversarial Examples by Adversarial Examples</code></strong></a></p>
<p><strong>To defend against adversarial examples, a plausible(合理的) idea is to obfuscate(模糊/扰乱) the network’s gradient with respect to the input image.(比如之前的模型压缩, 添加扰动模块, 打乱模型的信息等都是这一类方法)</strong> Yet, almost all of them have proven vulnerable.</p>
<p>We revisit this seemingly flawed idea from a radically different perspective: <strong>Using adversarial examples to resist adversarial examples, specifically,  turn the harmful attacking process into a useful defense mechanism</strong></p>
<p><strong>Our defense method is conceptually simple: before feeding an input image for classification, transform it by finding an adversarial example on a pre-trained external model.</strong></p>
<p>Particularly, in comparison to adversarial training, our method offers lower training cost as well as stronger robustness.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_One-Shot_Adversarial_Attacks_on_Visual_Tracking_With_Dual_Attention_CVPR_2020_paper.html"><strong><code>One-Shot Adversarial Attacks on Visual Tracking With Dual Attention</code></strong></a></p>
<p>Almost all adversarial attacks in computer vision are aimed at pre-known object categories, but for visual object tracking, the tracked target categories are normally unknown in advance. Further more, there is a problem that the attack on tracking has the free-model tracked target.(跟踪的目标是和模型无关的)</p>
<p><strong>We propose a method to attack tracking algorithms by adding slight perturbations on the target patch in the initial frame and cause the SOTA trackers to lose the target in subsequent frames.</strong></p>
<p>The optimization objective of the proposed attack consists of two components and leverages the dual attention mechanisms:</p>
<ol>
<li>The first component adopts a targeted attack strategy by optimizing the batch confidence loss with confidence attention.</li>
<li>The second one applies a general perturbation strategy by optimizing the feature loss with channel attention.</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Kong_PhysGAN_Generating_Physical-World-Resilient_Adversarial_Examples_for_Autonomous_Driving_CVPR_2020_paper.html"><strong><code>PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving</code></strong></a></p>
<p>Most existing algorithms are used to digital-world adversarial scenarios, it is unclear now how they perform in physical world, and more importantly, the generated perturbations would cover a whole scene including these fixed background(i.e. sky), but obviously that sky could not change in physical world.</p>
<p><strong>We present PhysGAN, which generates physical-world-resilient adversarial examples for misleading autonomous driving systems in a continuous manner.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Tu_Physically_Realizable_Adversarial_Examples_for_LiDAR_Object_Detection_CVPR_2020_paper.html"><strong><code>Physically Realizable Adversarial Examples for LiDAR Object Detection</code></strong></a></p>
<p>This paper concerns the security question in automatic driving and <strong>present a method to generate universal 3D adversarial objects to fool LiDAR detectors.</strong></p>
<p>We demonstrate that placing an adversarial object on the rooftop of any target vehicle to hide the vehicle entirely from LiDAR detectors.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Shi_Polishing_Decision-Based_Adversarial_Noise_With_a_Customized_Sampling_CVPR_2020_paper.html"><strong><code>Polishing Decision-based Adversarial Noise with a Customized Sampling</code></strong></a></p>
<p>Decision-based methods polish adversarial noise by querying the target model, among them,  boundary attack is widely applied due to its powerful noise compression capability, especially when combined with transfer-based methods.(这里的 adversarial noise 应该是指 $l_p \space norm$ 的约束, 即原图和对抗样本之间不能差距太大)</p>
<p><strong>Boundary attack splits the noise compression into several independent sampling processes, repeating each query with a constant sampling setting.</strong></p>
<p><strong>We demonstrate the advantage of using current noise and historical queries to customize the variance and mean of sampling in boundary attack to polish adversarial noise. Further more, we reveal the relationship between the initial noise and the compressed noise in boundary attack.</strong></p>
<p>We propose Customized Adversarial Boundary (CAB) attack that uses the current noise to model the sensitivity of each pixel and polish adversarial noise of each image with a customized sampling setting.</p>
<p>On the one hand, CAB uses current noise as a prior belief to customize the multivariate normal distribution. On the other hand, CAB keeps the new samplings away from historical failed queries to avoid similar mistakes.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Rahnama_Robust_Design_of_Deep_Neural_Networks_Against_Adversarial_Attacks_Based_CVPR_2020_paper.html"><strong><code>Robust Design of Deep Neural Networks against Adversarial Attacks based on Lyapunov Theory</code></strong></a></p>
<p>In this paper <strong>we take a control theoretic approach(Lyapunov theory) to the problem of robustness in DNNs.</strong></p>
<p>We treat each individual layer of the DNN as a nonlinear system and use Lyapunov theory to prove stability and robustness locally, then proceed to prove stability and robustness globally for the entire DNN.</p>
<p>We develop empirically tight bounds on the response of the output layer, or any hidden layer, to adversarial perturbations added to the input, or to any preceding hidden layer.</p>
<p>We show how the spectral norm of the weight matrix for an individual layer relates to Lyapunov properties of that layer, and consequently to the local and global stability and robustness of the DNN.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Dong_Robust_Superpixel-Guided_Attentional_Adversarial_Attack_CVPR_2020_paper.html"><strong><code>Robust Superpixel-Guided Attentional Adversarial Attack</code></strong></a></p>
<p><strong>Most of these methods add perturbations in a “pixel-wise” and “global” way, but these two methods both have problems.</strong></p>
<p>Because of the contradiction between the local smoothness of normal images and the noisy property of these adversarial perturbations, this “pixel-wise” way makes these methods not robust to image processing-based defense methods and steganalysis-based methods.</p>
<p>we find that adding perturbations to the background is less useful than to the salient object, thus the “global” way is not optimal.</p>
<p><strong>We propose the first robust superpixel-guided attentional adversarial attack method. The adversarial perturbations are only added to the salient regions and guaranteed to be the same within each superpixel</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/B.S._Single-Step_Adversarial_Training_With_Dropout_Scheduling_CVPR_2020_paper.html"><strong><code>Single-Step Adversarial Training With Dropout Scheduling</code></strong></a></p>
<p>In adversarial training regime, models are trained with mini-batches augmented with adversarial samples. Fast and simple methods (e.g., single-step gradient ascent) are used for generating adversarial samples, in order to reduce computational complexity.</p>
<p>It is shown that <strong>models trained using single-step adversarial training method</strong> (adversarial samples are generated using non-iterative method) <strong>are pseudo robust</strong>(伪鲁棒的).</p>
<p>Further, <strong>this pseudo robustness of models is attributed to the gradient masking effect</strong>. However, <strong>existing works fail to explain when and why gradient masking effect occurs during single-step adversarial training.</strong></p>
<p>In this work, we (1) show that <strong>models trained using single-step adversarial training method learn to prevent the generation of single-step adversaries</strong>, and this is <strong>due to over-fitting of the model during the initial stages of training.</strong> (2) <strong>to mitigate this effect, we propose a single step adversarial training method with dropout scheduling.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Addepalli_Towards_Achieving_Adversarial_Robustness_by_Enforcing_Feature_Consistency_Across_Bit_CVPR_2020_paper.html"><strong><code>Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes</code></strong></a></p>
<p><strong>We attempt to address adversarial attack by training networks to form coarse impressions based on the information in higher bit planes, and use the lower bit planes only to refine their prediction.</strong></p>
<p>By imposing consistency on the representations learned across differently quantized images, the adversarial robustness of networks improves significantly when compared to a normally trained model.</p>
<p>Though by now adversarial training is the best way to defense adversarial attacks, this work paves the way towards achieving robustness without having to explicitly train on adversarial samples, which means our method is faster, and also closer to the natural learning process in humans.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_Towards_Large_Yet_Imperceptible_Adversarial_Image_Perturbations_With_Perceptual_Color_CVPR_2020_paper.html"><strong><code>Towards Large Yet Imperceptible Adversarial Image Perturbations With Perceptual Color Distance</code></strong></a></p>
<p>The conventional <strong>assumption</strong> on imperceptibility is that <strong>perturbations should strive for tight $L_p-norm$ bounds in RGB space.</strong></p>
<p><strong>We drop this assumption by pursuing an approach that exploits human color perception,  more specifically, minimizing perturbation size with respect to perceptual color distance.</strong></p>
<p>The first proposed approach called Perceptual Color distance C&amp;W (PerC-C&amp;W), extends the widely-used C&amp;W approach and produces larger RGB perturbations.</p>
<p>The second proposed approach  Perceptual Color distance Alternating Loss (PerC-AL), achieves the same outcome, but does so more efficiently by alternating between the classification loss and perceptual color difference when updating perturbations.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Transferable_Controllable_and_Inconspicuous_Adversarial_Attacks_on_Person_Re-identification_With_CVPR_2020_paper.html"><strong><code>Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking</code></strong></a></p>
<p>In this work, <strong>we examine the insecurity of current best performing ReID models</strong> by proposing a learning-to-mis-rank formulation to perturb the ranking of the system output.</p>
<p>As the cross-dataset transferability is crucial in the ReID domain, we also perform a back-box attack by developing a novel multi-stage network architecture that pyramids the features of different levels to extract general and transferable features for the adversarial perturbations.</p>
<p>Our method can control the number of malicious pixels by using differentiable multi-shot sampling. To guarantee the inconspicuousness of the attack, we also propose a new perception loss to achieve better visual quality.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Understanding_Adversarial_Examples_From_the_Mutual_Influence_of_Images_and_CVPR_2020_paper.html"><strong><code>Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations</code></strong></a></p>
<p>We propose to treat the DNN logits as a vector for feature representation, and <strong>utilize this vector representation to understand adversarial examples and adversarial perturbations, and analyze the influence on each other, based on the Pearson correlation coefficient (PCC).</strong></p>
<p>Our results suggest a new perspective towards the relationship between images and universal perturbations:  <strong>Universal perturbations contain dominant features, and images behave like noise to them.</strong></p>
<p>This feature perspective leads to a new method for generating targeted universal adversarial perturbations using random source images. <strong>We are the first to achieve the challenging task of a targeted universal attack without utilizing original training data.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Guo_When_NAS_Meets_Robustness_In_Search_of_Robust_Architectures_Against_CVPR_2020_paper.html"><strong><code>When NAS Meets Robustness: In Search of Robust Architectures Against Adversarial Attacks --Dahua Lin</code></strong></a></p>
<p><strong>We take an architectural perspective and investigate the patterns of network architectures that are resilient to adversarial attacks.</strong></p>
<p>Training a large network for once (one-shot NAS) and then fine-tuning the sub-networks sampled therefrom.</p>
<p>The result shows some valuable observations:</p>
<ol>
<li>Densely connected patterns result in improved robustness.</li>
<li>Under computational budget, adding convolution operations to direct connection edge is effective.</li>
<li>Flow of solution procedure (FSP) matrix is a good indicator of network robustness.</li>
</ol>
<h2 id="ICCV"><a href="#ICCV" class="headerlink" title="ICCV"></a>ICCV</h2><h3 id="2019-24-papers"><a href="#2019-24-papers" class="headerlink" title="2019(24 papers)"></a>2019(24 papers)</h3><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Mustafa_Adversarial_Defense_by_Restricting_the_Hidden_Space_of_Deep_Neural_ICCV_2019_paper.html"><strong><code>Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks</code></strong></a></p>
<p>The robustness of existing defenses suffers greatly under the white-box attack, which means attacker has full knowledge about the network. <strong>The main reason for the existence of such perturbations is the close proximity of different class samples in feature space.</strong></p>
<p><strong>we propose to class-wise disentangle the intermediate representations of DNNs. Specifically, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes.</strong></p>
<p>In this manner, the network is forced to learn distinct and distant decision regions for each class, which means impove the model’s  robustness.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.html"><strong><code>Adversarial Defense via Learning to Generate Diverse Attacks</code></strong></a></p>
<p><strong>Propose to utilize the generator to learn how to create adversarial examples. Unlike the existing approaches that create a one-shot perturbations by a deterministic generator, we propose a recursive and stochastic generator that produces much stronger and diverse perturbations that comprehensively reveal the vulnerability of the target classifier.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Zhong_Adversarial_Learning_With_Margin-Based_Triplet_Embedding_Regularization_ICCV_2019_paper.html"><strong><code>Adversarial Learning with Margin-based Triplet Embedding Regularization</code></strong></a></p>
<p><strong>Propose to improve the smoothness of the representation space, by integrating a margin-based triplet embedding regularization term into the classification object, so that the obtained model learns to resist adversarial examples.</strong></p>
<p>The regularization term consists of two steps optimizations which find potential perturbations and punish them by a large margin in an iterative way.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Adversarial_Robustness_vs._Model_Compression_or_Both_ICCV_2019_paper.html"><strong><code>Adversarial Robustness vs. Model Compression, or Both?</code></strong></a></p>
<p>Adversarial robustness requires a significantly larger capacity of the network than that for the natural training with only benign examples, this paper <strong>propose a framework of concurrent adversarial training and weight pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the dilemma of adversarial training.</strong></p>
<p>Furthermore, this work studies two hypotheses about weight pruning in the conventional setting and finds that:</p>
<ol>
<li>weight pruning is essential for reducing the model size in the adversarial setting.</li>
<li>training a small model from scratch even with inherited initialization from the large model cannot achieve neither adversarial robust nor high standard accuracy.</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.html"><strong><code>Bilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks</code></strong></a></p>
<p>Study the fast training of adversarially robust models, from the analysis of the SOTA defense methods, i.e. multi-step adversarial training, we <strong>hypothesize that the gradient magnitude links to the model robust. Motivated by this, we propose to perturb both the image and the label during training, which called Bilateral Adversarial Training(BAT).</strong></p>
<p>To generate the adversarial image, using one-step targeted attack with the target label being the most confusing class. To generate the adversarial label, we derive an closed-form heuristic solution.</p>
<p>The paper first show that random start and the most confusing target attack effectively prevent the label leaking and gradient masking problem. Then coupled with the adversarial label part , our model significantly improves the SOTA results.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Gupta_CIIDefence_Defeating_Adversarial_Attacks_by_Fusing_Class-Specific_Image_Inpainting_and_ICCV_2019_paper.html"><strong><code>CIIDefence: Defeating Adversarial Attacks by Fusing Class-specific Image Inpainting and Image Denoising</code></strong></a></p>
<p>The proposed defence mechanism is inspired by the <strong>recent works mitigating(缓和) the adversarial disturbances(扰动) by the means of image reconstruction and denoising.</strong></p>
<p>Unlike previous work, <strong>this paper apply reconstruction only for small and carefully selected image area that are most influential to the current classification outcome.</strong> The selection process is guided by the CAM responses obtained for multiple top-ranking class label. The same regions are also the most prominent for the adversarial perturbations and hence most important to purify. The resulting inpainting task is substantially(本质上) more tractable(容易) than the full image reconstruction, while still being able to prevent the adversarial attacks.</p>
<p>We combine the selective image inpainting with wavelet(小波) based image denoising to produce a non-differentiable layer that prevents attacker from using gradient backpropagation. The proposed nonlinearity cannot be easily approximated with simple differentiable alternative as demonstrated in the experiments with Backward Pass Differentiable Approximation (BPDA) attack.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Mummadi_Defending_Against_Universal_Perturbations_With_Shared_Adversarial_Training_ICCV_2019_paper.html"><strong><code>Defending Against Universal Perturbations With Shared Adversarial Training</code></strong></a></p>
<p><strong>We show that adversarial training is more effective in preventing universal perturbations and investigate the trade-off between robustness against universal perturbations and performance on unperturbed data. Moreover, propose an extension of adversarial training that handles this trade-off more gracefully.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Enhancing_Adversarial_Example_Transferability_With_an_Intermediate_Level_Attack_ICCV_2019_paper.html"><strong><code>Enhancing Adversarial Example Transferability With an Intermediate Level Attack</code></strong></a></p>
<p><strong>Adversarial examples are typically overfit to exploit the particular architecture and feature representation ofa source model, resulting in sub-optimal black-box transfer attacks to other target models.</strong></p>
<p>This paper introduce the Intermediate Level Attack (ILA), which <strong>attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model.</strong></p>
<p>The result show that we can select a layer of the source model to perturb without any knowledge of the target models while achieving high transferability, what’s more, we provide some explanatory insights regarding our method and the effect of optimizing for adversarial examples using intermediate feature maps.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Evaluating_Robustness_of_Deep_Image_Super-Resolution_Against_Adversarial_Attacks_ICCV_2019_paper.html"><strong><code>Evaluating Robustness of Deep Image Super-Resolution Against Adversarial Attacks</code></strong></a></p>
<p><strong>This paper investigates(调查) the robustness of deep learning-based super-resolution methods against adversarial attacks.</strong> These attacks can significantly deteriorate the superresolved images without noticeable distortion in the attacked low-resolution images.</p>
<p>The results show that stateof-the-art deep super-resolution methods are highly vulnerable to adversarial attacks. We also present analysis on transferability ofattacks, and feasibility oftargeted attacks and universal attacks.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Brunner_Guessing_Smart_Biased_Sampling_for_Efficient_Black-Box_Adversarial_Attacks_ICCV_2019_paper.html"><strong><code>Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial Attacks</code></strong></a></p>
<p><strong>The paper proposes a efficient attack named Boundary Attack, can be reinterpreted as a biased sampling framework that gains efficient from domain knowledge. With combining there biases, named <code>image frequency</code>, <code>region masks</code> and <code>surrogate(替代) gradient</code>, outperforms the SOTA attack approaches.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Bai_Hilbert-Based_Generative_Defense_for_Adversarial_Examples_ICCV_2019_paper.html"><strong><code>Hilbert-based Generative Defense for Adversarial Examples</code></strong></a></p>
<p><strong>This paper aims to <code>improve the developed PixelDefend</code>, which purifies a perturbed image on PixelCNN in a raster(扫描线) scan order (row/col by row/col).</strong> This scan mode inefficiently exploits the correlations between pixels, which further limit its robustness performance.</p>
<p>We propose a more advanced Hilbert curve scan order to model the pixel dependencies in this paper. It could well preserve local consistency(一致性) when mapping from 2-D image to 1-D vector, thus the local features in neighboring pixels can be more effectively modeled, the defensive power can be further improved via ensembles ofHilbert curve with different orientations(方向).</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Improving_Adversarial_Robustness_via_Guided_Complement_Entropy_ICCV_2019_paper.html"><strong><code>Improving Adversarial Robustness via Guided Complement Entropy</code></strong></a></p>
<p>Many recent methods have proposed to improve adversarial robustness by utilizing adversarial training or model distillation, which adds additional procedures to model training.</p>
<p><strong>we propose a new training paradigm calledGuidedComplementEntropy (GCE) that is capable of achieving“adversarial defense for free,”which involves no additional procedures in the process of improving adversarial robustness.</strong></p>
<p>In addition to maximizing model probabilities on the ground-truth class like cross entropy, we neutralize its probabilities on the incorrect classes along with a “guided” term to balance between these two terms.</p>
<p>We also show that our method can be used orthogonal to adversarial training across wellknown methods with noticeable robustness gain.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Miss_Detection_vs._False_Alarm_Adversarial_Learning_for_Small_Object_ICCV_2019_paper.html"><strong><code>Miss Detection vs. False Alarm: Adversarial Learning for Small Object Segmentation in Infrared Images</code></strong></a></p>
<p><strong>Using a adversarial learning to improve the situation that the miss detection and false alarm in infrared small object segmentation.</strong></p>
<p>To balance miss detection (MD) and false alarm (FA), usually need “opposite” strategies to suppress the two terms, this problem is unsolved yet.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_On_the_Design_of_Black-Box_Adversarial_Examples_by_Leveraging_Gradient-Free_ICCV_2019_paper.html"><strong><code>On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method</code></strong></a></p>
<p>Existing studies on black-box adversarial attacks are still restricted to very specific settings of threat models,  and/or suffer from prohibitively high query complexity.</p>
<p><strong>we introduce a general framework based on an operator splitting method, the alternating direction method of multipliers (ADMM) to devise efficient, robust black-box attack</strong>s that work with various distortion metrics and feedback settings without incurring high query complexity.</p>
<p>Due to the black-box nature of the threat model, the proposed ADMM solution framework is integrated with zeroth-order (ZO) optimization and Bayesian optimization (BO), and thus is applicable to the gradient-free regime, resulting in  two new black-box adversarial attack generation methods, ZOADMM and BO-ADMM.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Han_Once_a_MAN_Towards_Multi-Target_Attack_via_Learning_Multi-Target_Adversarial_ICCV_2019_paper.html"><strong><code>Once a MAN: Towards Multi-Target Attack via Learning Multi-Target Adversarial Network Once --Xiaogang Wang</code></strong></a></p>
<p>Based on the first optimization-based attacking method, many following methods are proposed to improve the attacking performance and speed.</p>
<p>Recently, <strong>generation-based methods have received much attention since they directly use feed-forward networks to generate the adversarial examples, which avoid the time-consuming iterative attacking procedure in optimization-based and gradient-based methods.</strong></p>
<p>However, current generation based methods are only able to attack one specific target (category) within one model, thus making them not applicable to real classification systems that often have hundreds thousands of categories.</p>
<p><strong>We propose <code>the first</code> Multi-target Adversarial Network (MAN), which can generate multi-target adversarial examples with a single model, MAN can produce stronger  attack results and also have better transferability, using the adversarial examples generated by our MAN to improve the robustness of the classification model can also achieve better classification accuracy than other methods.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Wiyatno_Physical_Adversarial_Textures_That_Fool_Visual_Object_Tracking_ICCV_2019_paper.html"><strong><code>Physical Adversarial Textures That Fool Visual Object Tracking</code></strong></a></p>
<p><strong>Present a method for creating inconspicuous-looking adversarial textures, cause visual object tracking systems to become confused.</strong></p>
<p>As a target being visually tracked moves in front of such a poster, its adversarial texture makes the tracker lock onto it, thus allowing the target to evade.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Gowal_Scalable_Verified_Training_for_Provably_Robust_Image_Classification_ICCV_2019_paper.html"><strong><code>Scalable Verified Training for Provably Robust Image Classification --DeepMind</code></strong></a></p>
<p>Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations:<br>$$<br>\min <em>{\theta} \rho(\theta), \quad \text { where } \quad \rho(\theta)=\mathbb{E}</em>{(x, y) \sim \mathcal{D}}\left[\max _{\delta \in \mathcal{S}} L(\theta, x+\delta, y)\right]<br>$$</p>
<p>But there adversarial training methods often result in difficult optimization procedures that remain hard to scale to larger networks.</p>
<p><strong>We show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks</strong> that beat the state-of-the-art in verified accuracy.</p>
<p>While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Self-Training_and_Adversarial_Background_Regularization_for_Unsupervised_Domain_Adaptive_One-Stage_ICCV_2019_paper.html"><strong><code>Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection</code></strong></a></p>
<p><strong>Using adversarial background helps the network extract discriminative features for target backgrounds to reduce the domain shift, improving performance of domain adaption in one-stage object detection</strong></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Joshi_Semantic_Adversarial_Attacks_Parametric_Transformations_That_Fool_Deep_Classifiers_ICCV_2019_paper.html"><strong><code>Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers</code></strong></a></p>
<p>The majority of adversarial attacks assume global, fine-grained control over the image pixel space. <strong>This paper consider a different setting:  what happens if the adversary could only alter specific attributes of the input image? The results show that these would generate inputs that might be perceptibly different, but still natural-looking and enough to fool a classifier.</strong></p>
<p>We propose a novel approach to generate such “semantic” adversarial examples by optimizing a particular adversarial loss over the range-space of a parametric conditional generative model.</p>
<p>We demonstrate implementations of our attacks on binary classifiers trained on face images, and show that such natural-looking semantic adversarial examples exist</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Croce_Sparse_and_Imperceivable_Adversarial_Attacks_ICCV_2019_paper.html"><strong><code>Sparse and Imperceivable Adversarial Attacks</code></strong></a></p>
<p>From a safety perspective, highly sparse adversarial attacks are particularly dangerous.On the other hand the pixelwise perturbations of sparse attacks are typically large and thus can be potentially detected.</p>
<p><strong>We propose a new black-box technique to craft adversarial examples aiming at minimizing l0- distance to the original image.</strong></p>
<p>The proposed attack approach is not only better than existing SOTA models, but also can integrate additional bounds on the component-wise perturbation. Allowing pixels to change only in region of high variation and avoiding changes along axis aligned edges makes our adversarial examples almost non-perceivable.</p>
<p>Moreover, we adapt the Projected Gradient Descent attack to the l0-norm integrating component-wise constraints, which allow us to do adversarial training to improve the robustness of model.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Tolias_Targeted_Mismatch_Adversarial_Attack_Query_With_a_Flower_to_Retrieve_ICCV_2019_paper.html"><strong><code>Targeted Mismatch Adversarial Attack: Query with a Flower to Retrieve the Tower</code></strong></a></p>
<p><strong>Propose a targeted mismatch attack for DL-based retrieval system to generate an adversarial image to conceal(隐藏) the query image.</strong></p>
<p>We show successful attacks to partially unknown systems, by designing various loss functions for the adversarial image construction</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Finlay_The_LogBarrier_Adversarial_Attack_Making_Effective_Use_of_Decision_Boundary_ICCV_2019_paper.html"><strong><code>The LogBarrier adversarial attack: making effective use of decision boundary information</code></strong></a></p>
<p><strong>Propose a new untargeted attack, which uses the well-regarded logarithmic barrier method(best practices from the optimization literature), to solve the constrained minimization question in gradient-based attacks.</strong></p>
<p>The proposed LogBarrier attack perturbs all images more efficiently than other attacks: the distance needed to perturb all images is significantly smaller with the LogBarrier attack than with other state-ofthe-art attacks.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Universal_Adversarial_Perturbation_via_Prior_Driven_Uncertainty_Approximation_ICCV_2019_paper.html"><strong><code>Universal Adversarial Perturbation via Prior Driven Uncertainty Approximation --Hong Liu</code></strong></a></p>
<p>Compared to the conventional supervised universal adversarial perturbations(UAPs) that suffer from the knowledge of training data, the data-independent unsupervised UAPs are more applicable. <strong>Existing unsupervised methods fail to take advantage of the model uncertainty to produce robust perturbations.</strong></p>
<p><strong>we propose a new unsupervised universal adversarial perturbation method to generate a robust UAP by fully exploiting the model uncertainty.</strong></p>
<p>A Monte Carlo sampling method is deployed to activate more neurons to increase the model uncertainty for a better adversarial perturbation. Then a textural bias prior revealing a statistical uncertainty is proposed, which helps to improve the attacking performance.</p>
<p>The UAP is crafted by the stochastic gradient descent algorithm with a boosted momentum optimizer, and a Laplacian pyramid frequency model is finally used to maintain the statistical uncertainty.</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Addepalli_Towards_Achieving_Adversarial_Robustness_by_Enforcing_Feature_Consistency_Across_Bit_CVPR_2020_paper.html"><strong><code>What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance</code></strong></a></p>
<p>This paper examines a type of global image manipulation that can produce similar adverse effects.</p>
<p><strong>We explore how strong color casts caused by incorrectly applied computational color constancy (referred to as white balance (WB) in photography) negatively impact the performance of DNNs</strong> targeting image segmentation and classification. (探讨了由于强烈偏色导致的对模型性能所产生的负面影响)</p>
<p>We discuss how existing image augmentation methods used to improve the robustness of DNNs are not well suited for modeling WB errors. To address this problem, a novel augmentation method is proposed that can emulate accurate color constancy degradation.We also explore pre-processing training and testing images with a recent WB correction algorithm to reduce the effects of incorrectly white-balanced images.</p>
<h2 id="ECCV"><a href="#ECCV" class="headerlink" title="ECCV"></a>ECCV</h2><h3 id="2020-29-papers"><a href="#2020-29-papers" class="headerlink" title="2020(29 papers)"></a>2020(29 papers)</h3><p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590766.pdf"><strong><code>Adversarial Ranking Attack and Defense</code></strong></a><br><strong>Proposed two attacks against deep ranking systems, Candidate Attack and Query Attack,  that can raise or lower the rank of chosen candidates by adversarial perturbations.</strong></p>
<p>Specifically, the expected ranking order is first represented as a set of inequalities, and then a triplet-like objective function is designed to obtain the optimal perturbation.</p>
<p>Conversely, a defense method is also proposed to improve the ranking system robustness, which can mitigate all the proposed attacks simultaneously.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500647.pdf"><strong><code>Adversarial T-shirt! Evading Person Detectors in A Physical World</code></strong></a></p>
<p><strong>Most of the existing works on physical adversarial attacks focus on static objects.</strong></p>
<p><strong>we propose Adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person’s pose changes.</strong></p>
<p><strong>This is the first work that models the effect of deformation for designing physical adversarial examples with respect to non-rigid objects such as T-shirts.</strong>(第一次为非刚性物体设计对抗样本)</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570239.pdf"><strong><code>AdvPC: Transferable Adversarial Perturbations on 3D Point Clouds</code></strong></a><br>In this work, we <strong>present novel data-driven adversarial attacks against 3D point cloud networks.</strong></p>
<p>We aim to address the following problems:</p>
<ol>
<li>in current 3D point cloud adversarial attacks: they do not transfer well between different networks</li>
<li>they are easy to defend against via simple statistical methods.</li>
</ol>
<p><strong>We develop a new point cloud attack (dubbed AdvPC) that exploits the input data distribution by adding an adversarial loss, after Auto-Encoder reconstruction, to the objective it optimizes.</strong> AdvPC leads to perturbations that are resilient against current defenses, while remaining highly transferable compared to state-of-theart attacks.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580069.pdf"><strong><code>Anti-Bandit Neural Architecture Search for Model Defense --Hong Liu</code></strong></a></p>
<p><strong>We defend against adversarial attacks using neural architecture search (NAS) which is based on a comprehensive search of denoising blocks, weight-free operations, Gabor filters and convolutions.</strong></p>
<p>The resulting anti-bandit NAS (ABanditNAS) incorporates a new operation evaluation measure and search process based on the lower and upper confidence bounds (LCB and UCB).</p>
<p><strong>Unlike the conventional bandit algorithm using UCB for evaluation only, we use UCB to abandon arms for search efficiency and LCB for a fair competition between arms.</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580375.pdf"><strong><code>API-Net: Robust Generative Classifier via a Single Discriminator --Hong Liu</code></strong></a></p>
<p>For classification problem, a <strong>generative classifier</strong> typically models the distribution of inputs and labels, and thus <strong>can better handle off-manifold examples</strong> at the cost of a concise structure.(生成模型中的分类器, 通常对输入的分布和 label 的分布进行建模, 这样就能在简洁的结构上来更好地处理非流形的数据)</p>
<p>On the contrary, a <strong>discriminative classifier only models the conditional distribution of labels given inputs</strong>, but benefits from effective optimization owing to its succinct structure.(判别模型中的分类器, 只对在给定 label 下的输入, 即条件概率进行建模, 但由于其简洁的结构能得到有效的优化)</p>
<p><strong>This work aims for a solution of generative classifiers that can profit from the merits of both.</strong></p>
<p>We propose an Anti-Perturbation Inference (API) method, which searches for anti-perturbations to maximize the lower bound of the joint log-likelihood of inputs and classes.</p>
<p>By leveraging the lower bound to approximate Bayes’ rule, we construct a generative classifier Anti-Perturbation Inference Net (API-Net) upon a single discriminator. It takes advantage of the generative properties to tackle off-manifold examples while maintaining a succinct structure for effective optimization.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660035.pdf"><strong><code>APRICOT: A Dataset of Physical Adversarial Attacks on Object Detection</code></strong></a></p>
<p>We present APRICOTy, <strong>a collection of over 1,000 annotated photographs of printed adversarial patches in public locations.</strong></p>
<p>Our analysis suggests that maintaining adversarial robustness in uncontrolled settings is highly challenging but that it is still possible to produce targeted detections under white-box and sometimes black-box settings.</p>
<p><strong>We establish baselines for defending against adversarial patches via several methods</strong>, including using a detector supervised with synthetic data and using unsupervised methods such as kernel density(密度) estimation, Bayesian uncertainty, and reconstruction error.</p>
<p>Our results suggest that adversarial patches can be effectively flagged(被标记), both in a high-knowledge, attack specific scenario and in an unsupervised setting where patches are detected as anomalies(异常) in natural images.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600273.pdf"><strong><code>Boosting Decision-based Black-box Adversarial Attacks with Random Sign Flip</code></strong></a><br><strong>Existing decision-based attacks perform poorly on the $l_{\infty}$ setting and the required enormous queries</strong> cast a shadow over the practicality.</p>
<p>We show that <strong>just randomly flipping the signs of a small number of entries in adversarial perturbations can significantly boost the attack performance</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730749.pdf"><strong><code>Defense Against Adversarial Attacks via Controlling Gradient Leaking on Embedded Manifolds --Jun Zhu</code></strong></a></p>
<p><strong>we present a new perspective, namely gradient leaking hypothesis, to understand the existence of adversarial examples and to further motivate effective defense strategies.</strong></p>
<p>We consider the low dimensional manifold structure of natural images, and empirically verify that the leakage of the gradient (w.r.t input) along the (approximately) perpendicular(垂直的) direction to the tangent(切线的) space of data manifold is a reason for the vulnerability over adversarial attacks.</p>
<p>We further present a new robust learning algorithm which encourages a larger gradient component in the tangent space of data manifold, suppressing the gradient leaking phenomenon consequently.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620171.pdf"><strong><code>Design and Interpretation of Universal Adversarial Patches in Face Detection  --Jun Zhu</code></strong></a></p>
<p><strong>We propose new optimization based approaches to automatic design of universal adversarial patches for varying goals of the attack</strong>, including scenarios in which true positives are suppressed without introducing false positives.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540426.pdf"><strong><code>Gabor Layers Enhance Network Robustness</code></strong></a><br><strong>We explore the effect of replacing the first layers of various deep architectures with Gabor layers on robustness against adversarial attacks.</strong> (i.e. convolutional layers with filters that are based on learnable Gabor parameters)</p>
<p><strong>Architectures with Gabor layers gain a consistent boost in robustness over regular models and maintain high generalizing test performance.</strong></p>
<p>We then exploit the analytical expression of Gabor filters to derive a compact expression for a Lipschitz constant of such filters, and harness(利用) this theoretical result to develop a regularizer we use during training to further enhance network robustness.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700103.pdf"><strong><code>Improving Query Eﬃciency of Black-box Adversarial Attack</code></strong></a></p>
<p>Existing query-based black-box adversarial attacks often require excessive model queries to maintain a high attack success rate.</p>
<p><strong>In order to improve query efficiency, we explore the distribution of adversarial examples around benign inputs with the help of image structure information characterized by a Neural Process, and propose a Neural Process based black-box adversarial attack (NP-Attack) in this paper.</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670562.pdf"><strong><code>Improving the Transferability of Adversarial Examples with Resized-Diverse-Inputs, Diversity-Ensemble and Region Fitting</code></strong></a><br><strong>We introduce a three stage pipeline</strong>: resized-diverse-inputs (RDIM), diversity-ensemble (DEM) and region fitting, that <strong>work together to generate transferable adversarial examples.</strong></p>
<p>We first explore the internal relationship between existing attacks, and propose RDIM that is capable of exploiting this relationship.</p>
<p>Then we propose DEM, the multi-scale version of RDIM, to generate multi-scale gradients.</p>
<p>Finally,  we transform value fitting into region fitting across iterations.</p>
<p>RDIM and region fitting do not require extra running time and these three steps can be well integrated into other attacks</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490001.pdf"><strong><code>Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors</code></strong></a><br>Using standard detection datasets, we train patterns that suppress the objectness scores produced by a range of commonly used detectors, and ensembles of detectors.</p>
<p><strong>We benchmark the effectiveness of adversarially trained patches</strong> under both white-box and black-box settings, and <strong>quantify transferability of attacks between datasets, object classes, and detector models.</strong></p>
<p><strong>We present a detailed study of physical world attacks</strong> using printed posters and wearable clothes, and rigorously <strong>quantify the performance of such attacks with different metrics.</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750290.pdf"><strong><code>Manifold Projection for Adversarial Defense on Face Recognition</code></strong></a><br><strong>A recent study has shown that in addition to regular off-manifold adversarial images, there are also adversarial images on the manifold.</strong></p>
<p>We propose Adversarial Variational AutoEncoder (A-VAE), a novel framework to tackle both types of attacks. <strong>We hypothesize that both off-manifold and on-manifold attacks move the image away from the high probability region of image manifold.</strong></p>
<p><strong>We utilize variational auto-encoder (VAE) to estimate the lower bound of the log-likelihood of image and explore to project the input images back into the high probability regions of image manifold again.</strong></p>
<p>At inference time, our model synthesizes multiple similar realizations of a given image by random sampling, then the nearest neighbor of the given image is selected as the final input of the face recognition model.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650239.pdf"><strong><code>Motion-Excited Sampler: Video Adversarial Attack with Sparked Prior --Yi Yang</code></strong></a><br>We aim to <strong>attack video models by utilizing intrinsic(固有的) movement pattern and regional relative motion(运动) among video frames.</strong></p>
<p>We propose an effective motion excited sampler to obtain motion-aware noise prior, which we term as sparked prior. Our sparked prior underlines(强调) frame correlations and utilizes video dynamics via relative motion.</p>
<p>By using the sparked prior in gradient estimation, we can successfully attack a variety of video classification models with fewer number of queries.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470154.pdf"><strong><code>Multitask Learning Strengthens Adversarial Robustness</code></strong></a></p>
<p><strong>We connect the adversarial robustness of a model to the number of tasks</strong> that it is trained on.</p>
<p><strong>Results show that attack difficulty increases as the number of target tasks increase and when models are trained on multiple tasks at once, they become more robust to adversarial attacks on individual tasks.</strong></p>
<p>While adversarial defense remains an open challenge, our results suggest that deep networks are vulnerable partly because they are trained on too few tasks.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620664.pdf"><strong><code>Open-set Adversarial Defense</code></strong></a><br><strong>We show that open-set recognition systems are vulnerable to adversarial attacks and adversarial defense mechanisms trained on known classes do not generalize well to open-set samples.</strong> we emphasize the need of an Open-Set Adversarial Defense (OSAD) mechanism.</p>
<p>This paper proposes an Open-Set Defense Network (OSDN) as a solution to the OSAD problem. The proposed network uses an encoder with feature-denoising layers coupled with a classifier to learn a noise-free latent feature representation.</p>
<p>Two techniques are employed to obtain an informative latent feature space with the objective of improving open-set performance. First, a decoder is used to ensure that clean images can be reconstructed from the obtained latent features. Then, self-supervision is used to ensure that the latent features are informative enough to carry out an auxiliary task.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730307.pdf"><strong><code>Patch-wise Attack for Fooling Deep Neural Network</code></strong></a><br>Features of a pixel extracted by deep neural networks (DNNs) are influenced by its surrounding regions, and different DNNs generally focus on different discriminative regions in recognition.</p>
<p><strong>We propose a patch-wise iterative algorithm – a black-box attack towards mainstream normally trained and defense models, which differs from the existing attack methods manipulating pixel-wise noise.</strong></p>
<p>In this way, without sacrificing the performance of white-box attack, our adversarial examples can have strong transferability.</p>
<p><strong>Specifically, we introduce an amplification factor to the step size in each iteration, and one pixel’s overall gradient overflowing the $\epsilon$-constraint is properly assigned to its surrounding regions by a project kernel.</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720137.pdf"><strong><code>Practical Poisoning Attacks on Neural Networks</code></strong></a><br>Data poisoning attacks means that  poisoning samples are injected at the training phase to achieve adversarial goals at test time. Existing poisoning attacks rely on certain assumptions on the adversary knowledge and capability to ensure efficacy, which may be unrealistic in practice.</p>
<p>This paper presents a new, practical targeted poisoning attack method, namely BlackCard.</p>
<p><strong>BlackCard possesses a set of critical properties for ensuring attacking efficacy in practice</strong>, which has never been simultaneously achieved by any existing work, including knowledge-oblivious, clean-label, and clean-test.</p>
<p>We show that the effectiveness of BlackCard can be intuitively guaranteed by a set of analytical reasoning and observations, through exploiting an essential characteristic of gradient-descent optimization which is pervasively adopted in DNN models.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550188.pdf"><strong><code>Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks</code></strong></a></p>
<p>First introduce the notion of “backdoor attack”. A backdoor attack installs a backdoor into the victim model by injecting a backdoor pattern into a small proportion of the training data. At test time, the victim model behaves normally on clean test data, yet consistently predicts a specific (likely incorrect) target class whenever the backdoor pattern is present in a test example.</p>
<p>While existing backdoor attacks are effective, they are not stealthy. The modifications made on training data or labels are often suspicious and can be easily detected by simple data filtering or human inspection.</p>
<p><strong>we present a new type of backdoor attack inspired by an important natural phenomenon: reflection. Using mathematical modeling of physical reflection models, we propose reflection backdoor (Refool) to plant reflections as backdoor into a  victim model.</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560766.pdf"><strong><code>Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses</code></strong></a></p>
<p>Adversarial examples generated by existing attacks are generally hard to transfer to defense models.</p>
<p>We <strong>observe the property of regional homogeneity(区域同质性) in adversarial perturbations</strong> and suggest that <strong>the defenses are less robust to regionally homogeneous perturbations.</strong></p>
<p><strong>Therefore, we propose an effective transforming paradigm and a customized gradient transformer module to transform existing perturbations into regionally homogeneous ones.</strong></p>
<p>Without explicitly forcing the perturbations to be universal, we observe that a well-trained gradient transformer module tends to output input-independent gradients (hence universal) benefiting from the under-fitting phenomenon.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540392.pdf"><strong><code>Robust Neural Networks inspired by Strong Stability Preserving Runge-Kutta methods</code></strong></a></p>
<p>Recent works observe that a class of widely used neural networks can be viewed as the Euler(欧拉) method of numerical discretization(数值离散化).</p>
<p>From the numerical discretization perspective, Strong Stability Preserving (SSP) methods are more advanced techniques than the explicit Euler method that produce both accurate and stable solutions.</p>
<p>Motivated by the SSP property and a generalized Runge-Kutta method, <strong>we proposed Strong Stability Preserving networks (SSP networks) which improve robustness against adversarial attacks.</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640069.pdf"><strong><code>Robust Tracking against Adversarial Attacks</code></strong></a><br>We <strong>first attempt to generate adversarial examples on top of video sequences to improve the tracking robustness</strong> against adversarial attacks.</p>
<p><strong>We take temporal motion into consideration when generating lightweight perturbations over the estimated tracking results frame-by-frame.</strong> On one hand, we add the temporal perturbations into the original video sequences as adversarial examples to greatly degrade the tracking performance. On the other hand, we sequentially estimate the perturbations from input sequences and learn to eliminate their effect for performance restoration.</p>
<p>Our defense method not only eliminates the large performance drops caused by adversarial attacks, but also achieves additional performance gains when deep trackers are not under adversarial attacks.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590018.pdf"><strong><code>SemanticAdv: Generating Adversarial Examples via Attribute-conditioned Image Editing</code></strong></a><br>Most such adversarial examples try to guarantee “subtle” perturbation(轻微的扰动) by limiting the $L_p$ norm of the perturbation.</p>
<p><strong>We propose SemanticAdv to generate a new type of semantically realistic adversarial examples via attribute-conditioned image editing. Our SemanticAdv enables fine-grained analysis and evaluation of DNNs with input variations in the attribute space.</strong></p>
<p>Our adversarial examples not only exhibit semantically meaningful appearances but also achieve high targeted attack success rates under both white-box and black-box settings.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700205.pdf"><strong><code>SPARK: Spatial-aware Online Incremental Attack Against Visual Tracking</code></strong></a><br>we identify a new task for the adversarial attack to visual tracking: online generating imperceptible perturbations that mislead trackers along with an incorrect (Untargeted Attack, UA) or specified trajectory (Targeted Attack, TA).</p>
<p>We first propose a spatial-aware basic attack by adapting existing attack methods, i.e., FGSM, BIM, and C W, and comprehensively analyze the attacking performance.</p>
<p>We identify that online object tracking poses two new challenges: 1) it is difficult to generate imperceptible perturbations that can transfer across frames, and 2) realtime trackers require the attack to satisfy a certain level of efficiency.</p>
<p>To address these challenges, <strong>we further propose the spatial-aware online incremental attack (a.k.a. SPARK) that performs spatial-temporal sparse incremental perturbations online and makes the adversarial attack less perceptible.</strong></p>
<p>SPARK quickly converges(收敛) to very small losses within several iterations by considering historical incremental perturbations, making it much more efficient than basic attacks.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670035.pdf"><strong><code>Sparse Adversarial Attack via Perturbation Factorization</code></strong></a><br>This work studies the sparse adversarial attack, which aims to generate adversarial perturbations onto partial positions of one benign image. <strong>The sparse adversarial attack involves two challenges, i.e. where to perturb, and how to determine the perturbation magnitude.</strong></p>
<p>Many existing works determined the perturbed positions manually or heuristically, and then optimized the magnitude using a proper algorithm designed for the dense adversarial attack.</p>
<p><strong>We propose to factorize(分解) the perturbation at each pixel to the product of two variables, including the perturbation magnitude and one binary selection factor (0 or 1).</strong> One pixel is perturbed if its selection factor is 1, otherwise not perturbed.</p>
<p><strong>We formulate the sparse attack problem as a mixed integer programming (MIP) to jointly optimize the binary selection factors and continuous perturbation magnitudes</strong> of all pixels, with a cardinality constraint on selection factors to explicitly control the degree of sparsity.</p>
<p>The perturbation factorization provides the extra flexibility to incorporate other meaningful constraints on selection factors or magnitudes to achieve some desired performance, such as the group-wise sparsity or the enhanced visual imperceptibility. We develop an efficient algorithm by equivalently reformulating the MIP problem as a continuous optimization problem.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620120.pdf"><strong><code>Spatiotemporal Attacks for Embodied Agents</code></strong></a><br>Existing work on adversarial attacks have mainly focused on static scenes; however, it remains unclear whether such attacks are effective against embodied agents, which could navigate and interact with a dynamic environment.</p>
<p><strong>We take the first step to study adversarial attacks for embodied agents.</strong></p>
<p>We generate spatiotemporal(时间和空间的) perturbations to form 3D adversarial examples, which exploit the interaction history in both the temporal and spatial dimensions.</p>
<p>Regarding the temporal dimension, since agents make predictions based on historical observations, we develop a trajectory attention module to explore scene view contributions, which further help localize 3D objects appeared with highest stimuli(刺激).</p>
<p>By conciliating(顺应) with clues from the temporal dimension, along the spatial dimension, we adversarially perturb the physical properties (e.g., texture and 3D shape) of the contextual objects that appeared in the most important scene views.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680477.pdf"><strong><code>Square Attack: a query-eﬃcient black-box adversarial attack via random search</code></strong></a><br>We propose the Square Attack, a score-based black-box $l_2$- and $l_1$-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking.</p>
<p>Square Attack is based on a randomized search scheme which selects localized square shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set.</p>
<p>In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least 1:8 and up to 3 compared to the recent state-of-the-art l1-attack.</p>
<p><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460596.pdf"><strong><code>Targeted Attack for Deep Hashing based Retrieval</code></strong></a><br><strong>In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on deep hashing based retrieval.</strong></p>
<p>We first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived.</p>
<p>To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the $l_{\infty}$ restriction on the perturbation.</p>
<h2 id="ICLR"><a href="#ICLR" class="headerlink" title="ICLR"></a>ICLR</h2><h3 id="2019-10-papers"><a href="#2019-10-papers" class="headerlink" title="2019(10 papers)"></a>2019(10 papers)</h3><p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=r1lWUoA9FQ"><strong><code>Are adversarial examples inevitable?</code></strong></a></p>
<p><strong>This paper analyzes adversarial examples from a theoretical perspective, and identifies fundamental bounds on the susceptibility of a classifier to adversarial attacks.</strong></p>
<p><strong>We show that, for certain classes of problems, adversarial examples are inescapable.</strong></p>
<p>we explore the implications of theoretical guarantees for real-world problems and discuss how factors such as dimensionality and image complexity limit a classifier’s robustness against adversarial examples.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=r1g4E3C9t7"><strong><code>Characterizing Audio Adversarial Examples Using Temporal Dependency</code></strong></a></p>
<p>Our results <strong>reveal the importance of using the temporal dependency in audio data to gain discriminate power against adversarial examples.</strong></p>
<p>The results suggest that (i)  input transformation developed from image adversarial defense provides limited robustness improvement and is subtle to advanced attacks; (ii) temporal dependency can be exploited to gain discriminative power against audio adversarial examples and is resistant to adaptive attacks considered in our experiments.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=BygANhA9tQ"><strong><code>Cost-Sensitive Robustness against Adversarial Examples</code></strong></a></p>
<p>These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications.</p>
<p>For some tasks, there are some adversarial transformation are more important than others, <strong>we propose cost-sensitive robustness as the criteria for measuring the classifier’s performance</strong> for these tasks.</p>
<p>We <strong>encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt a robust training method</strong>  to optimize for cost-sensitive robustness.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=ryetZ20ctX"><strong><code>Defensive Quantization: When Efficiency Meets Robustness --Han Song</code></strong></a></p>
<p><strong>We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.</strong></p>
<p>We first conduct an empirical study to show that <strong>vanilla quantization suffers more from adversarial attacks</strong>. We observe that <strong>the inferior robustness comes from the error amplification effect</strong>, where the quantization operation further enlarges the distance caused by amplified noise.</p>
<p>Then <strong>we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization</strong>, such that the magnitude of the adversarial noise remains non-expansive during inference.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=HyGIdiRqtm"><strong><code>Evaluating Robustness of Neural Networks with Mixed Integer Programming</code></strong></a></p>
<p><strong>We formulate verification of piecewise-linear neural networks as a mixed integer program.</strong> On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art.</p>
<p>We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available.</p>
<p>we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded $l_{\infty}$ = 0:1.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=BkfbpsAcF7"><strong><code>Excessive Invariance Causes Adversarial Vulnerability</code></strong></a></p>
<p><strong>We decompose these errors caused by distribution shift into two complementary sources: sensitivity and invariance.</strong></p>
<p>We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from $\epsilon$-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. (作者认为网络对与任务无关的输入的变化过于敏感, 而且对与任务相关的输入的变化过于不变, 这2个原因是导致网络在输入空间中脆弱的原因)</p>
<p>We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an informationtheoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=BkMiWhR5K7"><strong><code>Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors</code></strong></a></p>
<p>We introduce a framework that conceptually unifies much of the existing work on black-box attacks.</p>
<p>Despite the current SOTA methods are optimal, <strong>we show how to improve black-box attacks by bringing a new element into the problem: gradient priors.</strong></p>
<p>We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=BkgzniCqY7"><strong><code>Structured Adversarial Attack: Towards General Implementation and Better Interpretability</code></strong></a></p>
<p>$l_p$ norm of the added perturbation is usually used to measure the similarity between original image and adversarial example, but s<strong>uch adversarial attacks perturbing the raw input spaces may fail to capture structural information hidden in the input.</strong></p>
<p><strong>The paper explores group sparsity in adversarial perturbations by sliding a mask through images aiming for extracting key spatial structures to develop a more general attack model.</strong></p>
<p>An ADMM (alternating direction method of multipliers)-based framework is proposed that can split the original problem into a sequence of analytically solvable subproblems and can be generalized to implement other attacking methods.</p>
<p>Strong group sparsity is achieved in adversarial perturbations even with the same level of $l_p$-norm distortion $(p \in {1, 2, \infty })$ as the SOTA attacks.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=HylTBhA5tQ"><strong><code>The Limitations of Adversarial Training and the Blind-Spot Attack</code></strong></a></p>
<p>We show that  <strong>the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network.</strong>(对抗训练的鲁棒性和测试点与训练数据的数据流形之间的距离有很大的关系)</p>
<p><strong>Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks</strong>(远离训练数据数据流形的测试样本会更容易被攻击)</p>
<p>Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the “blind-spot attack”, where the input images reside in “blind-spots” (low density regions) of the empirical distribution of training data but is still on the ground-truth data manifold.(在对抗训练这种防御方法可能受到盲点攻击, 其定义是虽然输入图像在 GT 数据流形上, 但可能已经远离了训练数据的分布, 所以也会被模型分类错误)</p>
<p><strong>For small datasets, these blind-spots can be easily found by simply scaling and shifting image pixel values. For large datasets, the blind-spots make defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data.</strong>(对一些小的数据集来说, 可以简单地利用缩放和移动这些像素的值来找到盲点, 但由于维度过高且对抗样本数据缺乏, 很难找到这些盲点. p.s. 生成对抗样本需要不少时间)</p>
<p>We find that blind-spots also exist on provable defenses because these trainable robustness certificates can only be practically optimized on a limited set of training data(我们发现在可证明的防御中也存在盲点，因为这些可训练的鲁棒性验证只能在有限的训练数据集上进行优化)</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=S1EHOsC9tX"><strong><code>Towards the first adversarially robust neural network model on MNIST</code></strong></a></p>
<p>The widely used $L_\infty$ defense:</p>
<ol>
<li>has lower $L_0$ robustness than undefended networks and is still highly susceptible to $L_2$ perturbations</li>
<li>classifies unrecognizable images with high certainty</li>
<li>performs not much better than simple input binarization</li>
<li>features adversarial perturbations that make little sense to humans.</li>
</ol>
<p><strong>We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions.</strong></p>
<p>We demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.</p>
<h3 id="2020-26-papers"><a href="#2020-26-papers" class="headerlink" title="2020(26 papers)"></a>2020(26 papers)</h3><p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SJxSDxrKDr"><strong><code>Adversarial Training and Provable Defense: Brigding The Gap</code></strong></a><br>We present COLT, a new method to train neural networks based on a novel combination of adversarial training and provable defenses.</p>
<p><strong>The key idea is to model neural network training as a procedure which includes both, the verifier and the adversary. In every iteration, the  verifier aims to certify the network using convex relaxation while the adversary tries to find inputs inside that convex relaxation which cause verification to fail.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=Hkem-lrtvH"><strong><code>BayesOpt Adversarial Attack</code></strong></a><br>Current approaches relying on substitute model training, gradient estimation or genetic algorithms often require an excessive number of queries.</p>
<p><strong>We propose a query-efficient black-box attack which uses Bayesian optimisation in combination with Bayesian model selection to optimise over the adversarial perturbation and the optimal degree of search space dimension reduction.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=r1xGnA4Kvr"><strong><code>Biologically Inspired Sleep Algorithm For In-Creased Generalization And Adversarial Robustness In Deep Neural Networks</code></strong></a><br>It has been hypothesized that sleep promotes generalization of knowledge and improves robustness against noise in  animals and humans.</p>
<p><strong>we utilize a biologically inspired sleep phase in ANNs and demonstrate the benefit of sleep on defending against adversarial attacks as well as in increasing ANN classification robustness.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SJxhNTNYwB"><strong><code>Black-box Adversarial Attack With Transferable Model-based Embedding</code></strong></a><br>Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model(以前的方法都使用梯度或者初始化来作为白盒模型的替代, 以这种方式把 transfer-based 和 socred-based 的方法联合起来), this new method <strong>tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network.</strong></p>
<p><strong>The method produces adversarial perturbations with high level semantic patterns that are easily transferable.</strong> We show that <strong>this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=HJxdTxHYvB"><strong><code>Breaking Certified Defenses: Semantic Adversarial Examples With Spoofed Robustness Certificates</code></strong></a><br><strong>We present a new attack that exploits not only the labelling function of a classifier, but also the certificate generator.</strong></p>
<p><strong>The proposed method applies large perturbations that place images far from a class boundary while maintaining the imperceptibility property of adversarial examples.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SJgwzCEKwH"><strong><code>Briding Mode Connectivity In Loss Landscapes And Adversarial Robustness</code></strong></a><br><strong>We propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness.</strong></p>
<p>When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide (诚意) data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models.</p>
<p>We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models. A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=Skgy464Kvr"><strong><code>Detecting And Diagnosing Adversarial Images With Class-Conditional Capsule Reconstructions</code></strong></a><br><strong>We first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input.</strong></p>
<p>To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate.</p>
<p><strong>We find that CapsNets always perform better thanconvolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class.</strong></p>
<p>Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=HJem3yHKwH"><strong><code>EMPIR: Ensembles of Mixed Precision Deep Networks For Increased Robustness Against Adversarial Attacks</code></strong></a><br><strong>We propose EMPIR, ensembles of quantized DNN models with different numerical precisions</strong>, as a new approach to increase robustness against adversarial attacks.</p>
<p>EMPIR is <strong>based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks</strong>, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs.</p>
<p>EMPIR overcomes this limitation to achieve the “best of both worlds”, i.e., <strong>the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble.</strong></p>
<p>Further, as low precision DNN models have significantly lower computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (&lt;25% in our evaluations).</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=Skgvy64tvr"><strong><code>Enhancing Adversarial Defense by k-Winners-Take-All</code></strong></a><br>Instead of using popular activation functions (such as ReLU), <strong>we advocate the use of k-Winners-Take-All (k-WTA) activation, a $C^0$ discontinuous activation function that purposely invalidates the neural network model’s gradient at densely distributed input data points.</strong></p>
<p><strong>The proposed k-WTA activation can be readily used in nearly all existing networks and training methods with no significant overhead.</strong> Our proposal is theoretically rationalized.</p>
<p><strong>We analyze why the discontinuities in k-WTA networks can largely prevent gradient-based search of adversarial examples and why they at the same time remain innocuous to the network training.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=BkgWahEFvr"><strong><code>Enhancing Transformation-based Defenses Against Adversarial Attacks With A Distribution Classifier</code></strong></a><br>Stochastic input transformation methods have been proposed, where the idea is to recover the image from adversarial attack by random transformation, and to take the majority vote as consensus(一致的) among the random samples. <strong>However, the transformation improves the accuracy on adversarial images at the expense of the accuracy on clean images.</strong></p>
<p><strong>We study the distribution of softmax induced by stochastic transformations. We observe that with random transformations on the clean images, although the mass of the softmax distribution could shift to the wrong class, the resulting distribution of softmax could be used to correct the prediction.</strong></p>
<p><strong>On the adversarial counterparts, with the image transformation, the resulting shapes of the distribution of softmax are similar to the distributions from the clean images.</strong></p>
<p>With these observations, we propose a method to improve existing transformation-based defenses. <strong>We train a separate lightweight distribution classifier to recognize distinct features in the distributions of softmax outputs of transformed images.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=HyxJhCEFDS"><strong><code>Intriguing Properties Of Adversarial Training At Scale</code></strong></a><br>We provide the first rigorous study on diagnosing elements of large-scale adversarial training on ImageNet, which reveals two intriguing properties.</p>
<p><strong>First, we study the role of normalization, Second, we study the role of network capacity.</strong></p>
<p>Batch Normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but <strong>we show it may prevent networks from obtaining strong robustness in adversarial training.</strong></p>
<p>One unexpected observation is that, <strong>for models trained with BN, simply removing clean images from training data largely boosts adversarial robustness</strong>, <strong>We relate this phenomenon to the hypothesis that clean images and adversarial images are drawn from two different domains.</strong> This two-domain hypothesis may explain the issue of BN when training with a mixture of clean and adversarial images, as estimating normalization statistics of this mixture distribution is challenging.</p>
<p>Guided by this two-domain hypothesis, we show disentangling the mixture distribution for normalization, i.e., <strong>applying separate BNs to clean and adversarial images for statistics estimation, achieves much stronger robustness</strong>. Additionally, we find that enforcing BNs to behave consistently at training and testing can further enhance robustness.</p>
<p>We find our so-called “deep” networks are still shallow for the task of adversarial learning. Unlike traditional classification tasks where accuracy is only marginally improved by adding more layers to “deep” networks (e.g., ResNet-152), adversarial training exhibits a much stronger demand on deeper networks to achieve higher adversarial robustness. This robustness improvement can be observed substantially and consistently even by pushing the network capacity to an unprecedented scale.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=rJx1Na4Fwr"><strong><code>MACER: Attack-Free And Scalable Robust Training via Maximizing Certified Radius</code></strong></a><br>We propose the MACER algorithm, which <strong>learns robust models without using adversarial training but performs better than all existing provable $l_2$-defenses.</strong></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf">Recent work</a> <strong>shows that randomized smoothing can be used to provide a certified $l_2$ radius to smoothed classifiers, and our algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER). The attack-free characteristic makes MACER faster to train and easier to optimize.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=ByxtC2VtPB"><strong><code>Mixup Inference: Better Exploiting Mixup To Defend Adversarial Attacks --Jun Zhu</code></strong></a><br><strong>Applying mixup in training provides an effective mechanism to</strong> improve generalization performance and <strong>model robustness</strong> against adversarial perturbations, <strong>which introduces the globally linear behavior in-between training examples.</strong></p>
<p>But <strong>the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited.</strong>  Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions.(由于对抗扰动有局部性, 利用模型预测的全局性能更有效地破坏对抗扰动的局部性, 从而提升鲁棒性)</p>
<p>Inspired by simple geometric intuition, <strong>we develop an inference principle, named mixup inference (MI), for mixup-trained models.</strong> <strong>MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial.</strong>(我没理解错的话就是把一些 adversarial examples 和一些 clean images 给 mixup 起来做 adversarial training……)</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=HkeryxBtPB"><strong><code>MMA Training: Direct Input Space Margin Maximization Through Adversarial Training</code></strong></a><br>We study adversarial robustness of neural networks from a margin maximization perspective, where margins are defined as the distances from inputs to a classifier’s decision boundary.(从最大边缘的角度来看待鲁棒性, 这里的边缘是指输入数据到决策边界的距离)</p>
<p><strong>Our study shows that maximizing margins can be achieved by minimizing the adversarial loss</strong> on the decision boundary at the “shortest successful perturbation”,** demonstrating a close connection between adversarial losses and the margins.</p>
<p><strong>We propose Max-Margin Adversarial (MMA) training to directly maximize the margins to achieve adversarial robustness.</strong> Instead of adversarial training with a fixed $\epsilon$, <strong>MMA offers an improvement by enabling adaptive selection of the “correct” $\epsilon$ as the margin individually for each data point.</strong></p>
<p><strong>In addition, we rigorously analyze adversarial training with the perspective of margin maximization, and provide an alternative interpretation for adversarial training, maximizing either a lower or an upper bound of the margins.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SJlHwkBYDH"><strong><code>Nesterov Accelerated Gradient And Scale Invariance For Adversarial Attacks</code></strong></a><br><strong>Under the blackbox setting, most existing adversaries often have a poor transferability to attack other defense models.</strong></p>
<p>In this work, <strong>from the perspective of regarding the adversarial example generation as an optimization process, we propose two new methods to improve the transferability</strong> of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM).</p>
<p>NI-FGSM aims to adapt Nesterov accelerated gradient into the iterative attacks so as to effectively look ahead and improve the transferability of adversarial examples.</p>
<p>SIM is based on our discovery on the scale-invariant property of deep learning models, for which we leverage to optimize the adversarial perturbations over the scale copies of the input images so as to avoid “overfitting” on the white-box model being attacked and generate more transferable adversarial examples.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=r1lF_CEYwS"><strong><code>On The Need For Topology-aware Generative Models For Manifold-based Defenses</code></strong></a><br><strong>One of the important classes of defenses are manifoldbased defenses, where a sample is “pulled back” into the data manifold before classifying. These defenses rely on the assumption that data lie in a manifold of a lower dimension than the input space. These defenses use a generative model to approximate the input distribution.</strong></p>
<p><strong>We prove that the generative models used in manifold-based defenses need to be topology-aware.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SyevYxHtDB"><strong><code>Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks</code></strong></a><br>Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks.(现在的针对偷模型的防御是很被动的, 例如拒绝攻击者不断地利用模型的预测, 但这种方法对抗偷模型的效果并不好)</p>
<p>In this paper, <strong>we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker.</strong></p>
<p><strong>Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=rklk_ySYPB"><strong><code>Provable Robustness Against All Adversarial $l_p$-Perturbations For p ≥ 1</code></strong></a><br>Often seemingly robust models turn out to be non-robust when more sophisticated attacks are used. One way out of this dilemma are provable robustness guarantees.</p>
<p>While provably robust models for specific $l_p$-perturbation models have been developed, we show that they do not come with any guarantee against other $l_q$-perturbations.</p>
<p>We propose a new regularization scheme, MMR-Universal, for ReLU networks which enforces robustness wrt $l_1-$ and $l_{\infty}-$ perturbations and show how that leads to the first provably robust models wrt any $l_p \space norm$ for $p$ ≥ 1.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=Skxd6gSYDS"><strong><code>Query-Efficient Meta Attack To Deep Neural Networks</code></strong></a><br><strong>We propose a meta attack approach that is capable of attacking a targeted model with much fewer queries. Its high queryefficiency stems from effective utilization of meta learning approaches in learning generalizable prior abstraction from the previously observed attack patterns and exploiting such prior to help infer attack patterns from only a few queries and outputs.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=H1lZJpVFvr"><strong><code>Robust Local Features For Improving The Generalization of Adversarial Training</code></strong></a><br>Adversarially trained models often lack adversarially robust eneralization on unseen testing data. <strong>Recent works show that adversarially trained models are more biased towards global structure features.</strong>(经过对抗训练得到的模型通常在未知数据上缺少泛化性, 最近的工作表明对抗训练得到的模型更偏向于学习全局的结构特征)</p>
<p><strong>We would like to investigate the relationship between the generalization of adversarial training and the robust local features, as the robust local features generalize well for unseen shape variation.</strong>(想研究对抗训练的泛化性和局部鲁棒特征之间的关系, 因为局部鲁棒性很好地概括了看不见的形状变化)</p>
<p>To learn the robust local features, we develop a Random Block Shuffle (RBS) transformation to <strong>break up the global structure features on normal adversarial examples</strong>.(首先对正常的对抗样本做一个随机阻塞变化以破坏其全局结构特征)</p>
<p>We continue to propose a new approach called Robust Local Features for Adversarial Training (RLFAT), which first <strong>learns the robust local features by adversarial training on the RBS-transformed adversarial examples, and then transfers the robust local features into the training of normal adversarial examples.</strong>(然后从被破坏了全局结构特征的对抗样本中提取局部鲁棒特征, 再把这些对抗样本恢复为正常的对抗样本)</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SklTQCNtvS"><strong><code>SIGN-OPT: A Query-Efficient Hard-Label Adversarial Attack</code></strong></a><br>Several algorithms have been proposed for <strong>hard-label black-box</strong> attack but they typically require huge amount (&gt;20,000) of queries for attacking one example.</p>
<p><strong><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=rJlk6iRqKX">Previous work </a>showed that hard-label attack can be modeled as an optimization problem</strong> where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied.</p>
<p><strong>We adopt the same optimization formulation but propose to directly estimate the sign of gradient at any direction instead of the gradient itself</strong>, which enjoys the benefit of single query.</p>
<p>Using this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=Skxuk1rFwB"><strong><code>Towards Stable And Efficient Training of Verifiably Robust Neural Networks</code></strong></a><br>Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures.(几种现有方法在扰动下利用基于线性松弛的神经网络输出范围, 但根据网络结构的不同, 训练速度会下降很多倍)</p>
<p>Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at the beginning of training.(基于 “间隔边界传播” 的方法相比于基于线性松弛的方法而言有效且显著地提高了效果, 但这种方法容易在训练开始时容易遇到稳定性问题.)</p>
<p>We propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass and a tight linear relaxation based bound, CROWN, in a backward bounding pass.(我们提出的方法在于联合前向边界传递中的 “间隔边界传播” 和在反向边界传递中的 “严格线性松弛”)</p>
<p>CROWN-IBP is computationally efficient and consistently outperforms IBP baselines on training verifiably robust neural networks.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=rJxAo2VYwr"><strong><code>Transferable Perturbations of deep feature Distributions</code></strong></a></p>
<p>This work <strong>presents a new adversarial attack based on the modeling and exploitation of class-wise and layer-wise deep feature distributions.</strong></p>
<p><strong>Our methodology affords an analysis of how adversarial attacks change the intermediate feature distributions of CNNs, as well as a measure of layer-wise and class-wise feature distributional separability/entanglement.</strong>(我们的方法对对抗攻击如何改变中间特征的分布进行了研究, 这也可以看作是对 layer-wise 和 class-wise 特征的可分离性和纠缠度的一种度量)</p>
<p>We also conceptualize a transition from task/data-specific to model-specific features within a CNN architecture that directly impacts the transferability of adversarial examples.(概念化了一种转换, 这种转换把特征从 task/data-specific 转换为 model-specific, 这样直接提高了对抗样本的迁移性)</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=rJgzzJHtDB"><strong><code>Triple Wins: Boosting Accuracy, Robustness And Efficiency Together by Enabling Input- Adaptive Inference</code></strong></a><br>Models cannot both have good accuracy and robustness is shown to be rooted in the inherently higher sample complexity and/or model capacity, for learning a high-accuracy and robust classifier.</p>
<p>In view of that, give a classification task, <strong>growing the model capacity appears to help draw a win-win between accuracy and robustness</strong>, yet at the expense of model size and latency, therefore posing challenges for resource-constrained applications.</p>
<p><strong>This paper studies multi-exit networks associated with input-adaptive efficient inference, showing their strong promise in achieving a “sweet point” in cooptimizing model accuracy, robustness and efficiency.</strong></p>
<p>Our method allows for each input (either clean or adversarial) to adaptively choose one of the multiple output layers (early branches or the final one) to output its prediction. That multi-loss adaptivity adds new variations and flexibility to adversarial attacks and defenses, on which we present a systematical investigation.</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=B1gX8kBtPr"><strong><code>Universal Approximation With Certified Networks</code></strong></a></p>
<p>We prove that for every continuous function $f$, there exists a network $n$ such that: (i) $n$ approximates $f$ arbitrarily close, and (ii) simple interval bound propagation of a region $B$ through $n$ yields a result that is arbitrarily close to the optimal output of $f$ on $B$.(一个区域 $B$ 经过网络 $n$ 能得到近似 $B$ 在函数 $f$ 上的最佳结果)</p>
<p><strong>Our result can be seen as a Universal Approximation Theorem for interval-certified ReLU networks.</strong> To the best of our knowledge, this is <strong>the first work to prove the existence of accurate, interval-certified networks.</strong></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=Sye_OgHFwH"><strong><code>Unrestricted Adversarial Examples via Smantic Manipulations</code></strong></a></p>
<p>We instead introduce “unrestricted” perturbations that <strong>manipulate semantically meaningful image-based visual descriptors – color and texture – in order to generate effective and photorealistic adversarial examples.</strong></p>
<p>We show that <strong>these semantically aware perturbations are effective against JPEG compression, feature squeezing and adversarially trained model.</strong> We also show that <strong>the proposed methods can effectively be applied to both image classification and image captioning tasks on complex datasets</strong></p>
<h2 id="ICML"><a href="#ICML" class="headerlink" title="ICML"></a>ICML</h2><h3 id="2019-17-papers"><a href="#2019-17-papers" class="headerlink" title="2019(17 papers)"></a>2019(17 papers)</h3><p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/bojchevski19a/bojchevski19a.pdf"><strong><code>Adversarial Attacks on Node Embeddings via Graph Poisoning</code></strong></a></p>
<p>We <strong>provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks</strong>. We <strong>derive efficient adversarial perturbations</strong> that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks.</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/li19j/li19j.pdf"><strong><code>Adversarial camera stickers: A physical camera-based attack on deep learning systems</code></strong></a></p>
<p>We show that by placing a carefully crafted and mainly-translucent sticker over the lens of a camera, one can create universal perturbations of the observed images that are inconspicuous, yet misclassify target objects as a different (targeted) class.(就是用个东西挡住相机的一部分, 这样也能产生对抗样本)</p>
<p>To accomplish this, <strong>we propose an iterative procedure for both updating the attack perturbation (to make it adversarial for a given classifier), and the threat model itself (to ensure it is physically realizable)</strong></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf"><strong><code>Adversarial Examples Are a Natural Consequence of Test Error in Noise</code></strong></a></p>
<p>We establish close connections between the adversarial robustness and corruption robustness research programs, with the strongest connection in the case of additive Gaussian noise.(为图像添加高斯模糊这种 corruption(图像损坏) 的方法也能误导分类器, 这篇文章想说明 adversarial robustness 和 corruption robustness 有很紧密的联系…尤其是在加高斯噪声的时候)</p>
<p><strong>This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions.</strong></p>
<p>Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as ImageNet-C.</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/bubeck19a/bubeck19a.pdf"><strong><code>Adversarial examples from computational constraints</code></strong></a></p>
<p><strong>We show that classifiers in high dimension are vulnerable to “adversarial” perturbations due to the computational constraints instead of information theoretic limitations</strong>.(有高维度的分类器容易被攻击是因为计算限制而不是信息理论的局限性)</p>
<p>We prove that for classification tasks, the robust model can be found by a  exponential-time algorithm with relatively few training examples. (我们证明了对分类来说, 鲁棒的分类器可以在指数时间内以较少的训练样本来找到)</p>
<p>Then construct two classification tasks where learning a robust classifier is computationally intractable.More precisely we construct two binary classifications task in high dimensional space which are (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (nonrobustly) by a simple linear separator, (iii) yet are not efficiently robustly learnable, even for small perturbations.</p>
<p>The results suggest that <strong>adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms.</strong></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/li19a/li19a.pdf"><strong><code>Are Generative Classifiers More Robust to Adversarial Attacks?</code></strong></a></p>
<p><strong>Most recent work focuses on discriminative classifiers</strong>, which only model the conditional distribution of the labels given the inputs.</p>
<p><strong>We propose and investigate the deep Bayes classifier, which improves classical naive Bayes with conditional deep generative models</strong></p>
<p>Experimental results suggest that deep Bayes classifiers are more robust than deep discriminative classifiers.</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf"><strong><code>Certified Adversarial Robustness via Randomized Smoothing</code></strong></a><br><strong>We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $l_2$ norm, and we establish a close connection between $l_2$ robustness and Gaussian noise.</strong>(如何把在高斯噪声下分类效果好的分类器转化为在 $l_2$ norm 限制下有鲁棒性的新分类器, 也在鲁棒性和高斯噪声之间建立了紧密的联系)</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/dohmatob19a/dohmatob19a.pdf"><strong><code>Generalized No Free Lunch Theorem for Adversarial Robustness</code></strong></a></p>
<p>We show that if conditioned on a class label the data distribution satisfies the W2 Talagrand transportation-cost inequality.</p>
<p><strong>Any classifier can be adversarially fooled with high probability once the perturbations are slightly greater than the natural noise level in the problem.</strong></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/qin19a/qin19a.pdf"><strong><code>Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition</code></strong></a></p>
<p><strong>The adversarial examples to speech recognition have neither of these properties</strong>: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air.</p>
<p><strong>This paper makes advances on both of these fronts.</strong></p>
<p>First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets.</p>
<p>Next, we make progress towards physical-world over-the-air audio adversarial examples by constructing perturbations which remain effective even after applying realistic simulated environmental distortions.</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/pang19a/pang19a.pdf"><strong><code>Improving Adversarial Robustness via Promoting Ensemble Diversity --Jun Zhu</code></strong></a></p>
<p><strong>This paper presents a new method that explores the interaction among individual networks to improve robustness for ensemble models.</strong></p>
<p>we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal predictions of individual members.(我们定义了集成多样性的新概念, 即个体成员的非最大值预测之间的多样性????????).</p>
<p>And present an adaptive diversity promoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examples difficult to transfer among individual members.</p>
<p>这篇文章定义了一个集成多样性的概念, 又提出了一个叫 ADP 的方法来鼓励集成多样性, 使得对抗样本难以在这些单个模型中迁移, 从而提高集成模型的鲁棒性.</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/yang19e/yang19e.pdf"><strong><code>ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation</code></strong></a></p>
<p><strong>This paper proposes ME-Net, a defense method that leverages matrix estimation (ME).</strong></p>
<p>In ME-Net, images are preprocessed using two steps:</p>
<ol>
<li>pixels are randomly dropped from the image;</li>
<li>the image is reconstructed using ME.</li>
</ol>
<p><strong>We show that this process destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image.</strong></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/li19g/li19g.pdf"><strong><code>NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks</code></strong></a></p>
<p>In this paper, we propose a black-box adversarial attack algorithm.</p>
<p><strong>Instead of searching for an “optimal” adversarial example for a benign input to a targeted DNN, our algorithm finds a probability density distribution over a small region centered around the input, such that a sample drawn from this distribution is likely an adversarial example, without the need of accessing the DNN’s internal layers or weights.</strong></p>
<p>Our approach is universal as it can successfully attack different neural networks by a single algorithm.</p>
<p>our results reveal that <strong>adversarial training remains one of the best defense techniques</strong>, and <strong>the adversarial examples are not as transferable across defended DNNs as them across vanilla DNNs</strong></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/liu19h/liu19h.pdf"><strong><code>On Certifying Non-Uniform Bounds against Adversarial Attacks</code></strong></a></p>
<p><strong>This work studies the robustness certification problem of neural network models</strong>, which aims to find certified adversary-free regions as large as possible around data points.</p>
<p><strong>In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the decision boundary of neural network models.</strong></p>
<p>We formulate our target as an optimization problem with nonlinear constraints. Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits so that the relaxed problem can be solved by the augmented Lagrangian method.</p>
<p><strong>Experiments show the non-uniform bounds have larger volumes than uniform ones and the geometric similarity of the  non-uniform bounds gives a quantitative, data agnostic metric of input features’ robustness.</strong></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/etmann19a/etmann19a.pdf"><strong><code>On the Connection Between Adversarial Robustness and Saliency Map Interpretability</code></strong></a></p>
<p><strong>Models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts.</strong></p>
<p><strong>We aim to quantify this behavior by considering the alignment between input image and saliency map.</strong></p>
<p>We hypothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models.</p>
<p>We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/moon19a/moon19a.pdf"><strong><code>Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization</code></strong></a></p>
<p>This paper <strong>proposes a method to significantly reduce the required queries</strong> compared to recently proposed methods.</p>
<p>Specifically, <strong>they propose an efficient discrete surrogate  (离散替代) to the optimization problem which does not require estimating the gradient and consequently becomes free of the first order update hyperparameters to tune.</strong></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.10660.pdf"><strong><code>Robust Decision Trees Against Adversarial Examples</code></strong></a></p>
<p>In this paper, <strong>we show that tree based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees.</strong></p>
<p>Our method aims to optimize the performance under the worst case perturbation of input features, which leads to a max-min saddle point problem.</p>
<p>To make our approach practical and scalable, we propose efficient tree building algorithms by approximating the inner minimizer in this saddle point problem, and present efficient implementations for classical information gain based trees as well as state-of-the-art tree boosting models such as XGBoost.</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/guo19a/guo19a.pdf"><strong><code>Simple Black-box Adversarial Attacks</code></strong></a></p>
<p>Propose a simple method to generate adversarial examples <strong>efficiently. (less query times)</strong></p>
<p>With only the mild assumption of continuous-valued confidence scores, our highly query-efficient algorithm utilizes the following simple iterative principle:  <strong>we randomly sample a vector from a predefined orthonormal basis and either add or subtract it to the target image.</strong></p>
<p>Despite its simplicity, the proposed method can be used for both untargeted and targeted attacks – resulting in previously unprecedented query efficiency in both settings.</p>
<p>We argue that our proposed algorithm should serve as a strong baseline for future black-box attacks, in particular because it is <strong>extremely fast</strong> and <strong>its implementation requires less than 20 lines of PyTorch code.</strong></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/roth19a/roth19a.pdf"><strong><code>The Odds are Odd: A Statistical Test for Detecting Adversarial Examples</code></strong></a></p>
<p><strong>We investigate conditions under which test statistics exist that can reliably detect examples.</strong>(我们研究了能够可靠检测对抗样本的测试统计信息存在的条件)</p>
<p><strong>These statistics can be easily computed and calibrated by randomly corrupting inputs.</strong></p>
<p>They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints.Access to the log-odds is the only requirement to defend models.</p>
<p>We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective.</p>
<h3 id="2020-24-papers"><a href="#2020-24-papers" class="headerlink" title="2020(24 papers)"></a>2020(24 papers)</h3><p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/1894-Paper.pdf"><strong><code>Adversarial Attacks on Copyright Detection Systems</code></strong></a><br>This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks.</p>
<p><strong>We describe a well-known music identification method and implement this system</strong> in the form of a neural net. We <strong>then attack this system using simple gradient methods and show that it is easily broken with white-box attacks.</strong></p>
<p>By scaling these perturbations up, we can create transfer attacks on industrial systems, such as the AudioTag copyright detector and YouTube’s Content ID system, using perturbations that are audible but significantly smaller than a random baseline.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/526-Paper.pdf"><strong><code>Adversarial Attacks on Probabilistic Autoregressive Forecasting Models</code></strong></a><br><strong>We develop an effective generation of adversarial attacks on neural models that output a sequence of probability distributions rather than a sequence of single values.</strong></p>
<p>The key technical challenge we address is effectively differentiating through the Monte-Carlo estimation of statistics of the joint distribution of the output sequence.</p>
<p>We extend prior work on probabilistic forecasting to the Bayesian setting which allows conditioning on future observations, instead of only on past observations.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/5336-Paper.pdf"><strong><code>Adversarial Nonnegative Matrix Factorization</code></strong></a><br>To overcome this limitation that existing NMF models are still vulnerable to adversarial attacks, <strong>we propose a novel Adversarial NMF (ANMF) approach in which an adversary can exercise some control over the perturbed data generation process.</strong></p>
<p>Different from the traditional NMF models which focus on either the regular input or certain types of noise, <strong>our model considers potential test adversaries that are beyond the pre-defined constraints, which can cope with various noises (or perturbations).</strong></p>
<p>We formulate the proposed model as a bilevel optimization problem and use Alternating Direction Method of Multipliers (ADMM) to solve it with convergence analysis.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/2962-Paper.pdf"><strong><code>Adversarial Risk via Optimal Transport and Optimal Couplings</code></strong></a><br><strong>We investigate adversarial risk classifers from an optimal transport perspective.</strong></p>
<p><strong>We present a new and simple approach to show that the optimal adversarial risk for binary classifcation with 0 − 1 loss function is completely characterized by an optimal transport cost between the probability distributions of the two classes.</strong></p>
<p>Propose a novel coupling strategy that achieves the optimal transport cost for several univariate distributions like Gaussian, uniform, and triangular. Using the optimal couplings, we obtain the optimal adversarial classifers in these settings and show how they differ from optimal classifers in the absence of adversaries.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/3863-Paper.pdf"><strong><code>Adversarial Robustness Against the Union of Multiple Perturbation Models</code></strong></a><br>While most work has defended against a single type of attack, recent work has looked at defending against multiple perturbation models using simple aggregations of multiple attacks(目前大多数防御方法只能防御一种攻击, 最近的工作是直接把多种攻击聚合起来做对抗训练以防止多种攻击), however,  these methods can be difficult to tune, and can easily result in imbalanced degrees of robustness to individual perturbation models, resulting in a sub-optimal worst-case loss over the union.(这些方法可能难以调整，并且很容易导致各个扰动模型的鲁棒性程度不平衡，从而导致在联合中出现次优的最坏情况损失。)</p>
<p><strong>We develop a natural generalization of the standard PGD-based procedure to incorporate(合并) multiple perturbation models into a single attack, by taking the worst-case over all steepest descent directions.</strong></p>
<p>This approach has the advantage of directly converging upon a trade-off between different perturbation models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against $ℓ_∞$, $ℓ_2$, and $ℓ_1$ attacks, outperforming past approaches.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/4379-Paper.pdf"><strong><code>Adversarial Robustness for Code</code></strong></a><br><strong>Explore that code system</strong>(including fnding and fxing bugs, code completion, decompilation, malware detection, type inference) <strong>are vulnerable to adversarial attacks</strong> by:</p>
<ol>
<li>instantiating adversarial attacks for code (a domain with discrete and highly structured inputs)</li>
<li>showing that, similar to other domains, neural models for code are vulnerable to adversarial attacks</li>
<li>developing a set of novel techniques that enable training robust and accurate models of code.</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/520-Paper.pdf"><strong><code>Adversarial Robustness via Runtime Masking and Cleansing</code></strong></a><br>We raise a fundamental question—<strong>do we have to trade off natural generalization for adversarial robustness?</strong></p>
<p><strong>We argue that adversarial training is to employ confident adversarial data for updating the current model.</strong></p>
<p>We propose a novel formulation of friendly adversarial training (FAT): <strong>rather than employing most adversarial data maximizing the loss, we search for least adversarial data (i.e., friendly adversarial data) minimizing the loss</strong>, among the adversarial data that are confidently misclassified.(在这些最容易被分类错误的 adversarial data 中找到最少的数据, 使其最小化 loss 而不是用最多的数据来最大化 loss, 并以此进行对抗训练)</p>
<p>Our novel formulation is easy to implement by just stopping the most adversarial data searching algorithms such as PGD (projected gradient descent) early, which we call early-stopped PGD.</p>
<p>Theoretically, FAT is justified by an upper bound of the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier question negatively—adversarial robustness can indeed be achieved without compromising the natural generalization.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/5833-Paper.pdf"><strong><code>Black-box Certiﬁcation and Learning under Adversarial Perturbations</code></strong></a><br><strong>We analyze a PAC type framework of semi-supervised learning and identify possibility and impossibility results for proper learning of VC-classes in this setting.</strong></p>
<p>We further introduce and study a new setting of blackbox certification under limited query budget. We analyze this for various classes of predictors and types of perturbation.</p>
<p>We also consider the viewpoint of a black-box adversary that aims at finding adversarial examples, showing that the existence of an adversary with polynomial query complexity implies the existence of a robust learner with small sample complexity.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/4835-Paper.pdf"><strong><code>Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks</code></strong></a><br>Our confidencecalibrated adversarial training (CCAT) tackles this problem(robustness does not generalize to previously unseen threat models) by <strong>biasing the model towards low confidence predictions on adversarial examples. By allowing to reject examples with low confidence, robustness generalizes beyond the threat model employed during training.</strong></p>
<p>CCAT, trained only on $L_1$ adversarial examples, increases robustness against larger $L_{\infty}$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/6130-Paper.pdf"><strong><code>Efficiently Learning Adversarially Robust Halfspaces with Noise</code></strong></a><br><strong>We study the problem of learning adversarially robust halfspaces in the distribution-independent setting.</strong>(我们研究在分布独立的情况下学习对抗性鲁棒半空间的问题)</p>
<p>In the realizable setting, we provide necessary and sufficient conditions on the adversarial perturbation sets under which halfspaces are efficiently  robustly learnable.</p>
<p>In the presence of random label noise, we give a simple computationally  efficient algorithm for this problem with respect to any $ℓ_p$-perturbation.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/5465-Paper.pdf"><strong><code>Fundamental Tradeoffs between Invariance and  Sensitivity to Adversarial Perturbations</code></strong></a><br><strong>This paper studies a complementary failure mode, invariance-based adversarial examples, that introduce minimal semantic changes that modify an input’s true label yet preserve the model’s prediction.</strong></p>
<p>We show that defenses against sensitivity-based attacks actively harm a model’s accuracy on invariance-based attacks, and that new approaches are needed to resist both attack types.</p>
<p>Finally, we formally show that the existence of excessively invariant classifiers arises from the presence of overly-robust predictive features in standard datasets.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/381-Paper.pdf"><strong><code>Implicit Euler Skip Connections: Enhancing Adversarial Robustness via Numerical Stability</code></strong></a><br><strong>We try to address such an issue that CNNs are vulnerable to adversarial examples from the perspective of dynamic system in this work.</strong></p>
<p>By viewing ResNet as an explicit Euler discretization of an ordinary differential equation (ODE), for the frst time, we fnd that the adversarial robustness of ResNet is connected to the numerical stability of the corresponding dynamic system, i.e., more stable numerical schemes may correspond to more robust deep networks(通过把 ResNet 看成是一个常微分方程的显式欧拉离散化来和动态系统的数值稳定性联系起来)</p>
<p>Furthermore, inspired by the implicit Euler method for solving numerical ODE  problems, we propose Implicit Euler skip connections (IE-Skips) by modifying the original skip connection in ResNet or its variants.(用数学的方法来解决…)</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/5124-Paper.pdf"><strong><code>Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization</code></strong></a><br>We develop a notion of representation vulnerability that captures the maximum change of mutual(相互的) information between the input and output distributions, under the worst-case input perturbation.</p>
<p>We prove a theorem that establishes a lower bound on the minimum adversarial risk that can be achieved for any downstream classifier based on its representation vulnerability.</p>
<p><strong>We propose an unsupervised learning method for obtaining intrinsically robust representations by maximizing the worst-case mutual information between the input and output distributions.</strong></p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/6735-Paper.pdf"><strong><code>Minimally Distorted Adversarial Examples with a Fast Adaptive Boundary Attack</code></strong></a><br><strong>We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in {1, 2, \infty}$ aiming at finding the minimal perturbation necessary to change the class of a given input.</strong></p>
<p>It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run).</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/5885-Paper.pdf"><strong><code>Neural Network Control Policy Verification with Persistent Adversarial Perturbations</code></strong></a><br><strong>We show how to combine recent works on static neural network certification tools with robust control theory to certify a neural network policy in a control loop.</strong></p>
<p>We give a sufficient condition and an algorithm to ensure that the closed loop state and control constraints are satisfied when the persistent adversarial perturbation is $l_{\infty}$ norm bounded.</p>
<p>Our method is based on finding a positively invariant set of the closed loop dynamical system, and thus we do not require the continuity of the neural network policy. We also develop an effective attack strategy for neural network control systems that outperforms exhaustive Monte-Carlo search significantly.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/2236-Paper.pdf"><strong><code>On Breaking Deep Generative Model-based Defenses and Beyond</code></strong></a><br>These generative-based defense often predict by inverting the deep generative models rather than simple feedforward propagation. Such defenses are difficult to attack due to obfuscated gradient.</p>
<p><strong>We develop a new gradient approximation attack to break these defenses. The idea is to view the inversion phase as a dynamical system, through which we extract the gradient w.r.t the input by tracing its recent trajectory.</strong>(轨道)</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/1661-Paper.pdf"><strong><code>Proper Network Interpretability Helps Adversarial Robustness in Classiﬁcation</code></strong></a><br>We theoretically show that with a proper measurement of interpretation, it is actually difficult to prevent prediction-evasion adversarial attacks from causing interpretation discrepancy(差异), (利用一个合适的可解释性措施, 很难防止 prediction-evasion 攻击引起的可解释性差异)</p>
<p><strong>We develop an interpretability-aware defensive scheme built only on promoting robust interpretation (without the need for resorting to adversarial loss minimization</strong></p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/2479-Paper.pdf"><strong><code>Randomization matters  How to defend against strong adversarial attacks</code></strong></a><br><strong>We present the adversarial attacks and defenses problem as an infinite zero-sum game where classical results do not apply.</strong></p>
<p>We demonstrate the nonexistence of a Nash equilibrium in our game when the classifier and the Adversary are both deterministic, hence giving a negative answer to the above question in the deterministic regime.</p>
<p>We tackle this problem by showing that, under mild conditions on the dataset distribution, any deterministic classifier can be outperformed by a randomized one. This gives arguments for using randomization, and leads us to a simple method for building randomized classifiers that are robust to state-or-the-art adversarial attacks.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/6846-Paper.pdf"><strong><code>Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks</code></strong></a><br><strong>We first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function.</strong></p>
<p>We then <strong>combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness</strong></p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/2933-Paper.pdf"><strong><code>Second-Order Provable Defenses against Adversarial Attacks</code></strong></a><br><strong>Computing exact robustness certificates for neural networks is difficult in general since it requires solving a non-convex optimization.</strong></p>
<p><strong>We provide computationally-efficient robustness certificates</strong> for neural networks with differentiable activation functions in two steps:</p>
<ol>
<li>We show that if the eigenvalues of the Hessian of the network (curvatures of the network) are bounded (globally or locally), we can compute a robustness certificate in the $l_2$ norm efficiently using convex optimization.</li>
<li>We derive a computationally-efficient differentiable upper bound on the curvature of a deep network.</li>
</ol>
<p>We also use the curvature bound as a regularization term during the training of the network to boost its certified robustness.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/3175-Paper.pdf"><strong><code>Sharp Statistical Guarantees for Adversarially Robust Gaussian Classification</code></strong></a><br><strong>We provide the first result of the optimal minimax guarantees for the excess risk for adversarially robust classification, under Gaussian mixture model</strong></p>
<p>The results are stated in terms of the Adversarial Signal-toNoise Ratio (AdvSNR), which generalizes a similar notion for standard linear classification to the adversarial setting.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/724-Paper.pdf"><strong><code>Stronger and Faster Wasserstein Adversarial Attacks</code></strong></a><br>Compared to $l_p$ norm metric,  Wasserstein distance, which takes geometry in pixel space into account, has long known to be a better metric for measuring image quality and has recently risen as a compelling alternative to the $l_p$ metric in adversarial attacks.</p>
<p>However, c<strong>onstructing an effective attack under the Wasserstein metric is computationally much more challenging and calls for better optimization algorithms.</strong></p>
<p>We address this gap in two ways:</p>
<ol>
<li>we develop an exact yet efficient projection operator to enable a stronger projected gradient attack</li>
<li>we show for the first time that Frank-Wolfe method equipped with a suitable linear minimization oracle works extremely fast under Wasserstein constraints</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/1310-Paper.pdf"><strong><code>Towards Understanding the Dynamics of the First-Order Adversaries</code></strong></a><br><strong>We analyze the dynamics of the maximization step towards understanding the experimentally observed effectiveness of this defense mechanism.</strong></p>
<p>Specifically, we investigate the non-concave landscape of the adversaries for a two-layer neural network with a quadratic loss. Our main result proves that projected gradient ascent finds a local maximum of this non-concave problem in a polynomial number of iterations with high probability.</p>
<p>Moreover, our analysis demonstrates that, in the initial phase of adversarial training, the scale of the inputs matters in the sense that a smaller input scale leads to faster convergence of adversarial training and a “more regular” landscape.</p>
<p><a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/1057-Paper.pdf"><strong><code>Towards Understanding the Regularization of Adversarial Robustness on Neural Networks</code></strong></a></p>
<p>$\epsilon$-adversarially robust (AR) is observed that such methods would lead to standard performance degradation, i.e., the degradation on natural examples.</p>
<p><strong>In this work, we study the degradation through the regularization perspective.</strong></p>
<p>We identify quantities from generalization analysis of NNs; with the identifed quantities we empirically fnd that AR is achieved by regularizing/biasing NNs towards less confdent solutions by making the changes in the feature space (induced by changes in the instance space) of most layers smoother uniformly in all directions; so to a certain extent, it prevents sudden change in prediction w.r t. perturbations.</p>
<p>Our studies suggest that one might consider ways that build AR into NNs in a gentler way to avoid the problematic regularization.</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ming Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://coderming.cn/2020/09/15/Adversarial-Examples/">https://coderming.cn/2020/09/15/Adversarial-Examples/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Adversarial/">Adversarial</a></div><div class="post_share"><div class="social-share" data-image="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Adversarial-Examples/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/wechat.png" target="_blank"><img class="post-qr-code-img" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/wechat.png" alt="WeChat"/></a><div class="post-qr-code-desc">WeChat</div></li><li class="reward-item"><a href="https://qr.alipay.com/fkx12431y6k0soy7vokzi19" target="_blank"><img class="post-qr-code-img" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/alipay.jpg" alt="AliPay"/></a><div class="post-qr-code-desc">AliPay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/09/15/Introduction-To-Adversarial-Examples/"><img class="prev-cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Adversarial-Examples/avatar.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Introduction to Adversarial Examples</div></div></a></div><div class="next-post pull-right"><a href="/2020/09/01/Weakly-Supervised-Object-Detection/"><img class="next-cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Weakly-Supervised-Object-Detection/avatar.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Weakly Supervised Object Detection</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2020/09/15/Introduction-To-Adversarial-Examples/" title="Introduction to Adversarial Examples"><img class="cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Introduction-To-Adversarial-Examples/avatar.png"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-15</div><div class="title">Introduction to Adversarial Examples</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/09/Adversarial-Examples/avatar.png)"><div id="footer-wrap"><div class="copyright">&copy;2020 By Ming Li</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div></div></body></html>