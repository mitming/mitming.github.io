<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Introduction to Video Action Understanding | Ming</title><meta name="keywords" content="Video,Action"><meta name="author" content="Ming Li"><meta name="copyright" content="Ming Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Introduction本文主要是对 Video Action Understanding: A Tutorial 一文进行翻译和整理, 这是一篇很好的综述文章, MIT 大佬写的, 推荐去看. 首先说明一下几个概念, 在文献中 instance 通常是指一个动作实例, segment 是指从一个视频中裁剪出的一个片段. 作者把视频行为理解分为了下图中的几个部分: Problem Definiti">
<meta property="og:type" content="article">
<meta property="og:title" content="Introduction to Video Action Understanding">
<meta property="og:url" content="https://coderming.cn/2020/10/19/Introduction-to-Video-Action-Understanding/index.html">
<meta property="og:site_name" content="Ming">
<meta property="og:description" content="Introduction本文主要是对 Video Action Understanding: A Tutorial 一文进行翻译和整理, 这是一篇很好的综述文章, MIT 大佬写的, 推荐去看. 首先说明一下几个概念, 在文献中 instance 通常是指一个动作实例, segment 是指从一个视频中裁剪出的一个片段. 作者把视频行为理解分为了下图中的几个部分: Problem Definiti">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/avatar.png">
<meta property="article:published_time" content="2020-10-19T02:23:32.000Z">
<meta property="article:modified_time" content="2020-10-20T09:59:00.474Z">
<meta property="article:author" content="Ming Li">
<meta property="article:tag" content="Video">
<meta property="article:tag" content="Action">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/avatar.png"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://coderming.cn/2020/10/19/Introduction-to-Video-Action-Understanding/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-10-20 17:59:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">19</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/images/"><i class="fa-fw fas fa-images"></i><span> Images</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Problem-Definition"><span class="toc-number">2.</span> <span class="toc-text">Problem Definition</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Action-Recognition"><span class="toc-number">2.1.</span> <span class="toc-text">Action Recognition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Action-Prediction"><span class="toc-number">2.2.</span> <span class="toc-text">Action Prediction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Temporal-Action-Proposal"><span class="toc-number">2.3.</span> <span class="toc-text">Temporal Action Proposal</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Temporal-Action-Localization-Detection"><span class="toc-number">2.4.</span> <span class="toc-text">Temporal Action Localization&#x2F;Detection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spatiotemporal-Action-Proposal"><span class="toc-number">2.5.</span> <span class="toc-text">Spatiotemporal Action Proposal</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spatiotemporal-Action-Localization-Detection"><span class="toc-number">2.6.</span> <span class="toc-text">Spatiotemporal Action Localization&#x2F;Detection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-Problems"><span class="toc-number">2.7.</span> <span class="toc-text">Related Problems</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dataset"><span class="toc-number">3.</span> <span class="toc-text">Dataset</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Action-Recognition-Dataset"><span class="toc-number">3.1.</span> <span class="toc-text">Action Recognition Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Temporally-Annotated-Datasets"><span class="toc-number">3.2.</span> <span class="toc-text">Temporally Annotated Datasets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spatiotemporally-Annotated-Datasets"><span class="toc-number">3.3.</span> <span class="toc-text">Spatiotemporally Annotated Datasets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Competitions"><span class="toc-number">3.4.</span> <span class="toc-text">Competitions</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Data-Preparation"><span class="toc-number">4.</span> <span class="toc-text">Data Preparation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Video-Data"><span class="toc-number">4.1.</span> <span class="toc-text">Video Data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-Augmentation"><span class="toc-number">4.2.</span> <span class="toc-text">Data Augmentation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hand-Crafted-Feature-Extraction"><span class="toc-number">4.3.</span> <span class="toc-text">Hand-Crafted Feature Extraction</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Models"><span class="toc-number">5.</span> <span class="toc-text">Models</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Building-Blocks"><span class="toc-number">5.1.</span> <span class="toc-text">Model Building Blocks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN"><span class="toc-number">5.1.1.</span> <span class="toc-text">CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN"><span class="toc-number">5.1.2.</span> <span class="toc-text">RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fusion"><span class="toc-number">5.1.3.</span> <span class="toc-text">Fusion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SOTA-Model-Architectures"><span class="toc-number">5.2.</span> <span class="toc-text">SOTA Model Architectures</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Action-Recognition-Models"><span class="toc-number">5.2.1.</span> <span class="toc-text">Action Recognition Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Temporal-Action-Proposal-Models"><span class="toc-number">5.2.2.</span> <span class="toc-text">Temporal Action Proposal Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Temporal-Action-Detection-Models"><span class="toc-number">5.2.3.</span> <span class="toc-text">Temporal Action Detection Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spatiotemporal-Action-Detection-Models"><span class="toc-number">5.2.4.</span> <span class="toc-text">Spatiotemporal Action Detection Models</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Metrics"><span class="toc-number">6.</span> <span class="toc-text">Metrics</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-class-Classification-Metrics"><span class="toc-number">6.1.</span> <span class="toc-text">Multi-class Classification Metrics</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Top-k-Category-Accuracy-a-k"><span class="toc-number">6.1.1.</span> <span class="toc-text">Top-k Category Accuracy($a_{k}$)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mean-Average-Precision-mAP"><span class="toc-number">6.1.2.</span> <span class="toc-text">Mean Average Precision (mAP)</span></a></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/avatar.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ming</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/images/"><i class="fa-fw fas fa-images"></i><span> Images</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Introduction to Video Action Understanding</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-10-19T02:23:32.000Z" title="Created 2020-10-19 10:23:32">2020-10-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-10-20T09:59:00.474Z" title="Updated 2020-10-20 17:59:00">2020-10-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Video/">Video</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">4.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>15min</span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文主要是对 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.06647.pdf">Video Action Understanding: A Tutorial</a> 一文进行翻译和整理, 这是一篇很好的综述文章, MIT 大佬写的, 推荐去看.</p>
<p>首先说明一下几个概念, 在文献中 <code>instance</code> 通常是指一个动作实例, <code>segment</code> 是指从一个视频中裁剪出的一个片段.</p>
<p>作者把视频行为理解分为了下图中的几个部分:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/action_understanding.png"></p>
<h1 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h1><h2 id="Action-Recognition"><a href="#Action-Recognition" class="headerlink" title="Action Recognition"></a>Action Recognition</h2><p>定义为对一个完整的视频进行分类的过程, 如果动作在整个视频中都存在, 那么被称为 trimmed action recognition, 否则被称为 untrimmed action recognition, 通常而言 untrimmed action recognition 会更难, 因为此时模型需要在完成分类任务的同时忽略非动作的背景片段.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/action_recognition.png"></p>
<h2 id="Action-Prediction"><a href="#Action-Prediction" class="headerlink" title="Action Prediction"></a>Action Prediction</h2><p>定义为对一个不完整的视频进行分类的过程, 即只知道视频的一部分, 用这一部分信息来对整个视频进行分类.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/action_prediction.png"></p>
<h2 id="Temporal-Action-Proposal"><a href="#Temporal-Action-Proposal" class="headerlink" title="Temporal Action Proposal"></a>Temporal Action Proposal</h2><p>通过求得每个行为实例的开始时间和结束时间来把输入的视频分为若干个动作和非动作的片段(片段是一系列连续的帧).不对每个行为实例进行具体分类.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/temporal_action_proposal.png"></p>
<h2 id="Temporal-Action-Localization-Detection"><a href="#Temporal-Action-Localization-Detection" class="headerlink" title="Temporal Action Localization/Detection"></a>Temporal Action Localization/Detection</h2><p>在 Temporal Action Proposal 的基础上对每个 Proposal(行为实例) 进行分类.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/temporal_action_detection.png"></p>
<h2 id="Spatiotemporal-Action-Proposal"><a href="#Spatiotemporal-Action-Proposal" class="headerlink" title="Spatiotemporal Action Proposal"></a>Spatiotemporal Action Proposal</h2><p>除了给出每个动作在时间上的开始和结束时间外(找到开始帧和结束帧), 还需要在空间上给出 Bounding Box 来框出每个正在进行某个动作的物体. 如果这里使用了某种连接策略来对每个帧的 Bounding Box 进行关联, 那么这个动作区域(时间和空间维度)被称为 tubes 或者 tubelets.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/spatiotemporal_action_proposal.png"></p>
<h2 id="Spatiotemporal-Action-Localization-Detection"><a href="#Spatiotemporal-Action-Localization-Detection" class="headerlink" title="Spatiotemporal Action Localization/Detection"></a>Spatiotemporal Action Localization/Detection</h2><p>在 Spatiotemporal Action Proposal 基础上对每个 Proposal(行为实例) 以及每一帧上的每个 Bounding Box 进行分类.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/spatiotemporal_action_detection.png"></p>
<h2 id="Related-Problems"><a href="#Related-Problems" class="headerlink" title="Related Problems"></a>Related Problems</h2><ol>
<li>Action instance segmentation<br>旨在对单个动作实例打标签, 即使这些动作实例会在时间或者空间上有重叠, 举个例子, 时空动作检测会检测出人的 Bounding Box 以及人鼓掌的时间段, 而 Action Instance Segmentation 要求只检测每个鼓掌的人的 Bounding Box, 并且跟踪每个人鼓掌的时间段.</li>
<li>Dense Caption<br>为视频生成句子描述</li>
<li>Action spotting<br>2018 年提出来的任务, 希望用尽可能少的观察来进行时序行为检测. 这和时序行为检测不同的地方在于, action spotting 只需要找到每个 action instance segment 中的一个帧即可, 无需找到开始帧和结束帧, 其次 action spotting 考虑了搜索过程的效率.</li>
<li>Object Tracking<br>检测目标并且将不同帧上检测到的物体联系起来.</li>
</ol>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>先看下总的数据集情况, Action 中的 <em>Instances</em> 表示动作实例的数目而非视频的总数, Actors 中的 <em>H</em> 和 <em>N</em> 分别表示人类和非人类, Annotations 中的 <em>C, T, S</em> 分别表示分类, 时序检测和时空检测.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/datasets.png"></p>
<h2 id="Action-Recognition-Dataset"><a href="#Action-Recognition-Dataset" class="headerlink" title="Action Recognition Dataset"></a>Action Recognition Dataset</h2><p>可以先看下所有的数据集情况, 这里只介绍一些比较大的数据集.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/action_recognition_datasets.png"></p>
<ol>
<li>Sports-1M<br>2014 年提出, 有 487 个体育项目, 从 YouTube 上收集的数据, 共 100 万个视频, 训练集/验证集/测试集按 7/2/1 的比例划分, 每个视频平均 5.5 分钟, 需要注意的是这个数据集的类间差异很小(有点像细粒度分类), 例如动作里面存在 23 种台球, 6 种保龄球以及 7 种美式足球…</li>
<li>Something-Something<br>v1 版本在 2017 年提出, 有 174 类, 共有 108,499 个视频, 按照 8/1/1 划分训练集, 验证集和测试集, 单个动作实例视频持续 2~6 秒. 在 2018 年扩充到了 v2 版本, 共 220,847 个视频, 增加了物体标注, 减少了 label noise 并提升了分辨率</li>
<li>Kinetics<br>Kinetics-400 有 400 个动作类别, 共 306,245 个视频, 对于每个类, 都选了 50 个视频用于验证, 100 个视频用于测试, 单个动作实例视频持续 10 秒. Kinetics-600 拓展到了 600 个类别, 共 495,547 个视频, Kinetics-700 拓展到了 700 个类别和 650,317 个视频.</li>
</ol>
<h2 id="Temporally-Annotated-Datasets"><a href="#Temporally-Annotated-Datasets" class="headerlink" title="Temporally Annotated Datasets"></a>Temporally Annotated Datasets</h2><p>先看下整体的情况</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/temporal_annotated_datasets.png"></p>
<ol>
<li>ActivityNet<br>ActivityNet-100(v1.2) 有 100 个类, 共 9,682 个视频, 4,819 videos (7,151 instances) 用于训练, 2,383 videos (3,582 instances) 用于验证, 2,480 videos 用于测试.<br>ActivityNet-200(v1.3), 共 200 个类, 19,994 videos, 10,024 videos (15,410 instances) 用于训练, 4,926 videos (7,654 instances) 用于验证以及 5,044 videos 用于测试.</li>
<li>Charades<br>共 157 类, 有 9,848 videos, 66,500 个时序标注, 按照 8/2 来划分训练集和验证集. 每个视频大约 30 秒, 平均有 6.8 个动作, 每个动作实例平均用时 12.8 秒.<br>Charades-Ego 也是 157 类, 只是既有第三视角又有第一视角的视频, 共 7,860 个视频, 68,536 个时序标注. 相比于 ActivityNet 多了个 multi-label AR 评价指标, 因为每个视频中有更多的平均动作数目.</li>
</ol>
<h2 id="Spatiotemporally-Annotated-Datasets"><a href="#Spatiotemporally-Annotated-Datasets" class="headerlink" title="Spatiotemporally Annotated Datasets"></a>Spatiotemporally Annotated Datasets</h2><p>先看下整体的情况</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/spatiotemporally_annotated_datasets.png"></p>
<ol>
<li>UCF101-24<br>这个数据集其实就是 UCF101, 只是其中有 24 类有时空标注, 是 THUMOS 数据集的子集, 对于时空检测任务而言, 有 3,207 个视频.</li>
<li>J-HMDB-21<br>是 HMDB51 的子集, 共 928 个视频, 和 UCF 101 一样按照 7/3 划分训练集和测试集, 每个视频都是一个动作, 每个动作持续整个视频.</li>
<li>EPIC-KITCHENS-55<br>用于以人为中心的厨房行为, 有 149 个类别, 共 432 个视频, 272 用于训练和验证, 106 用于测试集 1 (seen kitchen), 54 用于测试集 2 (unseen kitchen), 分别有 28,561, 8,064 和 2,939个动作实例. 55 是指视频长度共 55 小时. 然后在 2020 年这个数据集被扩充到了 700 个视频(共 100 小时), 89,879 个动作实例.</li>
<li>Atomic Visual Actions (AVA)<br>共 437 个 15 分钟的视频, 按 55/15/30 比例进行划分, 共 60 个类别, 有 214,622 training, 57,472 validation, 和 120,322 test action instances.</li>
<li>AVA-Kinetics<br>从 Kinetics-700 的子集中得到, 共 238,906 个视频, 按照 59/14/27 来划分, 从 AVA 数据集中标注共 80 个动作类别.</li>
</ol>
<h2 id="Competitions"><a href="#Competitions" class="headerlink" title="Competitions"></a>Competitions</h2><p>目前主要是 <a target="_blank" rel="noopener" href="http://activity-net.org/">ActivityNet Large Scale Activity Recognition Challenges</a>, 从 CVPR 2016 一直办到了 CVPR 2020, 有以下 5 个赛道:</p>
<ol>
<li>trimmed action recognition</li>
<li>untrimmed action recognition</li>
<li>temporal action proposal</li>
<li>temporal action detection</li>
<li>spatiotemporal action detection</li>
</ol>
<h1 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h1><p>有的数据集是提供了预处理之后的形式, 有的数据集却是原始的形式(使用原始的 frame rate, frame dimension and duration). 预处理包括以下几种形式:</p>
<ol>
<li>Data Clean<br>移除不完整或者相关的数据, 例如有的数据集是以 YouTube 链接形式存在, 那么肯定有些视频在 YouTube 上消失了, 这时候就需要把这些剔除.</li>
<li>Data Augmentation<br>这个和图像领域是一样的.</li>
<li>Hand-crafted Feature Extraction<br>例如光流</li>
</ol>
<h2 id="Video-Data"><a href="#Video-Data" class="headerlink" title="Video Data"></a>Video Data</h2><p>视频多了个时序的维度, 有些把视频视为 $(frames, channels, height, width)$, 有些则是 $(frames, height, width, channels)$, 维度上的顺序取决于训练环境的不同会导致精度的提升或者下降.</p>
<h2 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h2><ol>
<li>几何增强<br>resize, crop 和 flip 这些数据增强方法, 这些方法需要用于所有的帧, 如果有些帧使用这些有些帧不用的话, 视频会损失很多的语义信息. 除此以外还有一些几何增强方法例如剪切, 分段放射变换和旋转等.</li>
<li>光度增强(photometric augmentation)<br>这种数据增强的方式可以用于任意一帧, 无需对视频中的所有帧都进行这种操作. 包括:</li>
</ol>
<ul>
<li>color jitter(hue, saturation, contrast or brightness)</li>
<li>edge enhancement, 由于网络的浅层会更任意学到检测边缘和梯度, 所以增强边缘能够提高外观信息, 从而加速网络的学习过程.</li>
</ul>
<p>也有一些其他的增强方法, 但用的不多, 包括超分辨率, 随机灰度化, 随机擦除以及 vignetting(渐晕/光晕)</p>
<ol start="3">
<li>时序增强<br>上面的 2 种增强方法都是在空间上做数据增强, 这部分考虑到了时序维度:</li>
</ol>
<ul>
<li>trimming<br>变更视频的开始和结束, 本质上是 video cropping, 对于删除视频中不含有标注动作的区域有用.</li>
<li>Sampling<br>从视频中提取帧的过程, 本质上是 video resizing, 可以根据特定下标或者随机选择帧</li>
<li>Looping<br>重复一个视频中的某些帧来增加视频的长度, 本质上是 video padding, 当视频有的帧比模型输入少的时候会比较有用.</li>
</ul>
<h2 id="Hand-Crafted-Feature-Extraction"><a href="#Hand-Crafted-Feature-Extraction" class="headerlink" title="Hand-Crafted Feature Extraction"></a>Hand-Crafted Feature Extraction</h2><p>手工特征提取主要是提取动作表示(motion representations), 分为 2 种经典的理论: 拉格朗日流和欧拉流</p>
<ol>
<li>拉格朗日流<br>Lagrangian flow fields track individual parcel or particle motion, 在视频中表示为通过查看相邻帧之间临近的外观信息来判断一个像素是否移动, 最常用的拉格朗日流的 motion representation 就是光流.</li>
<li>欧拉流表征一个动作在特定空间区域的位置, 在视频中表示为一个特定的空间位置在时间维度上产生的视觉信息差异.主要有 RGB difference/derivative (dRGB) 和 phase difference/derivative (dPhase), dRGB 表示相邻帧之间等效空间位置处的 RGB 像素强度差异, 直接用一帧减去另一帧就能得到结果. dPhase 需要先转到频域, 计算完差值之后再转回到时域.</li>
</ol>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/rgb_of_drgb_dphase.png"></p>
<h1 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h1><h2 id="Model-Building-Blocks"><a href="#Model-Building-Blocks" class="headerlink" title="Model Building Blocks"></a>Model Building Blocks</h2><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p>先熟悉下名称表示, 1-Dimensional CNNs (C1D), 2-Dimensional CNNs (C2D), and 3-Dimensional CNNs (C3D), C1D 主要用于在时间维度上做卷积, C2D 和 C3D 用于从单帧和多帧中提取特征. 回顾下 C2D 和 C3D 的区别.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/2d_and_3d_cnn.png"></p>
<p>目前基于 CNN 的网络结构大致有以下几种思路:</p>
<ol>
<li>减少 C3D 的计算消耗<br><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_iccv_2017/html/Qiu_Learning_Spatio-Temporal_Representation_ICCV_2017_paper.html">P3D</a>, [R(2+1)D <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Ghadiyaram_Large-Scale_Weakly-Supervised_Pre-Training_for_Video_Action_Recognition_CVPR_2019_paper.html">link 1</a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html">link 2</a>], <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Appearance-and-Relation_Networks_for_CVPR_2018_paper.html">ARTNet</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ECCV_2018/html/Yunpeng_Chen_Fast_Multi-fiber_Network_ECCV_2018_paper.html">MFNet</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Grouped_Spatial-Temporal_Aggregation_for_Efficient_Action_Recognition_ICCV_2019_paper.html">GST</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Tran_Video_Classification_With_Channel-Separated_Convolutional_Networks_ICCV_2019_paper.html">CSN</a></li>
<li>识别长时间范围的时序依赖<br><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/7940083">LTC-CNN</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Appearance-and-Relation_Networks_for_CVPR_2018_paper.html">NL</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Hussein_Timeception_for_Complex_Action_Recognition_CVPR_2019_paper.html">Timeception</a>, <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320319303383">STDA</a></li>
<li>其他一些独特的模块. 例如 <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.html">TSM</a> 通过在时序维度上 shift 单个通道来提高 2D CNN 的能力, <a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/7489-trajectory-convolution-for-action-recognition">TrajectoryNet</a> 使用一个 <a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Wang_Action_Recognition_With_2015_CVPR_paper.html">TDD-like</a> 的 trajectory convolution 来代替时序卷积.</li>
</ol>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>文中就介绍了下 RNN, GRU, LSTM…我觉得还是 Transformer 更靠谱…</p>
<h3 id="Fusion"><a href="#Fusion" class="headerlink" title="Fusion"></a>Fusion</h3><p>对输入特征, embedded feature 和输出特征进行融合分别被称为 early fusion, slow fusion 和 late fusion(ensemble), 但最近几年还是 Attention 用的多. 这块可以看<a target="_blank" rel="noopener" href="https://web.eecs.umich.edu/~justincj/"> Justin Johnson </a> 开的计算机视觉课程 <a target="_blank" rel="noopener" href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/">EECS 498-007 / 598-005: Deep Learning for Computer Vision</a></p>
<h2 id="SOTA-Model-Architectures"><a href="#SOTA-Model-Architectures" class="headerlink" title="SOTA Model Architectures"></a>SOTA Model Architectures</h2><h3 id="Action-Recognition-Models"><a href="#Action-Recognition-Models" class="headerlink" title="Action Recognition Models"></a>Action Recognition Models</h3><p>作者把动作识别模型分为了 3 类: single-stream, two-stream 和 temporal segmentation.</p>
<p>Single-stream 结构采样或者提取一个 2D/3D 输入特征, 然后扔进 CNN 里面, 输出的结果就是最后的预测, 这种方式通常会缺少时序信息, 导致效果不好.</p>
<p>Two-stream 中一个 stream 用于学习 RGB 特征, 另一个用于学习 motion feature, 由于计算光流或者其他手工特征是非常耗时的, 所以有方法会学习一种”隐式” motion feature, 而非直接使用光流. <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-030-20893-6_23">MotionNet</a> 使用和双流网络相似的结构, 只是把光流替换为了 motion stream, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Crasto_MARS_Motion-Augmented_RGB_Stream_for_Action_Recognition_CVPR_2019_paper.html">MARS</a> 和 <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_WACV_2020/html/Stroud_D3D_Distilled_3D_Networks_for_Video_Action_Recognition_WACV_2020_paper.html">D3D</a> 在 2 个流之间添加 middle fusion(slow fusion), <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Feichtenhofer_Spatiotemporal_Multiplier_Networks_CVPR_2017_paper.html">这篇文章</a> 探讨了不同流之间的门控方式</p>
<p>出了 single-stream, two-stream 以外还有一种是 temporal segmentation architectures, 用于处理行为长时的依赖, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8454294/">TSN</a> 将输入的视频分为 N 个 segments, 从这些 segments 中采样, 通过平均这些 segments 的结果来觉得最终整个 video 的类别. <a target="_blank" rel="noopener" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17205">T-C3D</a>, <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/html/Bolei_Zhou_Temporal_Relational_Reasoning_ECCV_2018_paper.html">TRN</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ECCV_2018/html/Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper.html">ECO</a> 和 <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.html">SlowFast</a> 都在 Temporal Segmentation 的基础上添加了多尺度的 segmentation 和/或 融合</p>
<p>第 4 种范式是复杂度最高的模型, 被称为 two-stage learning, 第一个阶段使用 temporal segment method 来提取 segment embedded feature, 然后第二个阶段在这些特征上进行训练.这种方法包括 <a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html">3D-fusion</a> 以及 CNN+LSTM.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/action_recognition_models.png"></p>
<h3 id="Temporal-Action-Proposal-Models"><a href="#Temporal-Action-Proposal-Models" class="headerlink" title="Temporal Action Proposal Models"></a>Temporal Action Proposal Models</h3><p>第一种是 Top-Down, 第二种是 Bottom-Up, 第三种是前 2 种结合.</p>
<p>top-down 是指使用 sliding windows 来得到 segment-level 的 proposals, 例如 <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-319-46487-9_47">DAP</a> 和 <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Buch_SST_Single-Stream_Temporal_CVPR_2017_paper.html">SST</a> 使用 CNN 和 RNN 来做, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2016/html/Shou_Temporal_Action_Localization_CVPR_2016_paper.html">S-CNN</a> 使用多尺度的 sliding windows, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_iccv_2017/html/Gao_TURN_TAP_Temporal_ICCV_2017_paper.html">TURN TAP</a> 使用了多尺度的池化策略</p>
<p>bottom-up 使用 two-stream 的 frame-level 或者 short-segment-level 的提取特征来预测”动作”的置信度, 然后用不同的分组策略来将这些密集的预测转化为 proposals. 例如 <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_iccv_2017/html/Zhao_Temporal_Action_Detection_ICCV_2017_paper.html">TAG</a> 使用洪泛算法把这些 dense predictions 转化为多尺度的分组, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ECCV_2018/html/Tianwei_Lin_BSN_Boundary_Sensitive_ECCV_2018_paper.html">BSN</a> 和 <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.html">BMM</a> 使用额外的开始特征和结束特征来得到不同的 proposal generation 和 proposal evaluation techniques. <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8972408/">RecapNet</a> 使用一个 residual causal network 而不是通常的 1D CNN 来计算置信度, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_iccv_2017/html/Xu_R-C3D_Region_Convolutional_ICCV_2017_paper.html">R-C3D</a> 和 <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Chao_Rethinking_the_Faster_CVPR_2018_paper.html">TAL-Net</a> 使用 region-based 的方法来将图像中 2D 的 object proposals 转为视频中 1D 的 action proposals. 许多 bottom-up 结构都需要 NMS 来减少冗余 proposals 的权重.</p>
<p>最后一种就是 top-down 和 bottom-up 的结合了, 通常并行地先创造 segment proposals 和 actionness scores 然后利用这些 actionness 来 refine 这些 proposals. 例如 <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Shou_CDC_Convolutional-De-Convolutional_Networks_CVPR_2017_paper.html">CDC</a>, <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/html/Jiyang_Gao_CTAP_Complementary_Temporal_ECCV_2018_paper.html">CTAP</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Multi-Granularity_Generator_for_Temporal_Action_Proposal_CVPR_2019_paper.html">MGG</a>, <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-030-36718-3_40">DPP</a></p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/temporal_action_proposal_models.png"></p>
<h3 id="Temporal-Action-Detection-Models"><a href="#Temporal-Action-Detection-Models" class="headerlink" title="Temporal Action Detection Models"></a>Temporal Action Detection Models</h3><p>时序动作检测模型有 2 种范式, 第一种是 two-stage, 先进行 temporal action proposal, 再进行分类, 这种方法可以由任意的一种 Temporal Action Proposal 模型和任意一种 Action Recognition 模型组合而成. 第二种范式是 one-stage 范式, 同时生成 proposals 和分类.</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3123266.3123343">SSAD</a> 创建一个 snippet-level 的 action score sequence, 然后用一个 1D CNN 来提取多尺度的检测. <a target="_blank" rel="noopener" href="https://repository.kaust.edu.sa/handle/10754/663479">SS-TAD</a> 使用 parallel recurrent memory cells 来创建 proposals 并分类. <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8784822/">Decouple-SSAD</a> 在 SSAD 的基础上添加了 three-stream 的 decoupled-anchor 网络, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Long_Gaussian_Temporal_Awareness_Networks_for_Action_Localization_CVPR_2019_paper.html">GTAN</a> 使用多尺度的高斯核, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9108686">Two-stream SSD</a> 把 RGB 检测结果和光流检测结果进行融合, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9053319/">RBC</a> 在分类之间完成边界的 refinement.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/temporal_detection_models.png"></p>
<h3 id="Spatiotemporal-Action-Detection-Models"><a href="#Spatiotemporal-Action-Detection-Models" class="headerlink" title="Spatiotemporal Action Detection Models"></a>Spatiotemporal Action Detection Models</h3><p>目前有 2 种主流方法, 一种是 frame-level(region) proposal architectures, 使用不同的 region proposal 算法(如 R-CNN 系列)来从帧中得到 bounding box, 然后对每一帧都使用一种连接策略. 这些方法有 <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-319-46493-0_45">MR-TS</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.00042">CPLA</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_iccv_2017/html/Singh_Online_Real-Time_Multiple_ICCV_2017_paper.html">ROAD</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Gu_AVA_A_Video_CVPR_2018_paper.html">AVA I3D</a>, <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/html/Yin_Li_In_the_Eye_ECCV_2018_paper.html">RTPR</a>, and <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1047320318303468">PntMatch</a>.</p>
<p>第二种是 segment-level(tube) proposal architectures, 先得到 segment-level 的 temporally-small 的 tube 或者 tubelets, 然后使用一种 tube linking algorithm 来连接. 这类方法有 <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_iccv_2017/html/Hou_Tube_Convolutional_Neural_ICCV_2017_paper.html">include T-CNN</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_iccv_2017/html/Kalogeiton_Action_Tubelet_Detector_ICCV_2017_paper.html">ACT-detector</a> 和 <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_STEP_Spatio-Temporal_Progressive_Learning_for_Video_Action_Detection_CVPR_2019_paper.html">STEP</a></p>
<p>也有一些方法是不属于这 2 种的, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_A_Structured_Model_for_Action_Detection_CVPR_2019_paper.html">这篇文章</a> 使用一种跟踪网络和图卷积来得到人的 bounding box, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Girdhar_Video_Action_Transformer_Network_CVPR_2019_paper.html">VATX</a> 使用 multi-head, multi-layer 的 transformer 来增强 I3D 模型, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.04316">STAGE</a> 使用了一种时序图注意力机制.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/spatiotemporal_detection_models.png"></p>
<h1 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h1><p>首先对符号进行定义:</p>
<ul>
<li>$X=\left{x^{(1)}, \ldots, x^{(n)}\right}$: 有 $n$ 个视频的集合</li>
<li>$Y=\left{y^{(1)}, \ldots, y^{(n)}\right}$: $n$ 个视频的 ground truth 集合</li>
<li>$M: X \rightarrow \widehat{Y}$: 输入视频到预测结果的映射(函数/模型)</li>
<li>$\widehat{Y}=\left{\hat{y}^{(1)}, \ldots, \hat{y}^{(n)}\right}$: $n$ 个模型的输出(预测的结果)</li>
<li>$C={1, \ldots, m}$: $m$ 个动作类别的集合</li>
<li>$T P_{j}: \mathbb{N} \rightarrow{0,1}$: 一个映射函数, 对于列表 $L_{j}$ 中的每一项, 如果其排名是 true positive, 那么它的值被映射为 1, 否则为 0.</li>
</ul>
<p>也有一些评价指标用到了 IoU 的概念, 如下图所示.</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/iou.png"></p>
<h2 id="Multi-class-Classification-Metrics"><a href="#Multi-class-Classification-Metrics" class="headerlink" title="Multi-class Classification Metrics"></a>Multi-class Classification Metrics</h2><h3 id="Top-k-Category-Accuracy-a-k"><a href="#Top-k-Category-Accuracy-a-k" class="headerlink" title="Top-k Category Accuracy($a_{k}$)"></a>Top-k Category Accuracy($a_{k}$)</h3><p>这个指标评价了当 ground truth 能在 top k 个预测结果中被找到时的时间比率, top-1 accuracy 是最常用的, 偶尔也有 top-3 和 top-5, 在某些情况下, 会对这几种 top-k 的准确度或者错误率进行平均.</p>
<p>为了计算 top-k accuracy, 对于每个视频 $x^{(i)}$, $\hat{y}<em>{k}^{(i)} \subseteq \hat{y}^{(i)}$ 为包含 $k$ 个最高置信度分数的子集, 整个输入集合的 top-k 精度可以表示为下式, 其中 $\mathbb{1}$ 是一个 0-1 indicator function(如果 $\hat{y}</em>{k}^{(i)} = y^{(i)}$, 那么值为 1, 否则为 0, 这里的 $\hat{y}<em>{k}^{(i)}$ 是指输入视频 $x^{(i)}$ 的 top-k 个预测结果的集合 ${ {y}</em>{1}^{(i)}, …, {y}<em>{k}^{(i)} }$)<br>$$<br>a</em>{k}=\frac{1}{n} \sum_{i=1}^{n} \mathbb{1}<em>{\hat{y}</em>{k}^{(i)}}\left(y^{(i)}\right)<br>$$</p>
<h3 id="Mean-Average-Precision-mAP"><a href="#Mean-Average-Precision-mAP" class="headerlink" title="Mean Average Precision (mAP)"></a>Mean Average Precision (mAP)</h3><p>这个评价指标是每个类平均精度的算术平均值, 被用于 THUMOS 和 ActivityNet 竞赛.</p>
<p>为了计算每一类的 AP, 模型的输出需要按照置信度倒序排列, 用公式来表示就是, $L_{j}$ 是预测第 $j$ 类的结果的列表, 对于列表中任意的 2 个预测结果 $p_{j}^{(a)}, p_{j}^{(b)}$, 其中 $a &lt; b \space \space \space \forall a, b \in{1, \ldots, n}$, 都存在 $p_{j}^{(a)} \geq p_{j}^{(b)}$</p>
<p>对于结果列表 $L_{j}$ 中的每个排名 $r$, 如果其 ground truth label $y^{(i)}$ 是类别 $j$ 的话, 那么这个排名就是一个 True Positive, 即 $T P_{j}(r)=1$ if $y^{(i)}=j$, 使用这些列表 $L_{1}, …, L_{m}$ 中每个列表的 top-k 精度可以计算每个类别的 AP, mAP 是所有类 AP 的算术平均:<br>$$<br>\begin{array}{c}<br>P_{j}(k)=\frac{1}{k} \sum_{r=1}^{k} T P_{j}(r) \<br>A P(j)=\frac{\sum_{k=1}^{n} P_{j}(k) * T P_{j}(k)}{\sum_{k=1}^{n} T P_{j}(k)} \<br>m A P=\frac{1}{m} \sum_{j=1}^{m} A P(j)<br>\end{array}<br>$$</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ming Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://coderming.cn/2020/10/19/Introduction-to-Video-Action-Understanding/">https://coderming.cn/2020/10/19/Introduction-to-Video-Action-Understanding/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Video/">Video</a><a class="post-meta__tags" href="/tags/Action/">Action</a></div><div class="post_share"><div class="social-share" data-image="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/wechat.png" target="_blank"><img class="post-qr-code-img" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/wechat.png" alt="WeChat"/></a><div class="post-qr-code-desc">WeChat</div></li><li class="reward-item"><a href="https://qr.alipay.com/fkx12431y6k0soy7vokzi19" target="_blank"><img class="post-qr-code-img" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/alipay.jpg" alt="AliPay"/></a><div class="post-qr-code-desc">AliPay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2020/10/12/Deformable-CNNs/"><img class="next-cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Deformable-CNNs/avatar.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Deformable Convolutional Networks</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2020/10/01/Action-Localization/" title="Action Localization"><img class="cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Localization/temporal_action_localization.png"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-10-01</div><div class="title">Action Localization</div></div></a></div><div><a href="/2020/10/02/Action-Recognition/" title="Action Recognition"><img class="cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Action-Recognition/avatar.png"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-10-02</div><div class="title">Action Recognition</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/10/Introduction-to-Video-Action-Understanding/avatar.png)"><div id="footer-wrap"><div class="copyright">&copy;2020 By Ming Li</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div></div></body></html>