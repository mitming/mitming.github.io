<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Anchor Free Object Detection | Ming</title><meta name="keywords" content="Anchor Free"><meta name="author" content="Ming Li"><meta name="copyright" content="Ming Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="CornerNetPipeline 如下： 贡献 通过检测目标的左上角点和右下角点来检测出目标, 做到了 anchor free 提出了 corner pooling 来更好地定位角点  Anchor Based Methods 存在的问题 Anchor-Based 方法都会产生大量的 anchors&#x2F;proposals, 例如 DSSD 会产生超过 40k 个 anchors, RetinaNe">
<meta property="og:type" content="article">
<meta property="og:title" content="Anchor Free Object Detection">
<meta property="og:url" content="https://coderming.cn/2020/06/20/Anchor-Free-Object-Detection/index.html">
<meta property="og:site_name" content="Ming">
<meta property="og:description" content="CornerNetPipeline 如下： 贡献 通过检测目标的左上角点和右下角点来检测出目标, 做到了 anchor free 提出了 corner pooling 来更好地定位角点  Anchor Based Methods 存在的问题 Anchor-Based 方法都会产生大量的 anchors&#x2F;proposals, 例如 DSSD 会产生超过 40k 个 anchors, RetinaNe">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cover.png">
<meta property="article:published_time" content="2020-06-20T07:48:03.000Z">
<meta property="article:modified_time" content="2020-09-04T12:58:00.659Z">
<meta property="article:author" content="Ming Li">
<meta property="article:tag" content="Anchor Free">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cover.png"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://coderming.cn/2020/06/20/Anchor-Free-Object-Detection/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-09-04 20:58:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">18</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/images/"><i class="fa-fw fas fa-images"></i><span> Images</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CornerNet"><span class="toc-number">1.</span> <span class="toc-text">CornerNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE"><span class="toc-number">1.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Anchor-Based-Methods-%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.2.</span> <span class="toc-text">Anchor Based Methods 存在的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%B7%AF"><span class="toc-number">1.3.</span> <span class="toc-text">思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B"><span class="toc-number">1.4.</span> <span class="toc-text">角点检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%92%E7%82%B9%E5%88%86%E7%BB%84"><span class="toc-number">1.5.</span> <span class="toc-text">角点分组</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Corner-Pooling"><span class="toc-number">1.6.</span> <span class="toc-text">Corner Pooling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Prediction-Module"><span class="toc-number">1.7.</span> <span class="toc-text">Prediction Module</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hourglass-Network"><span class="toc-number">1.8.</span> <span class="toc-text">Hourglass Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-Details"><span class="toc-number">1.9.</span> <span class="toc-text">Training Details</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Test-Details"><span class="toc-number">1.10.</span> <span class="toc-text">Test Details</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ExtremeNet"><span class="toc-number">2.</span> <span class="toc-text">ExtremeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-1"><span class="toc-number">2.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%92%8C-CornerNet-%E7%9A%84%E4%B8%8D%E5%90%8C"><span class="toc-number">2.2.</span> <span class="toc-text">和 CornerNet 的不同</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%81%E5%80%BC%E7%82%B9%E5%92%8C%E4%B8%AD%E5%BF%83%E7%82%B9"><span class="toc-number">2.3.</span> <span class="toc-text">极值点和中心点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%AD%E5%BF%83%E7%82%B9%E5%88%86%E7%BB%84"><span class="toc-number">2.4.</span> <span class="toc-text">中心点分组</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ghost-Box-Suppression"><span class="toc-number">2.5.</span> <span class="toc-text">Ghost Box Suppression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Edge-aggregation"><span class="toc-number">2.6.</span> <span class="toc-text">Edge aggregation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Extreme-Instance-Segmentation"><span class="toc-number">2.7.</span> <span class="toc-text">Extreme Instance Segmentation</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CenterNet-Triplets"><span class="toc-number">3.</span> <span class="toc-text">CenterNet: Triplets</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-2"><span class="toc-number">3.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CornerNet-%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="toc-number">3.2.</span> <span class="toc-text">CornerNet 的不足</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Object-Detection-as-keypoint-triplets"><span class="toc-number">3.3.</span> <span class="toc-text">Object Detection as keypoint triplets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Center-Pooling"><span class="toc-number">3.4.</span> <span class="toc-text">Center Pooling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cascade-Corner-Pooling"><span class="toc-number">3.5.</span> <span class="toc-text">Cascade Corner Pooling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-and-Inference"><span class="toc-number">3.6.</span> <span class="toc-text">Training and Inference</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results"><span class="toc-number">3.7.</span> <span class="toc-text">Results</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CenterNet-Points"><span class="toc-number">4.</span> <span class="toc-text">CenterNet: Points</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-3"><span class="toc-number">4.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%B7%AF-1"><span class="toc-number">4.2.</span> <span class="toc-text">思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E5%86%85%E5%AE%B9"><span class="toc-number">4.3.</span> <span class="toc-text">前置内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E6%B5%8B%E5%8E%9F%E7%90%86"><span class="toc-number">4.4.</span> <span class="toc-text">检测原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Inference"><span class="toc-number">4.5.</span> <span class="toc-text">Inference</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results-1"><span class="toc-number">4.6.</span> <span class="toc-text">Results</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#FCOS"><span class="toc-number">5.</span> <span class="toc-text">FCOS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-4"><span class="toc-number">5.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Anchor-Based-%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="toc-number">5.2.</span> <span class="toc-text">Anchor-Based 的不足</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%B7%AF-2"><span class="toc-number">5.3.</span> <span class="toc-text">思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E5%8D%B7%E7%A7%AF%E6%A3%80%E6%B5%8B%E5%99%A8"><span class="toc-number">5.4.</span> <span class="toc-text">全卷积检测器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FCOS-%E4%B8%AD%E7%9A%84%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B"><span class="toc-number">5.5.</span> <span class="toc-text">FCOS 中的多尺度预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Center-ness-for-FCOS"><span class="toc-number">5.6.</span> <span class="toc-text">Center-ness for FCOS</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results-2"><span class="toc-number">5.7.</span> <span class="toc-text">Results</span></a></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cover.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ming</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/images/"><i class="fa-fw fas fa-images"></i><span> Images</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Anchor Free Object Detection</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-06-20T07:48:03.000Z" title="Created 2020-06-20 15:48:03">2020-06-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-09-04T12:58:00.659Z" title="Updated 2020-09-04 20:58:00">2020-09-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Object-Detection/">Object Detection</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">9.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>35min</span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h1><p>Pipeline 如下：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cornernet_pipeline.png"></p>
<h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ol>
<li>通过检测目标的左上角点和右下角点来检测出目标, 做到了 anchor free</li>
<li>提出了 corner pooling 来更好地定位角点</li>
</ol>
<h2 id="Anchor-Based-Methods-存在的问题"><a href="#Anchor-Based-Methods-存在的问题" class="headerlink" title="Anchor Based Methods 存在的问题"></a>Anchor Based Methods 存在的问题</h2><ol>
<li>Anchor-Based 方法都会产生大量的 anchors/proposals, 例如 DSSD 会产生超过 40k 个 anchors, RetinaNet 会产生 100k 个 anchors, 但是只有很少一部分的 anchors 是和 Ground Truth 有比较大的 overlap 的, 这就导致了 positive anchors 和 negative anchors 之间的数据不平衡从而影响训练速度和精度。</li>
<li>Anchor 机制会使得模型有很多的超参数和设计选择, 例如每个 grid 生成多少个 boxes, 每个 box 的 size 以及 aspect ratio 等等, 这种选择很大程度上是通过临时启发式方法做出的, 当与多尺度体系结构结合使用时, 可能会变得更加复杂。</li>
</ol>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>如下图所示, 在经过 backbone 后接 2 个预测模块, 每个模块都有自己的 corner pooling 层, 经过 corner pooling 后分别得到 heatmap, embedding vector 和 offsets, 如图所示：</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cornernet.png"></p>
<p>其中 Heatmap 的 size 为 $H \times W \times C$, $C$ 为类别数目, Embedding Vector 的 size 为 $H \times W \times l$, 这里的 $l$ 代表 Embedding Vector 的长度, 原文中 $l=1$, Offsets 的 size 为 $H \times W \times 2$, 预测的是 x 方向和 y 方向上的偏移量。Heatmap 用于检测角点, Embedding Vector 用于对角点进行配对分组, offsets 用于调整 2 个角点的坐标。所以可以看出这里有 3 个 Loss 值, 分别是 $L_{detection}, L_{group}, L_{offset}$</p>
<h2 id="角点检测"><a href="#角点检测" class="headerlink" title="角点检测"></a>角点检测</h2><p>这里预测 2 个热力图, 分别用于左上角点和右下角点, 每个热力图的通道数都是 $C$, 也就是类别数, 每个 channel 都是一个二值的 mask, 用于表示该类的角点位置。对每个角点而言都只有一个 Ground Truth, 其他的点都是 negative, 训练时并非简单地对其他 negative corners 都做一样的惩罚, 而是对离 Ground Truth 更近的点施以更小的惩罚, 在半径为 $r$ 的圆内, 惩罚的减少量满足非标准的 2-D 的高斯分布: $e^{-\frac{x^2 + y^2}{2\sigma^2}}$</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/2d_gaussian_distribution.jpg"></p>
<p>即 $y_{cij}$ 满足下式：<br>$$<br>y_{c i j}=\left{\begin{array}{ll}<br>e^{-\frac{x^{2}+y^{2}}{2 \sigma^{2}}}, &amp; x^{2}+y^{2} \leq r^{2} \<br>0, &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p>
<p>利用 2 个角点生成的 bounding box 必须和 Ground Truth 的 IoU 大于某个值来确定半径 $r$ 的值。2D高斯分布的中心就是真正的角点坐标, $\sigma = \frac{r}{3}$。</p>
<p>设 $p_{cij}$ 是在热力图的坐标 $(i, j)$ 位置上为类别 $c$ 预测的值, $y_{cij}$ 是非标准的二维高斯分布的值(上图所示), 也就是热力图上的 Ground Truth, 那么就能得到一个 Focal Loss 的变体, 为检测损失:</p>
<p>$$<br>L_{d e t}=-\frac{1}{N} \sum_{c=1}^{C} \sum_{i=1}^{H} \sum_{j=1}^{W}\left{\begin{array}{cc}<br>\left(1-p_{c i j}\right)^{\alpha} \log \left(p_{c i j}\right) &amp; \text { if } y_{c i j}=1 \<br>\left(1-y_{c i j}\right)^{\beta}\left(p_{c i j}\right)^{\alpha} \log \left(1-p_{c i j}\right) &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p>
<p>这里的 otherwise 并非 $y_{cij}=0$, 因为在半径内, $y_{cij}$ 的值满足二维的高斯分布。$\alpha$ 和 $\beta$ 是用于控制每个点权重的超参数(文中$\alpha=2, \beta=4$), 根据上面的 Focal Loss 变体可以看出：</p>
<ol>
<li>在角点位置 $(i, j)$ 时, 预测这个点为角点的概率 $p_{cij}$ 的值越大, 那么这里的 Detection Loss 就会越小。</li>
<li>不在角点位置时, 固定 $p_{cij}$, 离角点越近, $y_{cij}$ 越大, $1-y_{cij}$ 就越小, Detection Loss 越小。</li>
<li>不在角点位置时, 固定 $y_{cij}$, 如果预测这个点为角点的概率 $p_{cij}$ 越大, 那么Detection Loss 越大。</li>
</ol>
<p>所以这个公式是希望越接近角点的那些点被预测为角点的概率 $p_{cij}$ 越大, 越远离角点的那些点被预测为角点的概率越小。</p>
<p>把位置从热力图上重新映射到原图上时, 由于下采样因子的存在会存在像素偏移, 图像中的位置 $(x, y)$ 映射到热力图上是 $(\lfloor \frac{x}{n} \rfloor, \lfloor \frac{y}{n} \rfloor)$, $n$ 是下采样因子, 为了解决像素偏移的问题这里还预测了一个 offset：</p>
<p>$$<br>\boldsymbol{o}<em>{k}=\left(\frac{x</em>{k}}{n}-\left\lfloor\frac{x_{k}}{n}\right\rfloor, \frac{y_{k}}{n}-\left\lfloor\frac{y_{k}}{n}\right\rfloor\right)<br>$$</p>
<p>在测试阶段, 则根据预测的坐标偏差来修改坐标, 设 heatmap 上预测为角点的位置为 $(x,y)$, offset 为 $\hat{\sigma}=(\Delta x, \Delta y)$ 那么在原图上角点的坐标为 $x_0 = \lfloor (x+\Delta x)n \rfloor, y_0 = \lfloor (y+\Delta y)n \rfloor$, 对应得到一个 Smooth L1 的偏移损失：</p>
<p>$$<br>L_{o f f}=\frac{1}{N} \sum_{k=1}^{N} \text { SmoothL1Loss }\left(\boldsymbol{o}<em>{k}, \hat{\boldsymbol{o}}</em>{k}\right) \<br>\text { SmoothL1Loss }(x)=\left{\begin{array}{ll}<br>0.5 x^{2} &amp; \text { if }|x|&lt;1 \<br>|x|-0.5 &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p>
<h2 id="角点分组"><a href="#角点分组" class="headerlink" title="角点分组"></a>角点分组</h2><p>如果一张图像中有多个 objects, 那么肯定有多个左上角点和右下角点, 需要将左上角点和右下角点进行配对来组成一个 bounding box, 作者这里使用 embedding 的方法, 为每个 corner 生成 embedding vector, 如果 2 个 corners 是来自同一个目标, 那么对应的 2 个 embedding vector 应该很小, 在训练的时候, 会分别生成 2 个 embedding vector, 其输出 size 为 $h \times w$, 和 heatmap 的 size 相同, 这里作者简单的把通道数设置为 1, 即 embedding vector 的 size 为 $H \times W \times 1$, 这个 embedding 没有 target value, 引入 pull loss 和 push loss 来学习 embedding 的预测, 对于第 k 个目标的左上角点的 embedding $e_{tk}$, 和第 k 个目标的右下角点的 embedding $e_{bk}$, $L_{pull}$ 和 $L_{push}$ 分别为</p>
<p>$$<br>\begin{array}{c}<br>L_{p u l l}=\frac{1}{N} \sum_{k=1}^{N}\left[\left(e_{t k}-e_{k}\right)^{2}+\left(e_{b k}-e_{k}\right)^{2}\right] \<br>L_{p u s h}=\frac{1}{N(N-1)} \sum_{k=1}^{N} \sum_{j=1}^{N} \max \left(0, \Delta-\left|e_{k}-e_{j}\right|\right) \<br>e_{k}=\frac{e_{t k}+e_{b k}}{2}<br>\end{array}<br>$$</p>
<p>其中 $N$ 是一张图像中的 object 数目, $\Delta=1$, $L_{pull}$ 是为了将同一目标的 2 个 embedding 拉的更近, 同一目标的 2 个 embedding $e_{tk}$ 和 $e_{bk}$ 越接近那么 $L_{pull}$ 就越小。</p>
<p>$L_{push}$ 的目的是让不同目标的 average embedding 尽可能的远, 远离的程度由 $\Delta$ 控制, 当 2 个不同目标的 average embedding 的 L1 范数超过 $\Delta$ 时就可以认为此时 corner 是来自不同的目标, 这个时候 $L_{push} = 0$, 当 2 个不同目标的 average embedding 的 L1 范数没有超过 $\Delta$ 时认为没有真正区分开这 2 个目标的 embedding, 所以要添加惩罚。</p>
<p>embedding 本身没有 target value（事实上也很难去给出一个绝对的值）, embedding 是人为设计出来的一套自洽系统或者说一个空间, 只要 corner 的 embedding 遵循一定的规则, 就可以, 所以使用 pull 和 push 损失学习, 可以看成是为了让生成的 embedding 遵守规则, 这样在 Test 阶段, 就可以基于这些规则然后根据 embedding 值对 corner 进行同目标归并。</p>
<p>测试阶段别从两组 heatmap 上得到 K 个 top-left corners 和 K 个 bottom-right corners, 那么共有 $k^2$ pairs（一个 top-left 与一个 bottom-right corner 组成一个 pair）, 对每个 pair 计算 $e_{tk} - e_{bj}$, 如果超过一个阈值(0.5)那么这个 pair 就会被忽略, 实际上在源码中还有其他一些筛选条件, 比如得分（来自于 heatmap）, 坐标值范围检测, pair 中 top-left 与 bottom-right 两个 corner 的分类是否一致等。筛选之后还要（对每个分类分别）使用 soft nms 等方法, 才得到最终的检测结果。具体请阅读<a target="_blank" rel="noopener" href="https://github.com/princeton-vl/CornerNet">源码</a>。</p>
<h2 id="Corner-Pooling"><a href="#Corner-Pooling" class="headerlink" title="Corner Pooling"></a>Corner Pooling</h2><p>以 Top-Left 为例给出 Corner Pooling 的操作过程：</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/corner_pooling_1.png"></p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/corner_pooling_2.png"></p>
<p>如果想判断一个在 $(x, y)$ 的像素点是否为左上角点, $f_t$ 和 $f_l$ 是 top-left corner pooling layer 的输入, $f_{t_{ij}}, f_{l_{ij}}$ 代表在 $f_t$ 和 $f_l$ 上 $(x, y)$ 位置的向量, $f_t$ 和 $f_l$ 的 size 都是 $H \times W$, 首先最大池化 $f_t$ 中在 $(i, j)$ 和 $(i, H)$ 中的所有特征值, 使之成为特征向量 $t_{ij}$, 然后最大池化 $f_l$ 中在 $(i, j)$ 和 $(W, j)$ 中的所有特征值, 使之成为特征向量 $l_{ij}$, 最后把 $t_{ij}$ 和  $l_{ij}$ 进行 element-wise add 操作。公式如下：</p>
<p>$$<br>\begin{array}{l}<br>t_{i j}=\left{\begin{array}{cl}<br>\max \left(f_{t_{i j}}, t_{(i+1) j}\right) &amp; \text { if } i&lt;H \<br>f_{t_{H j}} &amp; \text { otherwise }<br>\end{array}\right. \<br>l_{i j}=\left{\begin{array}{cl}<br>\max \left(f_{l_{i j}}, l_{i(j+1)}\right) &amp; \text { if } j&lt;W \<br>f_{l_{i W}} &amp; \text { otherwise }<br>\end{array}\right.<br>\end{array}<br>$$</p>
<p>右下角点的 corner pooling 也是相同的思路。</p>
<h2 id="Prediction-Module"><a href="#Prediction-Module" class="headerlink" title="Prediction Module"></a>Prediction Module</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/prediction_module.png"></p>
<p>预测模块的第一部分是残差模块的修改版, 修改后的残差模块中, 将第一个 $3 \times 3$ 卷积替换为一个 Corner Pooling 模块。这个残差模块首先通过具有个 128 通道的 2 个 $3 \times 3$ 卷积模块的主干网络进行特征提取, 然后应用一个 Cornet Pooling 层。残差模块之后, 我们将池化特征输入到具有个 256 通道的 $3 \times 3$ 的 Conv+BN 层, 同时为这一层加上瓶颈结构。修改后的残差模块后面接一个具有个 256 通道的 $3 \times 3$ 卷积模块和 256个通道的 3 个 Conv-ReLU-Conv 来产生热力图, 嵌入和偏移量。</p>
<h2 id="Hourglass-Network"><a href="#Hourglass-Network" class="headerlink" title="Hourglass Network"></a>Hourglass Network</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/hourglass_network.jpg"></p>
<p>这个网络最早在在人体姿态估计任务中被引入, Hourglass 网络是全卷积网络, 包含一个或者多个 Hourglass 模块, 在 Hourglass 网络中首先使用一些卷积核池化层对输入特征进行下采样（注意论文指出这里没有使用池化层来做下采样, 而是只使用步长为卷积层）, 然后再上采样使得特征图的分辨率回到原始大小, 由于 Max-Pooling 层会丢失细节信息, 所以增加跳跃连接层将低级特征信息带到上采样特征图中, 因此 hourglass 不仅仅结合了局部特征还结合了全局特征, 当堆叠多个 hourglass 模块时就可以重复这个过程, 从而得到更加高级的特征。</p>
<h2 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h2><p>设置了网络的输入分辨率 511 × 511, 使用数据增强, 包括随机水平翻转、随机缩放、随机裁剪和随机色彩抖动, 其中包括调整图像的亮度, 饱和度和对比度使用 PyTorch 默认的初始化, 由于使用了 Focal Loss, 所以按照 RetinaNet 论文中指出的方式来对某些卷积层的偏置进行初始化. 总损失就是检测损失, 分组损失和偏移损失的加权和:<br>$$<br>L = L_{det} + \alpha L_{pull} + \beta L_{push} + \gamma L_{off}<br>$$</p>
<h2 id="Test-Details"><a href="#Test-Details" class="headerlink" title="Test Details"></a>Test Details</h2><ol>
<li>如何判断某个位置是角点？先在得到的 corner heatmaps 上利用 $3 \times 3$ 的最大池化层做 NMS, 不改变特征图大小(stride=1, padding=1), 在最大池化时值改变了的全部置 0, 值未变的保留原值。</li>
<li>经过 NMS 后最多从 heatmap 选择 Top 100 个左上角点和 Top 100 个右下角点。</li>
<li>对每个角点的坐标利用预测的 offsets 进行调整, 计算左上角点和右下角点 embedding 之间的 L1 距离, 对于这个距离大于某个阈值(0.5)或者不是同一类的角点对, 取消配对。</li>
<li>左上角点和右下角点的平均 score 作为最终检测的 score.</li>
<li>测试时保持原图像的 size 不变, 如果原来的图像长和宽不相等, 使用 zero-padding 的方式来填充.</li>
</ol>
<h1 id="ExtremeNet"><a href="#ExtremeNet" class="headerlink" title="ExtremeNet"></a>ExtremeNet</h1><h2 id="贡献-1"><a href="#贡献-1" class="headerlink" title="贡献"></a>贡献</h2><p>CornerNet 的问题是角点检测不准确，ExtremeNet 通过预测上下左右 4 个极值点以及一个中心点来检测目标。</p>
<h2 id="和-CornerNet-的不同"><a href="#和-CornerNet-的不同" class="headerlink" title="和 CornerNet 的不同"></a>和 CornerNet 的不同</h2><p>ExtremeNet 和 CornerNet 主要有 2 处不同，第一点是 CornerNet 的角点通常是在目标的外部，往往没有强烈的局部特征。而 ExtremeNet 的极值点就在物体的边缘上，视觉上很好辨认。第二点是 ExtremeNet 纯粹依赖于几何关系进行极值点分组，没有隐含的特征学习，效果更好。</p>
<h2 id="极值点和中心点"><a href="#极值点和中心点" class="headerlink" title="极值点和中心点"></a>极值点和中心点</h2><p>网络的 pipeline 如下图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/extremenet.png"></p>
<p>输出特征图的通道数为 $5 \times C + 4 \times 2$，$C$ 代表类别数，对于每个类别预测一张 heatmap 和一张 center map，所以是 $5 \times C$，对 4 个极值点预测 $x, y$ 方向上的 offset，不对中心点预测 offset，极值点的定义为一个 $3 \times 3$ 滑动窗口中的最大值且大于设定的阈值 $t_p$，论文中 $t_p = 0.1$</p>
<h2 id="中心点分组"><a href="#中心点分组" class="headerlink" title="中心点分组"></a>中心点分组</h2><p>对于给定的 4 个极值点 $t, b, r, l$，计算其几何中心 $c = (\frac{l_x + r_x}{2}, \frac{t_y + b_y}{2})$，如果这个几何中心的值在 center map 中是高响应的(大于阈值$t_c$)，那么就把这 5 个点看做是一个有效的检测，论文中 $t_c = 0.1$，该算法的流程图和伪代码如下所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/extremenet_points_match.png"><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/extremenet_center_grouping.png"></p>
<h2 id="Ghost-Box-Suppression"><a href="#Ghost-Box-Suppression" class="headerlink" title="Ghost Box Suppression"></a>Ghost Box Suppression</h2><p>中心点分组可能会得到一些高置信度但是是假阳性的检测结果，这种情况往往在 3 个及以上相同尺寸的物体共线性的时候出现，这就会导致最终预测一个超级大的 box，这个 bbox 会把 3 个 ground truth box 都包含进去，如下图所示，3 个实线框是3 个 ground truth box，虚线框是 ghost box。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/extremenet_bbox_suppression.png"></p>
<p>作者使用 soft non-maxima suppression 方法，根据 Algorithm 1，得到所有的 bbox 以及对应的得分。假设某个 bbox，其 score 为 $s_0$ ，所有被这个 bbox 包含的其他 bbox 的得分之和超出 $s_0$ 的 3 倍，那么这个 bbox 的 score 降为一半为 $\frac{s_0}{2}$ （视作对这样的 bbox 的 score 的惩罚，后期再对 bbox 得分低于 0.5 的进行过滤，就能把 ghost box 过滤掉了）。为什么是 3 倍？因为如上图所示，至少有 3 个  bbox，才会有选择中间 bbox 还是选择紧紧包围三个 bbox 的大 bbox 这样两难的问题。实际上，必须有奇数个 bbox，才会有可能出现 ghost box 的可能，而一个 image 中有大于 3 的相等大小的 bbox 水平或垂直排列，这种可能性极低，所以只要考虑 3 倍足以（纯粹个人理解）。</p>
<h2 id="Edge-aggregation"><a href="#Edge-aggregation" class="headerlink" title="Edge aggregation"></a>Edge aggregation</h2><p>有时候，极值点并不是唯一的，举例来说，一辆汽车，可能顶部这一条类似于水平的线上，每一个位置都有可能成为最顶部的极值点。也就是说，若极点来自物体的垂直或水平边，则沿边缘任何一点都可被认为是极点。因此，我们的网络沿着对象的任何对齐边缘产生弱响应，而不是单一的强峰值响应。这种弱响应有两个不足：</p>
<ol>
<li>弱响应点的得分可能会小于阈值，那么极值点就会被错过</li>
<li>即使检测到了这些极值点，它的得分将低于具有强峰值响应的稍微旋转的对象。<br>为了解决这个问题，提出了 edge aggregation。对每一个取局部最大值得到的极值点，若是左边和右边的极值点，那么选择竖直方向进行聚合；同理，若是顶部，底部的极值点，则选择从水平方向进行分数聚合。聚合的方法为：沿着聚合方向，聚合那些单调递减的 score，并在达到局部最小值的时候停止聚合。假定 m 为极值点，定义一个包含m点的水平线在 heatmaps 上的分数集：<br>$$<br>N_{i}^{m} = \hat{Y}_{m_x + i}<br>$$<br>由于 m 是极值点，极值点的选择机制是局部最大原则，所以需要在极值点m的左边和右边找到两个局部最小值，记为$i_0$ 和 $i_1$，应该满足：</li>
</ol>
<p>$$<br>\begin{array}{l}<br>N_{i_{0}-1}^{(m)}&gt;N_{i_{0}}^{(m)} \<br>N_{i_{1}}^{(m)}&lt;N_{i_{1}+1}^{(m)}<br>\end{array}<br>$$</p>
<p>此时原始极值点 m 的更新为：</p>
<p>$$<br>\tilde{Y}<em>{m}=\hat{Y}</em>{m}+\lambda_{a g g r} \sum_{i=i_{0}}^{i_{1}} N_{i}^{(m)}<br>$$</p>
<p>其中 $\lambda_{aggr}$ 是聚合权重，论文中设置为 0.1，下图为经过 edge aggregation 的效果展示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/edge_aggression.png"></p>
<h2 id="Extreme-Instance-Segmentation"><a href="#Extreme-Instance-Segmentation" class="headerlink" title="Extreme Instance Segmentation"></a>Extreme Instance Segmentation</h2><p>比起 bounding box，极点携带了更多的目标信息，bbox 的标准只要 4 个数据，左上右下两个 corner 的 4 个坐标值，而极点标注则是 4 个极点共 8 个坐标值。基于几点创建一个八边形来近似出一个目标的 mask，这个八边形的边以极点为中心，具体做法是：将四个极点分别沿着 bbox 四个边的方向延伸，得到 4 个线段，长度分别是 bbox 四个边的 1/4，如果延伸时遇到 bbox 的 corner，则截断（不再延伸），将得到的四个线段的端点按顺序连接起来，就得到八边形。同时将 ExtremeNet 获得的极值点传到 Deep Extreme Cut 网络可以获得一个类别未知的分割 Mask，注意类别其实在 ExtremeNet 中已经知道了，这就相当于一个双阶段的实例分割。</p>
<h1 id="CenterNet-Triplets"><a href="#CenterNet-Triplets" class="headerlink" title="CenterNet: Triplets"></a>CenterNet: Triplets</h1><h2 id="贡献-2"><a href="#贡献-2" class="headerlink" title="贡献"></a>贡献</h2><ol>
<li>相比于 CornerNet 只预测 2 个角点，这里还多预测了一个中心点，由于角点大都在物体之外，这使得 Corner Pooling 更加关心目标的边缘信息而对内部的感知不强，添加中心点能够得到更多的视觉信息。</li>
<li>提出 Center Pooling 来更好地定位中心点。</li>
<li>提出 Cascade Corner Pooling 给角点提供更加丰富的物体内部语义信息。</li>
</ol>
<h2 id="CornerNet-的不足"><a href="#CornerNet-的不足" class="headerlink" title="CornerNet 的不足"></a>CornerNet 的不足</h2><p>作者首先分析了 CornerNet 的结果中存在的一个严重问题是错误的 bounding box 数量很多，即使在 IoU 阈值很低的情况下也是这样，如下图所示，其中 $FD$ 代表错误的 bounding box 的百分比<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/false_discovery_in_cornernet.png"></p>
<p>作者认为出现这种情况的<strong>一个可能的原因</strong>是 CornerNet 仅预测角点，无法感知物体内部的信息，虽然可以添加 RoI Pooling 把 CornerNet 变成一个 Two-Stage 的方法来得到 bounding box 内部的信息，但这样计算量太大，所以才多预测了一个 Center Keypoint.</p>
<h2 id="Object-Detection-as-keypoint-triplets"><a href="#Object-Detection-as-keypoint-triplets" class="headerlink" title="Object Detection as keypoint triplets"></a>Object Detection as keypoint triplets</h2><p>在 CornerNet 的基础上多预测了一个中心点的 heatmap 和 offsets，然后和 CornerNet 一样利用角点生成 top-k 个 bounding box，接着按照以下步骤来检测 center keypoint:</p>
<ol>
<li>根据 heatmap 上的 score 选择 top-k 个 center keypoint</li>
<li>使用预测的 offsets 来重新映射 center map 在 input image 的位置</li>
<li>为每个 bounding box 定义一个 central region，并判断这个 central region 内是否含有 center keypoint(这里 center keypoint 的 class label 要和 bounding box 的 class label 一致)</li>
<li>如果一个 center keypoint 在 central region 内被检测到, 那么这个 bounding box 就是有效的, 且其 score 变为 3 个点的 scores 均值, 否则这个 bounding box 无效, 会被移除.</li>
</ol>
<p>可以看到 central region 的 size 会影响最终结果, 如果定义的 central region 太小会导致很多小尺度的错误目标框无法被去除，而 central region 会导致很多大尺度的错误目标框无法去除, 所以需要根据 bounding box 的大小来动态地调整 central region 的 size, 假设左上角点 $i$ 的坐标为 $(tl_x, tl_y)$, 右下角点 $j$ 的坐标为 $(br_x, br_y)$, 那么 central region 的坐标满足以下公式:<br>$$\left{\begin{array}{l}<br>\operatorname{ctl}<em>{\mathrm{x}}=\frac{(n+1) \mathrm{t} \mathrm{l}</em>{\mathrm{x}}+(n-1) \mathrm{br}<em>{\mathrm{x}}}{2 n} \<br>\mathrm{ctl}</em>{\mathrm{y}}=\frac{(n+1) \mathrm{t} \mathrm{l}<em>{\mathrm{y}}+(n-1) \mathrm{br}</em>{\mathrm{y}}}{2 n} \<br>\operatorname{cbr}<em>{\mathrm{x}}=\frac{(n-1) \mathrm{t} 1</em>{\mathrm{x}}+(n+1) \mathrm{br}<em>{\mathrm{x}}}{2 n} \<br>\operatorname{cor}</em>{\mathrm{y}}=\frac{(n-1) \mathrm{t} 1_{\mathrm{y}}+(n+1) \mathrm{br}_{\mathrm{y}}}{2 n}<br>\end{array}\right.$$</p>
<p>$n$ 是个超参数, 论文中将 bounding box size &lt; 150 的设置为 3, bounding box size &gt; 150 的设置为 5.</p>
<h2 id="Center-Pooling"><a href="#Center-Pooling" class="headerlink" title="Center Pooling"></a>Center Pooling</h2><p>下图中的Figure 4(a) 展示了 Center Pooling 的原理, Center Pooling 提取中心点水平方向和垂直方向的最大值并相加，给中心点提供除了所处位置以外的信息，这使得中心点有机会获得更易于区分于其他类别的语义信息。Center Pooling 可通过不同方向上的 Corner Pooling 的组合实现，例如一个水平方向上的取最大值操作可由 Left Pooling 和 Right Pooling 通过串联实现。同理，一个垂直方向上的取最大值操作可由 Top Pooling 和 Bottom Pooling 通过串联实现，具体操作如 Figure5（a）所示，特征图两个分支分别经过一个 $Conv_{3 \times 3}-BN-ReLU$，然后做水平方向和垂直方向的 Corner Pooling，最后再相加得到结果。</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/corner_center_cascade_corner_pooling.png"><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/center_pooling_and_cascade_corner_pooling.png"></p>
<h2 id="Cascade-Corner-Pooling"><a href="#Cascade-Corner-Pooling" class="headerlink" title="Cascade Corner Pooling"></a>Cascade Corner Pooling</h2><p>这里是为了更好地预测目标的左上和右下角点, Figure 4（b）是CornerNet中的做法即提取物体边界最大值进行相加，该方法只能提供关联物体边缘语义信息，对于更加丰富的物体内部语义信息则很难提取到。为了能够提取物体内部的语义信息, 作者提出了 Cascade Corner Pooling. 原理如 Figure 5（b）所示，它首先提取出目标边缘最大值，然后在边缘最大值处继续向物体内部(如Figure 4(c)所示)提取最大值，并和边界最大值相加，以此给角点提供更丰富的关联目标语义信息。Figure 5(b)展示了 Cascade Top Corner Pooling 的原理。这里需要注意一下 Cascade Corner Pooling 只是为了通过内部信息丰富角点特征，也就是级联不同方向的 Corner Pooling 实现内部信息的叠加，最终的目的还是要预测角点，所以最终左上角点通过 Cascade Top Corner Pooling + Cascade Left Corner Pooling 实现，右下角点通过 Cascade Right Corner Pooling + Cascade Bottom Corner Pooling 实现。</p>
<h2 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h2><p>这里的损失函数和 CornerNet 是差不多的, 只是多了个中心点的损失而已, co 代表角点, ce 代表中心点:<br>$$<br>\mathrm{L}=\mathrm{L}<em>{\text {det }}^{\text {co }}+\mathrm{L}</em>{\text {det }}^{\text {ce }}+\alpha \mathrm{L}<em>{\text {pull }}^{\text {co }}+\beta \mathrm{L}</em>{\text {push }}^{\text {co }}+\gamma\left(\mathrm{L}<em>{\text {off }}^{\text {co }}+\mathrm{L}</em>{\text {off }}^{\text {ce }}\right)<br>$$</p>
<p>其定义和 CornerNet 中完全一致.</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/centernet_triplets_result.png"></p>
<p>可以看到 CenterNet 获得了 47% 的 mAP，超过了所有的 One-Stage 算法，领先幅度越 5%，并且精度和 Two-Stage 的目标检测算法的最好结果也是接近的。</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/centernet_triplets_result_compare_to_cornernet.png"></p>
<p>上面的 Table 3 是 CenterNet 与 CornerNet 的单独对比，可以看出在 MS COCO 数据集上 CenterNet 消除大量误检框，尤其是在小物体上。</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/centernet_triplets_ablation_study.png"></p>
<p>Table 4 是消融实验。分别说明了 CRE（中心点加入），CTP（中心点池化），CCP（级联角点池化）的有效性。</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/centernet_triplets_error_analysis.png"><br>作者将检测的中心点用真实的中心点代替，实验结果表明中心点的检测准确度还有很大的提升空间。同时该结果还表明要<strong>想更进一步的提升检测精度，需要进一步提升中心点的检测精度</strong>。</p>
<h1 id="CenterNet-Points"><a href="#CenterNet-Points" class="headerlink" title="CenterNet: Points"></a>CenterNet: Points</h1><p>目前见过最优雅的方法, 速度快精度高而且拓展性非常强! 微信和支付宝扫一扫目前都是用的这个方法.可视化效果如下图所示:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cornernet_sample.png"></p>
<h2 id="贡献-3"><a href="#贡献-3" class="headerlink" title="贡献"></a>贡献</h2><ol>
<li>用 heatmap 预测的目标中心点代替 Anchor 机制来预测目标，使用更大分辨率的输出特征图（相对于原图缩放了 4 倍），因此无需用到多层特征，实现了真正的 Anchor-Free。</li>
<li>网络可拓展性非常强，论文中介绍了实现 3D 目标检测和人体姿态估计任务。具体来说对于 3D 目标检测，直接回归得到目标的深度信息，3D目标框的尺寸，目标朝向；对于人体姿态估计来说，将关键点位置作为中心的偏移量，直接在中心点回归出这些偏移量的值。例如对于姿态估计任务需要回归的信息如 Figure 4 所示。</li>
<li>模型设计简单，因此在运行速度和精度的平衡上取得了很好的结果。</li>
</ol>
<p>CenterNet 和 Anchor-Based 检测器的区别下图所示:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/diff_between_anchor_and_free.png"></p>
<h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p>整体思路很简洁, 在 backbone 后面接上 3 个卷积层来分别作为 heatmap, coordinate offsets 和 size(不是 offset) 就行. heatmap 用于预测中心点的位置, coordinate offset 预测中心点的偏移量, size offset 预测宽和高的偏移量, 直接看<a href="">源码</a>来理解:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(hm): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">80</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br><span class="line">(wh): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">2</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br><span class="line">(reg): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">2</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="前置内容"><a href="#前置内容" class="headerlink" title="前置内容"></a>前置内容</h2><p>对于 $I \in R^{H \times W \times 3}$ 的输入图像, 需要生成关键点的热力图 $\hat{Y} \in [0, 1]^{\frac{W}{R} \times \frac{H}{R}} \times C$, $R$ 是缩放步长, $C$ 是类别数, 原文中设置为 4. $\hat{Y}<em>{x,y,c}=1$ 表示检测到的关键点, $\hat{Y}</em>{x,y,c}=0$ 表示为背景, 对于原图上的坐标 $p$, 经过下采样之后的坐标为 $\tilde{p} = \lfloor \frac{p}{R} \rfloor$, GT 也是通过二维的高斯核来得到:</p>
<p>$$<br>Y_{x y c}=\exp \left(-\frac{\left(x-\tilde{p}<em>{x}\right)^{2}+\left(y-\tilde{p}</em>{y}\right)^{2}}{2 \sigma_{p}^{2}}\right)<br>$$</p>
<p>其中 $\sigma_{p}$ 是目标尺度-自适应的标准方差, 和 CornerNet 中一致, 如果两个高斯分布发生了重叠，直接取元素间最大的就可以, 损失函数也和 CornerNet 一样是 Focal Loss 的变体:</p>
<p>$$<br>L_{k}=\frac{-1}{N} \sum_{x y c}\left{\begin{array}{cl}<br>\left(1-\hat{Y}<em>{x y c}\right)^{\alpha} \log \left(\hat{Y}</em>{x y c}\right) &amp; \text { if } Y_{x y c}=1 \<br>\left(1-Y_{x y c}\right)^{\beta}\left(\hat{Y}<em>{x y c}\right)^{\alpha}<br>\log \left(1-\hat{Y}</em>{x y c}\right) &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p>
<p>$\alpha=2, \beta=4$ 是超参数, $N$ 是关键点的个数, 用于将 Focal Loss 归一化, 由于下采样会对关键点的坐标产生偏差, 所以这里使用 L1 Loss 对每个中心点都预测了偏移量:</p>
<p>$$<br>L_{o f f}=\frac{1}{N} \sum_{p}\left|\hat{O}_{\tilde{p}}-\left(\frac{p}{R}-\tilde{p}\right)\right|<br>$$</p>
<h2 id="检测原理"><a href="#检测原理" class="headerlink" title="检测原理"></a>检测原理</h2><p>上面已经能够得到中心点的坐标和偏移了, 所以为了得到最终的 bounding box 还需要预测 w 和 h, 对于目标 k 的 bounding box, 其坐标为 $(x_1^{(k)}, y_1^{(k)}, x_2^{(k)}, y_2^{(k)})$, 那么中心点的坐标为 $p_{k}=\left(\frac{x_{1}^{(k)}+x_{2}^{(k)}}{2}, \frac{y_{1}^{(k)}+y_{2}^{(k)}}{2}\right)$. 每个 bounding box 的真实尺寸为$s_{k}=\left(x_{2}^{(k)}-x_{1}^{(k)}, y_{2}^{(k)}-y_{1}^{(k)}\right)$, 从上面的代码可以看出这里生成一个 $\hat{S} \in \mathcal{R}^{\frac{W}{R} \times \frac{H}{R} \times 2}$ 特征图用于预测 $w$ 和 $h$, 那么尺寸误差就定义为中心点预测的尺度和真实尺度的 L1 Loss:<br>$$<br>L_{s i z e}=\frac{1}{N} \sum_{k=1}^{N}\left|\hat{S}<em>{p</em>{k}}-s_{k}\right|<br>$$</p>
<p>最终的总损失函数为:<br>$$<br>L_{d e t}=L_{k}+\lambda_{s i z e} L_{s i z e}+\lambda_{o f f} L_{o f f}<br>$$</p>
<p>原理就是在 backbone 后预测一个 $W \times H \times (C + 2 + 2)$ 的特征图, $C$ 用于预测关键点 $\hat{Y}$, 2 个 2 分别用于预测中心点的偏移量 $\hat{O}$ 以及 尺寸大小 $\hat{S}$</p>
<h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>推理的时候需要得到 heatmap 上每个类别的峰值点, 做法是将热力图上的所有响应点与其连接的 8 个临近点进行比较，如果该点响应值大于或等于其 8 个临近点值则保留(可以使用 $3 \times 3$ 的 maxpool 来实现)，最后我们保留所有满足之前要求的 top-100 个峰值点。对于检测到的中心点 $\hat{P}$, 预测的坐标为 $(\hat{x}<em>i, \hat{y}<em>i)$, 预测的偏移量为 $\left(\delta \hat{x}</em>{i}, \delta \hat{y}</em>{i}\right)=\hat{O}<em>{\hat{x}</em>{i} \hat{y}<em>{i}}$, 预测的 size 为 $\left(\hat{w}</em>{i}, \hat{h}<em>{i}\right)=\hat{S}</em>{\hat{x}<em>{i}, \hat{y}</em>{i}}$, 最终得到的 bounding box 的坐标为:</p>
<p>$$\begin{array}{l}<br>\left(\hat{x}<em>{i}+\delta \hat{x}</em>{i}-\hat{w}<em>{i} / 2, \hat{y}</em>{i}+\delta \hat{y}<em>{i}-\hat{h}</em>{i} / 2\right. \<br>\left.\hat{x}<em>{i}+\delta \hat{x}</em>{i}+\hat{w}<em>{i} / 2, \hat{y}</em>{i}+\delta \hat{y}<em>{i}+\hat{h}</em>{i} / 2\right)<br>\end{array}$$</p>
<h2 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h2><p>可以看到 CenterNet 的精度吊打了 YOLOv3，并且完全的 Anchor-Free 使得我们看到了目标检测更好的思路!<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cornernet_results.png"></p>
<p>作者也做了消融性验证:<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/centernet_ablation_study.png"></p>
<h1 id="FCOS"><a href="#FCOS" class="headerlink" title="FCOS"></a>FCOS</h1><p>这是 CVPR 2019 的一篇文章，也是 Anchor-Free 的文章，2019 真的算是 Anchor-Free 的当打之年啊。</p>
<h2 id="贡献-4"><a href="#贡献-4" class="headerlink" title="贡献"></a>贡献</h2><ol>
<li>FCOS 框架和许多基于 FCN 的框架统一, 这样就能更好地用于其他任务</li>
<li>FCOS 是 anchor-free 的, 这样减少了超参的个数, 解放了调参侠</li>
<li>通过消除锚框，新探测器完全避免了复杂的 IoU 计算以及训练期间锚框和真实边框之间的匹配，并将总训练内存占用空间减少了 2 倍左右。</li>
<li>FCOS 可以作为 Two-Stage 中的 RPN 来生成 proposals, 而且效果还比 RPN 好!</li>
</ol>
<h2 id="Anchor-Based-的不足"><a href="#Anchor-Based-的不足" class="headerlink" title="Anchor-Based 的不足"></a>Anchor-Based 的不足</h2><p>还是批评了 Anchor-Based 方法的不足之处：</p>
<ol>
<li>检测结果的好坏对于一些超参数是很敏感的，比如 sizes, aspect ratios 以及 anchor boxes 的数目等</li>
<li>由于 anchor 的 size 是 fixed 的，所以检测器在处理那些形状变化比较大的 object (尤其是小物体)时会遇到困难，此外虽然 anchor 给了检测器一个先验，但是却影响了其泛化能力。</li>
<li>为了提高 Recall Rate，检测器必须生成很多的 anchor，而在训练过程中，一大部分 anchor 都是负样本(背景)，这又会导致正负样本不均衡的问题。</li>
<li>计算 proposals 和 Ground Truth 之间的 IoU 是一个计算量比较大的过程。</li>
</ol>
<h2 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos.png"></p>
<p>从上面的 Pipeline 可以看到 FCOS 的结构非常简洁, backbone 后接 FPN, 然后用共享的 head 来预测 3 个 branches, 分别用于 classification, center-ness 和 regression.</p>
<h2 id="全卷积检测器"><a href="#全卷积检测器" class="headerlink" title="全卷积检测器"></a>全卷积检测器</h2><p>在其他方法中, 是把原图中的 $(x, y)$ 映射为 $(\lfloor \frac{x}{n} \rfloor, \lfloor \frac{y}{n} \rfloor)$, 然后预测 offsets:<br>$$<br>\boldsymbol{o}<em>{k}=\left(\frac{x</em>{k}}{n}-\left\lfloor\frac{x_{k}}{n}\right\rfloor, \frac{y_{k}}{n}-\left\lfloor\frac{y_{k}}{n}\right\rfloor\right)<br>$$</p>
<p>在 FCOS 中, 对于特征图上的一点 $(x, y)$, 把它映射回原图中的位置是:<br>$$<br>(\lfloor \frac{s}{2} \rfloor + xs, \lfloor \frac{s}{2} \rfloor + ys)<br>$$</p>
<p>作者认为这样映射到原图上的位置能够接近特征图上的点 $(x, y)$ 感受野的中心从而得到更好的结果, 然后直接预测一个 4 维向量 $t^{<em>} = (l^{</em>}, t^{<em>}, r^{</em>}, b^{<em>})$, 代表这个点在原图上分别到 bounding box 的左, 上, 右, 下边界的距离, 除此以外还预测一个类别 $c^{</em>}$, $c^{*} = 0$ 表示为背景, 否则表示为其他类别.</p>
<p>虽然上述方法很简明, 但是会存在一个问题, 就是当特征图上的一个点被映射回原图上时, 位于原图上的点可能在多个 bounding box 里面, 这样的点在文章中被称为 <code>&quot;ambiguous samples&quot;</code>, 作者的解决方式是简单地选择更小面积的 bounding box 来作为回归的目标.</p>
<p>对于 Ground Truth 的 Bounding Box $B_i = (x_o^{(i)}, y_o^{(i)}, x_1^{(i)}, y_1^{(i)})$, 特征图上的点映射到原图中为 $(x, y)$, 那么训练过程中需要回归的目标就是:<br>$$\begin{aligned}<br>l^{<em>} &amp;=x-x_{0}^{(i)}, \quad t^{</em>}=y-y_{0}^{(i)} \<br>r^{<em>} &amp;=x_{1}^{(i)}-x, \quad b^{</em>}=y_{1}^{(i)}-y<br>\end{aligned}$$</p>
<p>可以看到<strong>以这种方式来进行检测, 可以尽可能多的得到前景的 samples 来进行检测器的训练, 作者认为这也是 FCOS 好的原因所在</strong></p>
<p>网络最后得到一个 80D 的特征图用于分类, 得到一个 4D 的特征图用于回归, 和 RetinaNet 一样训练 C binary 的分类器,, 由于回归的结果总会是正数, 所以使用 $exp(x)$ 来代替回归分支中的任一真实值, 这样能让结果更加平滑, 有点类似于 CornerNet 中的预测 offsets 而不是直接预测坐标, 这样能让网络更容易学习. 最终的损失函数可以表示为:<br>$$\begin{aligned}<br>L\left(\left{\boldsymbol{p}<em>{x, y}\right},\left{\boldsymbol{t}</em>{x, y}\right}\right) &amp;=\frac{1}{N_{\mathrm{pos}}} \sum_{x, y} L_{\mathrm{cls}}\left(\boldsymbol{p}<em>{x, y}, c</em>{x, y}^{<em>}\right) \<br>&amp;+\frac{\lambda}{N_{\mathrm{pos}}} \sum_{x, y} \mathbb{1}<em>{\left{c</em>{x, y}^{</em>}&gt;0\right}} L_{\mathrm{reg}}\left(\boldsymbol{t}<em>{x, y}, \boldsymbol{t}</em>{x, y}^{*}\right)<br>\end{aligned}$$</p>
<p>$L_{cls}$ 就是 RetinaNet 中提出的 Focal Loss, $L_{reg}$ 是 UnitBox 中提出的 IoU Loss, $N_{pos}$ 表示正样本的数量, $\lambda=1$ 用于平衡 $L_{reg}$ 的权重, 在特征图的所有位置上计算总和, $\mathbb{1}<em>{\left{c</em>{x, y}^{<em>}&gt;0\right}} = 1$ 如果 $c^{</em>}<em>{i} &gt; 0$, 否则 $\mathbb{1}</em>{\left{c_{x, y}^{*}&gt;0\right}} = 0$.</p>
<p>在 Inference 阶段直接前向传播网络, 在特征图的每个位置上都能得到一个分类概率 $p_{x, y}$ 和 一个回归向量 $t_{xy}$, 选择 $p_{x, y} &gt; 0.05$ 的位置作为正样本, 然后使用下式来得到 bounding box.<br>$$\begin{aligned}<br>l^{<em>} &amp;=x-x_{0}^{(i)}, \quad t^{</em>}=y-y_{0}^{(i)} \<br>r^{<em>} &amp;=x_{1}^{(i)}-x, \quad b^{</em>}=y_{1}^{(i)}-y<br>\end{aligned}$$</p>
<h2 id="FCOS-中的多尺度预测"><a href="#FCOS-中的多尺度预测" class="headerlink" title="FCOS 中的多尺度预测"></a>FCOS 中的多尺度预测</h2><p>这里使用了 FPN 的结构来进行多尺度预测, 之所以使用多尺度预测的好处主要有 2 个:</p>
<ol>
<li>当 stride 很大时(e.g. 16), BPR(best possible recall, 可以理解为一个检测器能够达到的 recall rate 的上界) 会很低, 在 anchor-based 方法中可以通过降低 positive sample 所需要的 IoU 来缓解这个问题, 但在 FCOS 中即使是很大的 stride , BPR 也很高, 使用多尺度预测只是为了进一步提高 BPR.</li>
<li>能减少上文中的 <code>&quot;ambiguous samples&quot;</code> 现象, 从而得到更好的结果.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_fpn.png"></li>
</ol>
<p>作者直接限制 bounding box 在不同 level 的特征图上的回归范围, 具体的, 先在特征图上计算回归的 targets: $l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}$, 接着如果一个位置满足 $max(l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}) &gt; m_i$ 或者 $min(l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}) &lt; m_{i-1}$, 那么这个位置就被认为是一个负样本 (negative sample), 不进行任何一个 bounding box 的 regression, 其中 $m_i$ 是 feature level $i$ 需要回归的最大距离, 文中 $m_2, m_3, m_4, m_5, m_6, m_7$ 分别设置为 $0, 64, 128, 256, 512, \infty$, 这样就实现了多尺度的预测, 如果一个位置在经过多尺度预测后在原图上还是存在多个 bounding box, 那么也是简单地选择 bounding box 面积最小的那个.</p>
<p>作者和 FPN 一样, 使用了权重共享的 head, 但是作者发现不同 feature level 需要回归的尺寸范围不同, 比如 P3 的范围是 [0, 64], P4 的范围是 [64, 128], 所以使用权重共享的 head 是不合理的, 但为了减少参数和计算量, 作者还是希望使用共享权重的 head, 所以作者并没有在特征图上使用标准的 $exp(x)$, 而是加了一个标量 $exp(s_{i}x)$, 用于在特征图 $P_i$ 上进行动态调整.</p>
<h2 id="Center-ness-for-FCOS"><a href="#Center-ness-for-FCOS" class="headerlink" title="Center-ness for FCOS"></a>Center-ness for FCOS</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_center-ness.png"><br>作者在使用了多尺度预测后发现和 anchor-based 方法相比还是存在精度上的 gap, 作者分析了原因, 认为这是由于很多特征图上的点映射到原图时会偏离物体的中心点太远, 这样的点产生的 bounding box 的质量都很低, 从而导致精度一直上不去, 作者提出了一个无需超参的方法来解决这个问题, 直接<strong>在 head 上加一个和分类分支并行的分支</strong>用于预测一个位置上的 center-ness, center-ness 度量的是特征图上的点映射到原图上的位置距离物体中心点的距离, 所以对于一个位置上给定的回归 targets: $l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}$, center-ness target 可以被定义为:<br>$$<br>\text { centerness }^{<em>}=\sqrt{\frac{\min \left(l^{</em>}, r^{<em>}\right)}{\max \left(l^{</em>}, r^{<em>}\right)} \times \frac{\min \left(t^{</em>}, b^{<em>}\right)}{\max \left(t^{</em>}, b^{*}\right)}}$$</p>
<p>上式可以看到使用平方根来减缓 center-ness 的衰减, center-ness 的范围是 [0, 1], 使用 BCE Loss 训练, 这个损失会添加到上面说的损失函数中, 在 test 时每个 bounding box 的 score 是预测的 center-ness 和 classification score 相乘得到的, 最后使用 NMS 来进一步过滤低质量的 bounding box. 作者在提交后也仅用真实的中心来作为正样本(其他位置不知道是设为负样本还是用二维高斯分布来做, 具体可以看源码)来代替 center-ness 的形式, 得到了更好的结果.</p>
<h2 id="Results-2"><a href="#Results-2" class="headerlink" title="Results"></a>Results</h2><p>可以看到没使用多尺度预测的 FCOS 的 BPR 已经很高了, 使用之后效果更好.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_results_bpr.png"><br>使用多尺度预测后的 “ambiguous samples” 比例显著嫌少<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_results_ambiguous_samples.png"><br>进行了消融性验证, 肯定了 center-ness 的重要性<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_results_center-ness.png"><br>作者把 FPN 中的 RPN 给换成了 FCOS, 然后用 FCOS 生成的 bounding box 在 FPN 上的结果和使用 RPN 的 FPN 结果进行了对比…发现 FPN with FCOS 完虐 FPN with RPN…<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_results_fcos_as_rpn.png"><br>最后就是和其他 SOTA 方法的效果对比了, 直接上图, 结果很不错.<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/fcos_results_with_sota.png"><br>上图中的 improvements 包括:</p>
<ol>
<li>把 center-ness branch 从 classification branch 移动到 regression branch</li>
<li>只将 ground-truth boxes 的中心位置设置为正样本</li>
<li>使用 GIoU Loss</li>
<li>标准化回归的目标 $l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}$</li>
</ol>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ming Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://coderming.cn/2020/06/20/Anchor-Free-Object-Detection/">https://coderming.cn/2020/06/20/Anchor-Free-Object-Detection/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Anchor-Free/">Anchor Free</a></div><div class="post_share"><div class="social-share" data-image="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/wechat.png" target="_blank"><img class="post-qr-code-img" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/wechat.png" alt="WeChat"/></a><div class="post-qr-code-desc">WeChat</div></li><li class="reward-item"><a href="https://qr.alipay.com/fkx12431y6k0soy7vokzi19" target="_blank"><img class="post-qr-code-img" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/alipay.jpg" alt="AliPay"/></a><div class="post-qr-code-desc">AliPay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/07/20/NAS/"><img class="prev-cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/07/NAS/avatar.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Network Architecture Search</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/20/Two-Stage-Object-Detection/"><img class="next-cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Two-Stage Object Detection</div></div></a></div></nav></article></main><footer id="footer" style="background-image: url(https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cover.png)"><div id="footer-wrap"><div class="copyright">&copy;2020 By Ming Li</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div></div></body></html>