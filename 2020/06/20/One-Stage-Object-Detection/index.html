<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>One-Stage Object Detection | Ming</title><meta name="keywords" content="One-Stage"><meta name="author" content="Ming Li"><meta name="copyright" content="Ming Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="YOLO v1CSDN的这篇文章写的非常详细, 可以瞻仰下 贡献One-Stage 算法的开山之作, 直接在输出层回归 bounding box 的位置以及所属类别。 简述回顾 YOLO 之前的目标检测算法, 都是基于产生大量可能包含物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体, 以及物体所属类别的概率或者置信度, 同时需要后处理修正边界框, 最后基于一些准则过滤掉置">
<meta property="og:type" content="article">
<meta property="og:title" content="One-Stage Object Detection">
<meta property="og:url" content="https://coderming.cn/2020/06/20/One-Stage-Object-Detection/index.html">
<meta property="og:site_name" content="Ming">
<meta property="og:description" content="YOLO v1CSDN的这篇文章写的非常详细, 可以瞻仰下 贡献One-Stage 算法的开山之作, 直接在输出层回归 bounding box 的位置以及所属类别。 简述回顾 YOLO 之前的目标检测算法, 都是基于产生大量可能包含物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体, 以及物体所属类别的概率或者置信度, 同时需要后处理修正边界框, 最后基于一些准则过滤掉置">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/cover.png">
<meta property="article:published_time" content="2020-06-20T07:47:12.000Z">
<meta property="article:modified_time" content="2020-08-27T03:24:45.970Z">
<meta property="article:author" content="Ming Li">
<meta property="article:tag" content="One-Stage">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/cover.png"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://coderming.cn/2020/06/20/One-Stage-Object-Detection/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-08-27 11:24:45'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">19</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/images/"><i class="fa-fw fas fa-images"></i><span> Images</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#YOLO-v1"><span class="toc-number">1.</span> <span class="toc-text">YOLO v1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE"><span class="toc-number">1.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E8%BF%B0"><span class="toc-number">1.2.</span> <span class="toc-text">简述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.3.</span> <span class="toc-text">原理介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5"><span class="toc-number">1.4.</span> <span class="toc-text">测试阶段</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E8%B6%B3"><span class="toc-number">1.6.</span> <span class="toc-text">不足</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SSD"><span class="toc-number">2.</span> <span class="toc-text">SSD</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-1"><span class="toc-number">2.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81%E5%9B%BE%E6%A3%80%E6%B5%8B"><span class="toc-number">2.2.</span> <span class="toc-text">多尺度特征图检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%A3%80%E6%B5%8B%E5%99%A8"><span class="toc-number">2.3.</span> <span class="toc-text">卷积检测器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Default-boxes-and-aspect-ratio"><span class="toc-number">2.4.</span> <span class="toc-text">Default boxes and aspect ratio</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">2.5.</span> <span class="toc-text">训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8C%B9%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">2.5.1.</span> <span class="toc-text">匹配策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1"><span class="toc-number">2.5.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE-scales-%E5%92%8C-ratios"><span class="toc-number">2.5.3.</span> <span class="toc-text">设置 scales 和 ratios</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hard-negative-mining"><span class="toc-number">2.5.4.</span> <span class="toc-text">Hard negative mining</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#YOLO-v2"><span class="toc-number">3.</span> <span class="toc-text">YOLO v2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-2"><span class="toc-number">3.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E6%96%B9%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">改进方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Normalization"><span class="toc-number">3.2.1.</span> <span class="toc-text">Batch Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#High-Resolution-Classifier"><span class="toc-number">3.2.2.</span> <span class="toc-text">High Resolution Classifier</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolutional-With-Anchor-Boxes"><span class="toc-number">3.2.3.</span> <span class="toc-text">Convolutional With Anchor Boxes.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dimension-Clusters"><span class="toc-number">3.2.4.</span> <span class="toc-text">Dimension Clusters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Direct-location-prediction"><span class="toc-number">3.2.5.</span> <span class="toc-text">Direct location prediction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fine-Grained-Features"><span class="toc-number">3.2.6.</span> <span class="toc-text">Fine-Grained Features</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-Scale-Training"><span class="toc-number">3.2.7.</span> <span class="toc-text">Multi-Scale Training</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%9A%84-3-%E4%B8%AA%E9%98%B6%E6%AE%B5"><span class="toc-number">3.3.</span> <span class="toc-text">训练的 3 个阶段</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-2"><span class="toc-number">3.4.</span> <span class="toc-text">损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#YOLO-v3"><span class="toc-number">4.</span> <span class="toc-text">YOLO v3</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E6%AE%8B%E5%B7%AE%E7%BB%93%E6%9E%84"><span class="toc-number">4.1.</span> <span class="toc-text">引入残差结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FPN-%E7%BB%93%E6%9E%84%E7%9A%84%E5%BC%95%E5%85%A5"><span class="toc-number">4.2.</span> <span class="toc-text">FPN 结构的引入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Anchor"><span class="toc-number">4.3.</span> <span class="toc-text">Anchor</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#YOLO-v4"><span class="toc-number">5.</span> <span class="toc-text">YOLO v4</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RefineDet"><span class="toc-number">6.</span> <span class="toc-text">RefineDet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Gaussian-YOLO-v3"><span class="toc-number">7.</span> <span class="toc-text">Gaussian YOLO v3</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Guided-Anchoring"><span class="toc-number">8.</span> <span class="toc-text">Guided Anchoring</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#M2Det"><span class="toc-number">9.</span> <span class="toc-text">M2Det</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#EfficientDet"><span class="toc-number">10.</span> <span class="toc-text">EfficientDet</span></a></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/cover.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ming</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/images/"><i class="fa-fw fas fa-images"></i><span> Images</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">One-Stage Object Detection</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-06-20T07:47:12.000Z" title="Created 2020-06-20 15:47:12">2020-06-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-08-27T03:24:45.970Z" title="Updated 2020-08-27 11:24:45">2020-08-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Object-Detection/">Object Detection</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">5.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>21min</span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/c20081052/article/details/80236015">CSDN的这篇文章写的非常详细, 可以瞻仰下</a></p>
<h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><p>One-Stage 算法的开山之作, 直接在输出层回归 bounding box 的位置以及所属类别。</p>
<h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>回顾 YOLO 之前的目标检测算法, 都是基于产生大量可能包含物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体, 以及物体所属类别的概率或者置信度, 同时需要后处理修正边界框, 最后基于一些准则过滤掉置信度不高和重叠度较高的边界框, 进而得到检测结果。这种基于先产生候选区域再进行检测的方法虽然有较高的精度, 但速度非常慢。YOLO 直接将目标检测堪称一个回归问题进行处理, 将候选区和检测两个阶段合二为一。 YOLO 的检测过程如下所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov1_system.png"></p>
<p>可以看到 YOLO v1 用于检测只有非常简单的三个步骤:</p>
<ol>
<li>resize 图像</li>
<li>CNN 提取特征并预测</li>
<li>利用 NMS 过滤 bbox</li>
</ol>
<h2 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h2><p>最终会得到一个 $S \times S$ 的特征图, <strong>如果某个 object 的中心落在这个网格中, 则这个网格就负责预测这个 object</strong>。每个 grid Cell 预测 $B$ 个 bounding box 的 4 个坐标以及每个 bounding box 的 confidence, 这里的 confidence 表示一个 bounding box 是否含有目标以及含有目标的准确度, 如果一个 bounding box 没有目标, 那么它的 confidence 应该为 0, 否则它的 confidence 应该为 bounding box 和 Ground Truth 的 IoU大小, 即定义为以下公式：<br>$$<br>confidence = Pr(Object) \cdot IoU_{pred}^{trutth}<br>$$</p>
<p>坐标 x, y 代表了预测的 bounding box 的中心与栅格边界的相对值(offset)。坐标 w, h 代表了预测的 bounding box 的 width、height 相对于整幅图像 width, height 的比例。每一个栅格还要预测 C 个 conditional class probability（条件类别概率）：$Pr(Class_i | Object)$。即在一个栅格包含一个 Object 的前提下, 它属于某个类的概率。我们只为每个栅格预测一组（C个）类概率, 而不考虑框 B 的数量。即不管一个 grid 预测多少个 bounding box, 每个 bounding box 的类别就是那个 grid 预测的类别。也就是说 class 信息是针对每个网格的, confidence 信息是针对每个 bounding box 的, 如图所示：</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolo_model.png"></p>
<h2 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h2><p>在 test 的时候, 将每个 bounding box 的 confidence 和每个 grid 预测的条件概率相乘得到每个 bounding box 的 class-specific confidence score：<br>$$<br>Pr(Class_{i} | Object) \cdot Pr(Object) *  IoU_{pred}^{trutth} = Pr(Class_{i}) \cdot IoU_{pred}^{trutth}<br>$$</p>
<p>得到每个 bbox 的 class-specific confidence score 以后, 设置阈值, 滤掉得分低的 boxes, 对保留的 boxes 进行 NMS 处理, 就得到最终的检测结果。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>这里有 3 个损失, 分别是坐标误差, 置信度误差以及分类误差。</p>
<p>这里简单的使用平方误差和 (sum-squared error loss) 来优化模型, 因为平方误差和容易优化, 但是这样会有 2 个不足之处:</p>
<ol>
<li>简单地将 localization error 和 classification error 设为同等权重是明显不合理的</li>
<li>有很多的 grid Cells 并不会含有 object, 这就导致这些 Cells 的 预测的 bounding box 的 confidence 为 0, 相比于较少的有object的栅格, 这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献, 这会导致网络不稳定甚至发散。</li>
</ol>
<p>为了解决上面的问题作者设置了 2 个参数, 分别用于增大 bounding box 回归的损失权重以及减少那些不含 object 的 bounding box 的分类损失权重, 在原文中分别设置为 $\lambda_{coord} = 5$ and  $\lambda_{noobj} = 0.5$</p>
<p>在对不同大小的 bbox 预测中, 相比于大 bbox 预测偏一点, 小 bbox 预测偏一点肯定更不能被忍受的。而 sum-square error loss 中对同样的偏移 loss 是一样。为了解决这个问题, 在计算 loss 时使用 bbo x的 width 和 height 取平方根代替原本的 height 和 width, 原因是小 bbox 的横轴值较小, 发生偏移时, 反应到 y 轴上相比大 box 要大。</p>
<p>所以最终的损失函数如下所示：</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov1_loss_function.png"></p>
<div class="hide-toggle" style="border: 1px solid  #D3D3D3"><div class="hide-button toggle-title" style="background-color:  #D3D3D3;"><i class="fas fa-caret-right fa-fw"></i><span>Copy LaTex formula</span></div>
    <div class="hide-content"><p>$$<br>\begin{aligned}<br>{\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left[\left(x</em>{i}-\hat{x}<em>{i}\right)^{2}+\left(y</em>{i}-\hat{y}<em>{i}\right)^{2}\right]} \<br>{+\lambda</em>{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left[(\sqrt{w</em>{i}}-\sqrt{\hat{w}<em>{i}})^{2}+(\sqrt{h</em>{i}}-\sqrt{\hat{h}<em>{i}})^{2}\right]} \<br>+\sum</em>{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\lambda</em>{\text {noobj }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {noobj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\sum</em>{i=0}^{S^{2}} \mathbb{1}<em>{i}^{\text {obj }} \sum</em>{c \in \text { classes }}\left(p_{i}(c)-\hat{p}_{i}(c)\right)^{2}<br>\end{aligned}<br>$$</p>
</div></div>


<p>在这个损失函数中：</p>
<ol>
<li>只有当某个网格中有 object 的时候才对 classification error 进行惩罚。</li>
<li>只有当某个 bbox predictor 对某个 ground truth box 负责的时候, 才会对 bbox 的 coordinate error 进行惩罚, 而对哪个 ground truth bbox 负责就看其预测值和 ground truth bbox 的 IoU 是不是在那个 Cell 的所有 box 中最大。</li>
</ol>
<p>总的来说就是与 ground truth 匹配的先验框计算坐标误差、置信度误差（此时 target 为 1）以及分类误差, 而其它的边界框只计算置信度误差（此时 target 为 0）。</p>
<h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><ol>
<li>每个 grid Cell 只预测一个 类别的 Bounding Boxes, 而且最后只取置信度最大的那个 Box。这就导致如果多个不同物体(或者同类物体的不同实体)的中心落在同一个网格中, 会造成漏检。</li>
<li>预测的 bbox 对于尺度的变化比较敏感, 在尺度上的泛化能力比较差。</li>
<li>相对 Two-Stage 算法而言检测精度低。</li>
<li>Recall 很低。</li>
</ol>
<h1 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h1><h2 id="贡献-1"><a href="#贡献-1" class="headerlink" title="贡献"></a>贡献</h2><ol>
<li>通过在特征图上使用 $3 \times 3$ 的小卷积来直接预测类别以及边界框的偏移值 (YOLO v1 和 Faster R-CNN 的 FPN 中都是预测是否含有物体, 而不是直接预测类别)。</li>
<li>在不同尺度的特征图上使用 $3 \times 3$ 的小卷积预测, 提高了小目标的检测精度, 一定程度上解决了多尺度的问题。</li>
</ol>
<h2 id="多尺度特征图检测"><a href="#多尺度特征图检测" class="headerlink" title="多尺度特征图检测"></a>多尺度特征图检测</h2><p>在 YOLO v1 中只在最后的那个 $7 \times 7$ 特征图来进行检测(生成 anchor), 在 SSD 中是在 backbone 中多个不同尺度的特征图上都进行检测。如图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/ssd_and_yolov1.png"></p>
<p>原文对于 300x300 的输入, 分别在 conv4_3,  conv7, conv8_2, conv9_2, conv10_2, conv11_2 的特征图上的每个单元取 4, 6, 6, 6, 4, 4 个 default box。</p>
<h2 id="卷积检测器"><a href="#卷积检测器" class="headerlink" title="卷积检测器"></a>卷积检测器</h2><p>由上图可以看到, 对于 backbone 中每个用于检测的特征图, 都使用一个 $3 \times 3 \times (k \times (classes + 4))$ 的卷积核来进行, 其中 $k$ 是每个特征图上的 grid 生成 anchor 的数目。这里为每个 anchor 都预测了其类别和坐标的 offsets。</p>
<h2 id="Default-boxes-and-aspect-ratio"><a href="#Default-boxes-and-aspect-ratio" class="headerlink" title="Default boxes and aspect ratio"></a>Default boxes and aspect ratio</h2><p>这里其实就是相当于预设好的 anchors, 和 Faster R-CNN 中一致, 只是在多个特征层上都提生成了 anchor 而已, 一般宽高比在{ 1, 2, 3, 1/2, 1/3} 中选取, 以上图为例, 由于以上特征图的大小分别是 38x38, 19x19,10x10, 5x5, 3x3, 1x1, 所以一共得到 38 x 38 x 4 + 19 x 19 x 6 + 10 x 10 x 6 + 5 x 5 x 6 +  3 x 3 x 4 + 1 x 1 x 4 = 8732 个 default box。即对一张 300 x 300 的图片输入网络将会针对这 8732 个 default box 预测 8732 个边界框。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h3 id="匹配策略"><a href="#匹配策略" class="headerlink" title="匹配策略"></a>匹配策略</h3><p>在训练过程中需要决定哪个 default box(anchor) 和 Ground Truth 对应。对于所有的 anchors 而言(不限位置、纵横比还是尺度), 只要其和 Ground Truth 的 jaccard overlap &gt; 0.5, 作者就将这个 anchor 和 Ground Truth 对应, 这样简化了学习问题, 这样网络可以对多个重叠的 anchor 都预测高得分而不是只对重叠度最高的预测得分。</p>
<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>设定 $x^{p}<em>{ij} = \lbrace 1, 0 \rbrace$ 表示第 $i$ 个 anchor 是否和第 $j$ 个 Ground Truth 在类别 $p$ 下对应。对于一张图片, 至少有一个 anchor 会和 Ground Truth 对应, 所以有 $\sum</em>{i} x_{i j}^{p} \geq 1$, 这样总的 loss function 就是 localization loss 和 confidence loss 的加权和(交叉验证时 $\alpha=1$)：</p>
<p>$$<br>L(x, c, l, g)=\frac{1}{N}\left(L_{c o n f}(x, c)+\alpha L_{l o c}(x, l, g)\right)<br>$$</p>
<p>其中 $N$ 是和 Ground Truth 匹配的 anchors 的数目, 如果 $N=0$ 那么 loss = 0, localization loss 是 anchors 预测的 offsets 和实际的 offsets 之间的 Smooth L1 Loss：<br>$$<br>\begin{array}{rl}<br>L_{l o c}(x, l, g)=\sum_{i \in \text {Pos }}^{N} \sum_{m\in{c x, c y, w, h}} &amp; x_{i j}^{k} \operatorname{smooth}<em>{\mathrm{L} 1}\left(l</em>{i}^{m}-\hat{g}<em>{j}^{m}\right) \<br>\hat{g}</em>{j}^{c x}=\left(g_{j}^{c x}-d_{i}^{c x}\right) / d_{i}^{w} &amp; \hat{g}<em>{j}^{c y}=\left(g</em>{j}^{c y}-d_{i}^{c y}\right) / d_{i}^{h} \<br>\hat{g}<em>{j}^{w}=\log \left(\frac{g</em>{j}^{w}}{d_{i}^{w}}\right) &amp; \hat{g}<em>{j}^{h}=\log \left(\frac{g</em>{j}^{h}}{d_{i}^{h}}\right)<br>\end{array}<br>$$</p>
<p>confidence loss 是一个多分类的 softmax loss：<br>$$<br>L_{c o n f}(x, c)=-\sum_{i \in P o s}^{N} x_{i j}^{p} \log \left(\hat{c}<em>{i}^{p}\right)-\sum</em>{i \in N e g} \log \left(\hat{c}<em>{i}^{0}\right) \quad \text { where } \quad \hat{c}</em>{i}^{p}=\frac{\exp \left(c_{i}^{p}\right)}{\sum_{p} \exp \left(c_{i}^{p}\right)}<br>$$</p>
<h3 id="设置-scales-和-ratios"><a href="#设置-scales-和-ratios" class="headerlink" title="设置 scales 和 ratios"></a>设置 scales 和 ratios</h3><p>网络中不同层级的特征图有着不同的感受野, 在 SSD 中设定的 anchors 不一定要和每层中的实际感受野对应, 假设一共使用 $m$ 个特征图来做预测, 那么每个特征图的 anchor 的 scale 可以定义为：<br>$$<br>s_{k}=s_{\min }+\frac{s_{\max }-s_{\min }}{m-1}(k-1), \quad k \in[1, m]<br>$$</p>
<p>原文中 $s_{max} = 0.9, s_{min} = 0.2$, 意味着最低层的尺度是 0.2, 最高层的尺度为 0.9, 对于 anchor, 使用 5 种不同的宽高比 $\alpha_r = \lbrace 1, 2, 3, \frac{1}{2}, \frac{1}{3} \rbrace$, 这样就能计算出每个 anchor 的宽和高：<br>$$<br>\begin{array}{rl}<br>w_{k}^{a}=s_{k} \sqrt{a}<em>{r} \<br>h</em>{k}^{a}=s_{k} \sqrt{a}_{r}<br>\end{array}<br>$$</p>
<p>对于宽高比为 1 的 anchors, 额外增加一个 scale 为 $s’_k = \sqrt{s_k s_{k+1}}$ 的 anchor, 这样一共就能得到 6 个 anchors, 每个 anchor 的中心坐标为 $(\frac{i+0.5}{|f_k|}, \frac{j+0.5}{|f_k|})$ 其中 $f_k$ 是第 $k$ 个特征图的大小, 例如 $k = 2$ 时 $|f_k| = 4$, 这样就得到了所有的 default boxes 即 anchors。</p>
<h3 id="Hard-negative-mining"><a href="#Hard-negative-mining" class="headerlink" title="Hard negative mining"></a>Hard negative mining</h3><p>匹配步骤后, 默认框中的大多数都是 negative, 尤其是候选框个数众多的时候。这就导致 positive 和 negative 训练样本不平衡。这些 negative 样本不全用, 而是对于每一个默认框, 通过最高置信度损失来对它们进行排序, 选择最高的几个, 这样 negative 和 positive 的比例最多是 3:1。这样训练更稳定也更快。</p>
<h1 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h1><h2 id="贡献-2"><a href="#贡献-2" class="headerlink" title="贡献"></a>贡献</h2><p>加了很多的改进方法使得 mAP 从 63.4 提高到了 78.6。</p>
<h2 id="改进方法"><a href="#改进方法" class="headerlink" title="改进方法"></a>改进方法</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov2_tricks.png"></p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>在所有的建基层后面加上 BN 后在 mAP 上提高了 2 个点, 同时由于 BN 能够正则化模型, 避免过拟合, 所以作者把 YOLO v1 中的 Dropout 去掉了。</p>
<h3 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h3><p>几乎所有的目标检测方法都使用在 ImageNet 上预训练的 backbone, 而训练图像的大小肯定会影响最终结果, YOLO v1 是在 $224 \times 224$ 上训练的, 对于 YOLO v2 而言, 先在 ImageNet 上用 $448 \times 448$ 的分辨率对 classification network 做 10 个 epoch 的 fine tune, 再用 $448 \times 448$ 的图片来训练, 这使得最终 mAP 提高了 4 个点。</p>
<h3 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes."></a>Convolutional With Anchor Boxes.</h3><ol>
<li>在 YOLO v1 中直接使用 FC 层来预测 bounding box 的坐标, 这回导致丢失一些空间信息, 在 YOLO v2 里面直接删除了 FC 层, 并且使用 anchor 机制来预测 bounding box。</li>
<li>为了得到更高分辨率的特征图, 去掉了 YOLO v1 中的一个池化层</li>
<li>把 $448 \times 448$ 的分辨率 shrink 到 $416 \times 416$, 这样经过下采样之后的特征图 size 就会是一个奇数, 由于图片中的物体都倾向于出现在图片的中心位置, 若特征图恰好有一个中心位置, 利用这个中心位置预测中心点落入该位置的物体会对这些物体的检测会更容易。</li>
</ol>
<p>这样对于 $416 \times 416$ 的输入就能得到 $13 \times 13$ 的特征图, 特征图上每个 grid 预测 5 个 anchor, 虽然使用 anchor 机制让 mAP 从 69.5 下降到了 69.2, 但这让 Recall 从 81% 提高到了 87%。</p>
<h3 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h3><p>此前 anchor 的 size 都是动手设置好的, 可以把 anchor 看成是一个先验 prior, 虽然网络可以通过学习参数来对最终的 bounding box 坐标进行调整, 但如果一开始就给模型一个更好的 prior, 即更好的 anchor, 网络就能更容易训练也能获得更好的效果。</p>
<p>这里使用了 k-means 聚类算法来对所有的 bounding box 进行聚类, 但问题是如果使用标准的欧式距离的话, 更大的 bounding box 会产生更大的 loss, 我们当然希望 loss 值和 bounding box 的 size 没有关系, 所以利用 IoU 定义了以下的距离函数, 使得误差和 bounding box 的大小无关：<br>$$<br>d(box, centroid) = 1 - IoU(box, centroid)<br>$$</p>
<h3 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a>Direct location prediction</h3><p>作者在引入 anchor 机制后发现模型在早期的训练是很不稳定的, 主要原因在于 bounding box 的 x 和 y 坐标, 通常 anchor 机制中预测的是 offset, 即预测下式中的 $t_x$ 和 $t_y$：</p>
<p>$$<br>\begin{array}{l}<br>x=\left(t_{x} * w_{a}\right)-x_{a} \<br>y=\left(t_{y} * h_{a}\right)-y_{a}<br>\end{array}<br>$$</p>
<p>其中 $(x_a, y_a, w_a, h_a)$ 分别是 anchor 的 4 个坐标。可以看到这个公式没有任何的限制, 所以任何一个 anchor 都可以在图像中的任意一点结束, 这也就导致了模型在随机初始化的情况下, 需要花费很长一段时间来预测合理的 offset。</p>
<p>所以作者继续沿用了 YOLO v1 中直接预测相对于 grid Cell 的坐标位置的方式来得到每个 anchor 的坐标, 对于最终的特征图, 每个 grid Cell 预测 5 个 bounding box, 每个 bounding box 预测 5 个参数, 分别是 $t_x, t_y, t_w, t_h, t_o$, 如果这个 Cell 的左上角点坐标为 $(c_x, c_y)$, 且这个 bounding box 的先验为 $p_w, p_h$, 那么最终得到的 bounding box 为：</p>
<p>$$<br>\begin{aligned}<br>b_{x} &amp;=\sigma\left(t_{x}\right)+c_{x} \<br>b_{y} &amp;=\sigma\left(t_{y}\right)+c_{y} \<br>b_{w} &amp;=p_{w} e^{t_{w}} \<br>b_{h} &amp;=p_{h} e^{t_{h}} \<br>\operatorname{Pr}(\text { object }) * I O U(b, \text { object }) &amp;=\sigma\left(t_{o}\right)<br>\end{aligned}<br>$$</p>
<p>其中 $t_x$ 和 $t_y$ 都经过 $sigmoid$ 函数处理为 [0, 1] 之间的数, 这样归一化处理能使模型训练更加稳定, </p>
<p>可以根据下图来理解：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov2_coordinate_predict.png"></p>
<p>蓝色框为最终预测的 bounding box, 黑色的虚线为先验框, $c_x$ 和 $c_y$ 表示 grid Cell 和图像左上角点的横纵坐标距离,  $p_w$ 和 $p_h$ 是先验框的 size。</p>
<h3 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h3><p>Faster R-CNN(RPN) 和 SSD 都在多个特征图上提取 proposals 以得到不同分辨率, 而作者使用一个 passthrough 层来做这件事, passthrough 层抽取原特征图每个 2 x 2 的局部区域组成新的 channel, 即原特征图大小降低 4 倍, channel 增加 4 倍, 这使得最终 mAP 上升了 1 个点。</p>
<h3 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h3><p>由于 YOLO v2 中只有卷积层和池化层, 所以可以处理任意大小的图片, 为了模型能够在不同的分辨率图像上都取得好结果, 作者采用了多尺度训练的方式, 在训练过程中每隔 10 个 batches 重新选择输入图片的尺寸, 由于 DarkNet-19 的下采样总步长为 32, 所以输入图片的尺寸一般为 32 的倍数{320,352, …。608}, 这种机制使得网络可以更好地预测不同尺寸的图片, 意味着同一个网络可以进行不同分辨率的检测任务</p>
<h2 id="训练的-3-个阶段"><a href="#训练的-3-个阶段" class="headerlink" title="训练的 3 个阶段"></a>训练的 3 个阶段</h2><p>YOLO v2 的训练主要包括三个阶段。第一阶段就是先在 ImageNet 分类数据集上预训练 Darknet-19, 此时模型输入为 $224 \times 224$ , 共训练 160 个 epochs。然后第二阶段将网络的输入调整为 $448 \times 448$ , 继续在 ImageNet 数据集上 finetune 分类模型, 训练 10 个 epochs, 此时分类模型的 top-1 准确度为 76.5%, 而 top-5 准确度为 93.3%。第三个阶段就是修改 Darknet-19 分类模型为检测模型, 并在检测数据集上继续 finetune 检测网络。</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov2_training_stage.jpg"></p>
<h2 id="损失函数-2"><a href="#损失函数-2" class="headerlink" title="损失函数"></a>损失函数</h2><p>首先回顾下 YOLO v1 的损失函数：</p>
<p>$$<br>\begin{aligned}<br>{\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left[\left(x</em>{i}-\hat{x}<em>{i}\right)^{2}+\left(y</em>{i}-\hat{y}<em>{i}\right)^{2}\right]} \<br>{+\lambda</em>{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left[(\sqrt{w</em>{i}}-\sqrt{\hat{w}<em>{i}})^{2}+(\sqrt{h</em>{i}}-\sqrt{\hat{h}<em>{i}})^{2}\right]} \<br>+\sum</em>{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\lambda</em>{\text {noobj }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {noobj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\sum</em>{i=0}^{S^{2}} \mathbb{1}<em>{i}^{\text {obj }} \sum</em>{c \in \text { classes }}\left(p_{i}(c)-\hat{p}_{i}(c)\right)^{2}<br>\end{aligned}<br>$$</p>
<p>第一行和第二行是坐标误差, 第三行和第四行是置信度误差, 第五行是分类误差, 第二行之所以用 $\sqrt{w}, \sqrt{h}$ 来代替 $w, h$ 是因为更小的 bbox 在偏移相同的尺寸下对 IoU 影响更大。</p>
<p>YOLO v2 的损失函数和 YOLO v1 其实相差并不大：<br>$$<br>\begin{aligned}<br>{\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left[\left(x</em>{i}-\hat{x}<em>{i}\right)^{2}+\left(y</em>{i}-\hat{y}<em>{i}\right)^{2}\right]} \<br>{+\lambda</em>{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left(2-w_i * h_i)[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2+(w_i-\hat{w}_i)^2+(h_i-\hat{h}_i)^2\right]} \<br>+\sum</em>{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {obj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\lambda</em>{\text {noobj }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}<em>{i j}^{\text {noobj }}\left(C</em>{i}-\hat{C}<em>{i}\right)^{2} \<br>+\sum</em>{i=0}^{S^{2}} \mathbb{1}<em>{i}^{\text {obj }} \sum</em>{c \in \text { classes }}\left(p_{i}(c)-\hat{p}<em>{i}(c)\right)^{2} \<br>+0.01 *  \sum</em>{i=0}^{S^{2}} \sum_{j=0}^{B} 1^{noobj}_{ij}[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2+(w_i-\hat{w}_i)^2+(h_i-\hat{h}_i)^2]<br>\end{aligned}<br>$$</p>
<p>可以看到和 YOLO v1 的损失函数相比, 第 2 项改变了, 多了第 6 项, 其中第 2 项的作用和 YOLO v1 中第 2 项作用一致, 只是实现思路不同, 都是为了解决更小的 bbox 在偏移相同的尺寸下对 IoU 影响更大的问题。第 6 项只在训练初期使用, 只在 12800 样本之前计算未预测到 target (不含 object) 的 anchors 的坐标损失。</p>
<h1 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h1><p>YOLO v3 相比于 YOLO v2 来说可以简单理解为加入了残差结构和 FPN 架构。</p>
<h2 id="引入残差结构"><a href="#引入残差结构" class="headerlink" title="引入残差结构"></a>引入残差结构</h2><p>主要是把 backbone 换成了 DarkNet-53, 网络结构如下所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/darknet53.png"></p>
<h2 id="FPN-结构的引入"><a href="#FPN-结构的引入" class="headerlink" title="FPN 结构的引入"></a>FPN 结构的引入</h2><p>这里和 FPN 还是有点区别的, 详细可以看以下图解：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/yolov3.jpg"></p>
<p>先把最后一层的 feature map (上图中通道数为 1024)做上采样并利用卷积减少通道数, 再和前一层的 feature map 直接 concatenate 起来(在 FPN 中特征融合的方式是 element-wise add)。对 concatenate 在一起的特征图在经过 CNN 分别得到下一层需要的特征图(上图中的 Conv2D Block 5L 256 ) 以及最终用于生成 anchor 的最终 feature (上图中第 2 个红色的)。</p>
<p>这里之所以出现了 $3 \times 3$ 的卷积是为了能更好的进行分类和边界框回归, 因为预训练好的网络得到的特征图往往是有着较强的分类特征, 使用 $3 \times 3$ 卷积对其进行二次变换能够将分类特征迁移到 分类和回归 这2个 task 所需要的分布上来。这个最早在 Faster R-CNN 中就已经有体现。</p>
<h2 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a>Anchor</h2><p>从上面的 YOLO v3 结构图可以看到一共有 3 个输出(红色的特征图), 其维度分别是 $(batchsize, 52, 52, 75)$、$(batchsize, 26, 26, 75)$ 和 $(batchsize, 13, 13, 75)$, 在这三个输出的特征图上设置 anchor, 原文中使用 k-means 聚类后一共得到 9 个 anchor, 把这 9 个 anchor 平均分到这 3 个不同 scale 的特征图上即可, 例如在 COCO 上聚类得到的 9 个 anchor size 分别为 (10×13,  (16×30), (33×23),  (30×61),  (62×45),  (59×119), (116 × 90), (156 × 198), (373 × 326)。</p>
<p>这样前 3 个 anchor size 在  $(batchsize, 52, 52, 75)$ 的特征图上生成, 即在  $(batchsize, 52, 52, 75)$ 这个特征图上一共生成 $52 \times 52 \times 3 = 8112$ 个 anchor, 所有 anchor 的 size 都是 (10×13);, (16×30),  (33×23) 中的一个, 以此类推, $(batchsize, 26, 26, 75)$ 特征图用于生成 (30×61),  (62×45),  (59×119) 尺寸的 anchors, $(batchsize, 13, 13, 75)$ 特征图用于生成 (116 × 90), (156 × 198), (373 × 326) 大小的 anchors。</p>
<h1 id="YOLO-v4"><a href="#YOLO-v4" class="headerlink" title="YOLO v4"></a>YOLO v4</h1><p>TODO</p>
<h1 id="RefineDet"><a href="#RefineDet" class="headerlink" title="RefineDet"></a>RefineDet</h1><p>TODO</p>
<h1 id="Gaussian-YOLO-v3"><a href="#Gaussian-YOLO-v3" class="headerlink" title="Gaussian YOLO v3"></a>Gaussian YOLO v3</h1><p>TODO</p>
<h1 id="Guided-Anchoring"><a href="#Guided-Anchoring" class="headerlink" title="Guided Anchoring"></a>Guided Anchoring</h1><p>TODO</p>
<h1 id="M2Det"><a href="#M2Det" class="headerlink" title="M2Det"></a>M2Det</h1><p>TODO</p>
<h1 id="EfficientDet"><a href="#EfficientDet" class="headerlink" title="EfficientDet"></a>EfficientDet</h1><p>TODO</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ming Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://coderming.cn/2020/06/20/One-Stage-Object-Detection/">https://coderming.cn/2020/06/20/One-Stage-Object-Detection/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/One-Stage/">One-Stage</a></div><div class="post_share"><div class="social-share" data-image="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/cover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/wechat.png" target="_blank"><img class="post-qr-code-img" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/wechat.png" alt="WeChat"/></a><div class="post-qr-code-desc">WeChat</div></li><li class="reward-item"><a href="https://qr.alipay.com/fkx12431y6k0soy7vokzi19" target="_blank"><img class="post-qr-code-img" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/alipay.jpg" alt="AliPay"/></a><div class="post-qr-code-desc">AliPay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/06/20/Two-Stage-Object-Detection/"><img class="prev-cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Two-Stage Object Detection</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/20/Object-Detection-Review/"><img class="next-cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Object-Detection-Review/review.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Object Detection Review</div></div></a></div></nav></article></main><footer id="footer" style="background-image: url(https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/cover.png)"><div id="footer-wrap"><div class="copyright">&copy;2020 By Ming Li</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div></div></body></html>