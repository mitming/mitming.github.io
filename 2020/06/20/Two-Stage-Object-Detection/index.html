<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Two-Stage Object Detection | Ming</title><meta name="keywords" content="Two-Stage"><meta name="author" content="Ming Li"><meta name="copyright" content="Ming Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Two-Stage 方法简述two-stage检测器包括两个步骤：  使用 RPN 生成 anchor 并对 anchor 做筛选，过滤掉很多的负样本 proposals，生成稀疏的 proposals； 对得到的 proposals 进行分类和回归，对选择的 proposals，使用 roi pooling(roi align) 等操作，进一步的精细化，因此得到的框更加精准。（比如，一个 anc">
<meta property="og:type" content="article">
<meta property="og:title" content="Two-Stage Object Detection">
<meta property="og:url" content="https://coderming.cn/2020/06/20/Two-Stage-Object-Detection/index.html">
<meta property="og:site_name" content="Ming">
<meta property="og:description" content="Two-Stage 方法简述two-stage检测器包括两个步骤：  使用 RPN 生成 anchor 并对 anchor 做筛选，过滤掉很多的负样本 proposals，生成稀疏的 proposals； 对得到的 proposals 进行分类和回归，对选择的 proposals，使用 roi pooling(roi align) 等操作，进一步的精细化，因此得到的框更加精准。（比如，一个 anc">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.jpg">
<meta property="article:published_time" content="2020-06-20T07:47:19.000Z">
<meta property="article:modified_time" content="2020-08-27T03:25:02.065Z">
<meta property="article:author" content="Ming Li">
<meta property="article:tag" content="Two-Stage">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.jpg"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://coderming.cn/2020/06/20/Two-Stage-Object-Detection/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-08-27 11:25:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">19</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/images/"><i class="fa-fw fas fa-images"></i><span> Images</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Two-Stage-%E6%96%B9%E6%B3%95%E7%AE%80%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">Two-Stage 方法简述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#R-CNN"><span class="toc-number">2.</span> <span class="toc-text">R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE"><span class="toc-number">2.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%9D%E6%83%B3"><span class="toc-number">2.2.</span> <span class="toc-text">思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="toc-number">2.3.</span> <span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E8%B6%B3"><span class="toc-number">2.4.</span> <span class="toc-text">不足</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SPPNet"><span class="toc-number">3.</span> <span class="toc-text">SPPNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-1"><span class="toc-number">3.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%9D%E6%83%B3-1"><span class="toc-number">3.2.</span> <span class="toc-text">思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spatial-Pyramid-Pooling"><span class="toc-number">3.3.</span> <span class="toc-text">Spatial Pyramid Pooling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-1"><span class="toc-number">3.4.</span> <span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E8%B6%B3-1"><span class="toc-number">3.5.</span> <span class="toc-text">不足</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fast-R-CNN"><span class="toc-number">4.</span> <span class="toc-text">Fast R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-2"><span class="toc-number">4.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-2"><span class="toc-number">4.2.</span> <span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RoI-Pooling"><span class="toc-number">4.3.</span> <span class="toc-text">RoI Pooling</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%9C%E7%94%A8"><span class="toc-number">4.3.1.</span> <span class="toc-text">作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%9D%E8%B7%AF"><span class="toc-number">4.3.2.</span> <span class="toc-text">思路</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E8%B6%B3-2"><span class="toc-number">4.4.</span> <span class="toc-text">不足</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Faster-R-CNN"><span class="toc-number">5.</span> <span class="toc-text">Faster R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-3"><span class="toc-number">5.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-3"><span class="toc-number">5.2.</span> <span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RPN"><span class="toc-number">5.3.</span> <span class="toc-text">RPN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B1%E4%BA%AB%E5%8D%B7%E7%A7%AF%E5%90%8E%E7%9A%84-3%C3%973-%E5%8D%B7%E7%A7%AF"><span class="toc-number">5.3.1.</span> <span class="toc-text">共享卷积后的 3×3 卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E5%88%86%E6%94%AF%E5%89%8D%E7%9A%84-1%C3%971-%E5%8D%B7%E7%A7%AF"><span class="toc-number">5.3.2.</span> <span class="toc-text">不同分支前的 1×1 卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Anchor"><span class="toc-number">5.3.3.</span> <span class="toc-text">Anchor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A4%E6%96%AD%E5%89%8D%E6%99%AF%E5%92%8C%E8%83%8C%E6%99%AF"><span class="toc-number">5.3.4.</span> <span class="toc-text">判断前景和背景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%B9%E6%A1%86%E4%BF%AE%E6%AD%A3"><span class="toc-number">5.3.5.</span> <span class="toc-text">边框修正</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E5%92%8C%E5%AE%9A%E4%BD%8D"><span class="toc-number">5.4.</span> <span class="toc-text">分类和定位</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4"><span class="toc-number">5.4.1.</span> <span class="toc-text">具体步骤</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.5.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#RPN-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.5.1.</span> <span class="toc-text">RPN 损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1"><span class="toc-number">5.5.1.1.</span> <span class="toc-text">分类损失</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1"><span class="toc-number">5.5.1.2.</span> <span class="toc-text">回归损失</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Faster-R-CNN-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.5.2.</span> <span class="toc-text">Faster R-CNN 损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1-1"><span class="toc-number">5.5.2.1.</span> <span class="toc-text">分类损失</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1-1"><span class="toc-number">5.5.2.2.</span> <span class="toc-text">回归损失</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83"><span class="toc-number">5.6.</span> <span class="toc-text">网络训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#R-FCN"><span class="toc-number">6.</span> <span class="toc-text">R-FCN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-4"><span class="toc-number">6.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%9D%E6%83%B3-2"><span class="toc-number">6.2.</span> <span class="toc-text">思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Translation-invariance-variance"><span class="toc-number">6.3.</span> <span class="toc-text">Translation invariance&#x2F;variance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Position-sensitive-score-maps"><span class="toc-number">6.4.</span> <span class="toc-text">Position-sensitive score maps</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Position-sensitive-RoI-Pooling"><span class="toc-number">6.5.</span> <span class="toc-text">Position-sensitive RoI Pooling</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FPN"><span class="toc-number">7.</span> <span class="toc-text">FPN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-5"><span class="toc-number">7.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%9D%E8%B7%AF-1"><span class="toc-number">7.2.</span> <span class="toc-text">思路</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Image-Pyramid"><span class="toc-number">7.2.1.</span> <span class="toc-text">Image Pyramid</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Single-Feature-Map"><span class="toc-number">7.2.2.</span> <span class="toc-text">Single Feature Map</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Pyramidal-Feature-Hierarchy"><span class="toc-number">7.2.3.</span> <span class="toc-text">Pyramidal Feature Hierarchy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Feature-Pyramid-Network"><span class="toc-number">7.2.4.</span> <span class="toc-text">Feature Pyramid Network</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FPN-%E7%BB%93%E6%9E%84"><span class="toc-number">7.3.</span> <span class="toc-text">FPN 结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FPN-%E5%9C%A8-RPN-%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">7.4.</span> <span class="toc-text">FPN 在 RPN 中的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FPN-%E5%AF%B9-RPN-%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">7.5.</span> <span class="toc-text">FPN 对 RPN 的影响</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mask-R-CNN"><span class="toc-number">8.</span> <span class="toc-text">Mask R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-6"><span class="toc-number">8.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-4"><span class="toc-number">8.2.</span> <span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RoI-Pooling-%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">8.3.</span> <span class="toc-text">RoI Pooling 的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RoI-Align"><span class="toc-number">8.4.</span> <span class="toc-text">RoI Align</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%92%8C%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC"><span class="toc-number">8.5.</span> <span class="toc-text">线性插值和双线性插值</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC"><span class="toc-number">8.5.1.</span> <span class="toc-text">线性插值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC"><span class="toc-number">8.5.2.</span> <span class="toc-text">双线性插值</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mask-%E5%88%86%E6%94%AF"><span class="toc-number">8.6.</span> <span class="toc-text">Mask 分支</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classifier-%E5%88%86%E6%94%AF"><span class="toc-number">8.7.</span> <span class="toc-text">Classifier 分支</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mask-R-CNN-%E8%AE%AD%E7%BB%83%E5%92%8C%E9%A2%84%E6%B5%8B"><span class="toc-number">8.8.</span> <span class="toc-text">Mask R-CNN 训练和预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cascade-R-CNN"><span class="toc-number">9.</span> <span class="toc-text">Cascade R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE-7"><span class="toc-number">9.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF"><span class="toc-number">9.2.</span> <span class="toc-text">设计思路</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mismatch-%E9%97%AE%E9%A2%98"><span class="toc-number">9.3.</span> <span class="toc-text">Mismatch 问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cascade-%E7%BB%93%E6%9E%84"><span class="toc-number">9.4.</span> <span class="toc-text">Cascade 结构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%92%8C-Iterative-BBox-%E6%AF%94%E8%BE%83"><span class="toc-number">9.4.1.</span> <span class="toc-text">和 Iterative BBox 比较</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%92%8C-Integral-Loss-%E6%AF%94%E8%BE%83"><span class="toc-number">9.4.2.</span> <span class="toc-text">和 Integral Loss 比较</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">9.5.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ming</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/images/"><i class="fa-fw fas fa-images"></i><span> Images</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Two-Stage Object Detection</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-06-20T07:47:19.000Z" title="Created 2020-06-20 15:47:19">2020-06-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-08-27T03:25:02.065Z" title="Updated 2020-08-27 11:25:02">2020-08-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Object-Detection/">Object Detection</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">10.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>34min</span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="Two-Stage-方法简述"><a href="#Two-Stage-方法简述" class="headerlink" title="Two-Stage 方法简述"></a>Two-Stage 方法简述</h2><p>two-stage检测器包括两个步骤：</p>
<ol>
<li>使用 RPN 生成 anchor 并对 anchor 做筛选，过滤掉很多的负样本 proposals，生成稀疏的 proposals；</li>
<li>对得到的 proposals 进行分类和回归，对选择的 proposals，使用 roi pooling(roi align) 等操作，进一步的精细化，因此得到的框更加精准。（比如，一个 anchor 有可能只覆盖了一个目标的50%，但却作为完全的正样本，因此其预测肯定是有误差的。）</li>
</ol>
<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/rcnn.jpg"></p>
<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ol>
<li>使用 CNN 进行特征提取。</li>
<li>使用 bounding box regression 进行目标框的修正。</li>
</ol>
<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li>输入一张图片，通过指定算法 (Selective Search) 从图片中提取 2000 个类别独立的候选区域（可能目标区域）</li>
<li>对于每个候选区域利用卷积神经网络来获取一个特征向量，这里无视候选框的大小和形状，直接暴力 resize 后输入到 CNN 中进行特征提取。</li>
<li>对于每个区域相应的特征向量，利用 SVM 进行分类，这里的 SVM 是一个二分类器，每个类别都训练一个 SVM。</li>
<li>使用 NMS 去除一些的候选框。</li>
<li>使用一个回归器进行边框回归，调整目标包围框的大小，这里输入的是 CNN 提取的特征，输出为 x, y 方向上的平移和缩放。</li>
</ol>
<h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><ol>
<li>耗时的 Selective Search 算法</li>
<li>耗时的串行式 CNN 前向传播，对于每一个候选框都需要经过预训练的 CNN 提取特征</li>
<li>三个模块是分开训练的（CNN特征提取、SVM分类、检测框修正），不是端到端。</li>
</ol>
<h2 id="SPPNet"><a href="#SPPNet" class="headerlink" title="SPPNet"></a>SPPNet</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/spp.jpg"></p>
<h3 id="贡献-1"><a href="#贡献-1" class="headerlink" title="贡献"></a>贡献</h3><p>提出了 spatial pyramid pooling layer，解决了利用深度学习算法进行特征学习时输入的样本图片的尺寸必须时固定的问题。</p>
<h3 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h3><p>解决了利用深度学习算法进行特征学习时输入的样本图片的尺寸必须时固定的问题，这里的固定实际上就是指在网络训练过程中输入的图像的尺寸必须一样大。虽然可以采用 crop 以及 warp 的方法强制将图片转换到同一个尺寸，但是由此产生图片本身信息的形变和缺失在一定程度上限制了网络的性能。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/crop_warp_spp.jpg"><br>图中第一行对应的是一般情况下CNN网络的结构，对于一个给定的图像，需要先采取 crop 或者是 warp 操作来将图像转换为一个固定的大小，这期间就会导致原始图像视觉信息的缺失和扭曲（如图）。第二行是文中提出的网络结构，该结构可以输入任意尺寸的图像，在通过卷积操作后利用 spatial pytramid pooling 空间金字塔池化操作得到尺寸一致的特征向量，从而避免了由于输入图像尺寸不一致带来的全连接层参数不固定的情形。</p>
<h3 id="Spatial-Pyramid-Pooling"><a href="#Spatial-Pyramid-Pooling" class="headerlink" title="Spatial Pyramid Pooling"></a>Spatial Pyramid Pooling</h3><p>对经过 backbone 得到的特征图进行池化操作，利用不同 stride 和 size 来得到不同的池化层结果，构成最终的空间金字塔。对于 $a \times a$ 的特征图，如果想得到 $n \times n$ 的特征，那么池化层的 stride 和 kernel_size 分别为：<br>$$<br>size = \lceil \frac{a}{n} \rceil \<br>stride = \lfloor \frac{a}{n} \rfloor<br>$$</p>
<p>如图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/spp_example.png"><br>对于 size 为 $13 \times 13$ 的特征图，如果我们想得到一个 3 层的空间金字塔池化，即需要分别得到 $4 \times 4$、$2 \times 2$ 和 $1 \times 1$ 的特征，那么分别需要 kernel_size=$\lceil \frac{13}{4} \rceil = 4$，stride = $\lfloor \frac{13}{4} \rfloor = 3$ 的池化层用来得到 $4 \times 4$ 的特征，需要 kernel_size=$\lceil \frac{13}{2} \rceil = 8$，stride = $\lfloor \frac{13}{2} \rfloor = 7$ 的池化层用来得到 $2 \times 2$ 的特征，以及 kernel_size=$\lceil \frac{13}{1} \rceil = 13$，stride = $\lfloor \frac{13}{1} \rfloor = 13$ 的池化层用来得到 $1 \times 1$ 的特征。</p>
<h3 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h3><h3 id="不足-1"><a href="#不足-1" class="headerlink" title="不足"></a>不足</h3><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/fast_rcnn.jpg"></p>
<h3 id="贡献-2"><a href="#贡献-2" class="headerlink" title="贡献"></a>贡献</h3><ol>
<li>只对整个图像进行一次特征提取，避免 R-CNN 的上千次特征提取，提升速度。</li>
<li>提出 RoI Pooling 保证不同 size 的 proposals 的特征保持固定大小, 便于分类和回归的同时不会像直接暴力 resize 那样丢失信息</li>
</ol>
<h3 id="算法步骤-2"><a href="#算法步骤-2" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li>首先还是采用 selective search 提取 2000 个候选框 RoI</li>
<li>使用一个卷积神经网络对全图进行特征提取</li>
<li>使用一个 RoI Pooling Layer 在全图特征上摘取每一个 RoI 对应的特征</li>
<li>分别经过为21和84维的全连接层（并列的，前者是分类输出，后者是回归输出） Fast R-CNN 通过 CNN 直接获取整张图像的特征图，再使用 RoI Pooling Layer 在特征图上获取对应每个候选框的特征，避免了 R-CNN 中的对每个候选框串行进行卷积（耗时较长）。</li>
</ol>
<h3 id="RoI-Pooling"><a href="#RoI-Pooling" class="headerlink" title="RoI Pooling"></a>RoI Pooling</h3><h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><ol>
<li>把原图上所有的 RoIs 的坐标映射到特征图的对应位置，即为每个 RoI 提取特征。</li>
<li>把不同 size 的 RoI 进行处理，使其形成大小固定的特征图，便于后续接全连接层进行分类和回归。</li>
<li>传统的 crop 与 warp 会破坏图像原有的结构信息，导致结果不好，RoI Pooling 不存在这个问题。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/crop_and_warp.jpg"></li>
</ol>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><ol>
<li>将 RoIs 分为目标为 $H \times W$ 大小的分块</li>
<li>对每个分块做 Max Pooling</li>
<li>把所有结果组合起来便形成了 $H \times W$ 的特征图</li>
</ol>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_pooling.gif"><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_pooling.jpeg"></p>
<h3 id="不足-2"><a href="#不足-2" class="headerlink" title="不足"></a>不足</h3><ol>
<li>还是耗时的 Selective Search 算法</li>
<li>由于使用的是 Selective Search, 所以仍然无法实现真正的端到端</li>
</ol>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.jpg"><br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.png"></p>
<h3 id="贡献-3"><a href="#贡献-3" class="headerlink" title="贡献"></a>贡献</h3><ol>
<li>提出 RPN 来生成 proposals 而不是 Selective Search，极大提升了检测速度。</li>
<li>真正意义上实现了端到端的检测。</li>
</ol>
<h3 id="算法步骤-3"><a href="#算法步骤-3" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li>使用预训练好的 CNN 为全图进行特征提取，得到共享的特征图。</li>
<li>将得到的特征图送入 RPN，RPN 生成 proposals(RoIs)，并对每个 RoI 的边界框进行第一次修正。</li>
<li>RoI Pooling 对 RPN 得到的修正后的 RoIs 进行特征提取并固定最终特征维度。</li>
<li>使用全连接层对边界框进行分类和回归，这里回归就是对这些边界框进行第二次修正。</li>
</ol>
<h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/rpn.jpg"><br>可以看到 RPN 实际上有 2 条分支，上面的一条分支是分类分支，通过 softmax 来分类 anchors ，判断其是属于前景还是背景；下面的是回归分支，用于计算 anchors 的边框偏移量，从而获得更加精准的 proposals，最后的 proposal 层负责综合前景的边界框和偏移量来获取最终的 proposals 并同时剔除太小和超出边界的 proposals。</p>
<h4 id="共享卷积后的-3×3-卷积"><a href="#共享卷积后的-3×3-卷积" class="headerlink" title="共享卷积后的 3×3 卷积"></a>共享卷积后的 3×3 卷积</h4><p>在特征图进入2个分支之前还有一个 $3 \times 3$ 的卷积，这个卷积的作用是为了能更好的进行分类和边界框回归，因为预训练好的网络得到的特征图往往是有着较强的分类特征，使用 $3 \times 3$ 卷积对其进行二次变换能够将分类特征迁移到 分类和回归 这2个 task 所需要的分布上来。也有一种说法是进一步扩大感受野从而匹配图像中的目标的大小，可以试着去掉这个卷积或者把这个 $3 \times 3$ 的卷积只用于回归分支，分类分支直接用 $1 \times 1$ 卷积。</p>
<h4 id="不同分支前的-1×1-卷积"><a href="#不同分支前的-1×1-卷积" class="headerlink" title="不同分支前的 1×1 卷积"></a>不同分支前的 1×1 卷积</h4><ul>
<li>分类分支<br>分类分支的 $1 \times 1$ 卷积把特征图的通道数变为18, 这是由于一共设置了 9 个 anchor, 对每个 anchor 判断其为前景还是背景, 即预测每个 anchor 是否含有物体</li>
<li>回归分支<br>回归分支的 $1 \times 1$ 卷积把特征图的通道数变为36, 每个特征图上的点设置 9 个 anchor, 每个 anchor 的坐标是一个 4 维的向量 (x, y, w, h), 所以是 $4 \times 9 = 36$</li>
</ul>
<h4 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a>Anchor</h4><p>Anchor 可以定义为 RPN 为特征图上的每一个像素点生成 9 种预先设置好长宽比的目标框。包括 3 个 anchor scale，每个 scale 包含 3 种长宽比 (1:1, 1:2, 2:1)，以下图为例详细说明：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/generate_anchor.jpg"></p>
<p>如上图所示，例如原图尺寸为  $800 \times 600$，backbone 为 ResNet，那么经过 backbone 后的尺寸为  $\frac{800}{16} \times \frac{600}{16} = 50 \times 38$，这样的话就有 $50 \times 38$ 个 grid，每个 grid 都生成 9 个 anchor，总的 anchor 数为 $50 \times 38 \times 9 = 17100$。</p>
<p>其实 RPN 最终就是在原图尺度上，设置了密密麻麻的候选 anchor。进而去判断 anchor 到底是前景还是背景，意思就是判断这个 anchor 到底有没有覆盖目标，以及为属于前景的 anchor 进行第一次坐标修正。</p>
<h4 id="判断前景和背景"><a href="#判断前景和背景" class="headerlink" title="判断前景和背景"></a>判断前景和背景</h4><p>直接使用 Softmax Loss 进行训练, 在训练的时候排除超过图像边界的 anchor.</p>
<h4 id="边框修正"><a href="#边框修正" class="headerlink" title="边框修正"></a>边框修正</h4><p>给定 anchor 的坐标: $(A_{x}, A_{y}, A_{w}, A_{h})$ 和 ground truth 的坐标 $(G_{x}, G_{y}, G_{w}, G_{h})$，我们的目的是找到一种变换使得<br>$$<br>F\left(A_{x}, A_{y}, A_{w}, A_{h}\right)=\left(G_{x}^{\prime}, G_{y}^{\prime}, G_{w}^{\prime}, G_{h}^{\prime}\right)<br>$$</p>
<p>其中<br>$$<br>(G_{x}, G_{y}, G_{w}, G_{h}) \approx (G_{x}^{\prime}, G_{y}^{\prime}, G_{w}^{\prime}, G_{h}^{\prime})<br>$$</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/bbox_regression.jpg"></p>
<p>这里的变换可以通过平移和缩放来实现：</p>
<ul>
<li><p>平移(anchor 中心点的移动)<br>$$<br>\begin{aligned}<br>&amp;G_{x}^{\prime}=A_{w} \cdot t_x+A_{x}\<br>&amp;G_{y}^{\prime}=A_{h} \cdot t_y+A_{y}<br>\end{aligned}<br>$$</p>
</li>
<li><p>缩放(anchor 长和宽的缩放)<br>$$<br>\begin{aligned}<br>&amp;G_{w}^{\prime}=A_{w} \cdot \exp \left(t_w\right)\<br>&amp;G_{h}^{\prime}=A_{h} \cdot \exp \left(t_h\right)<br>\end{aligned}<br>$$</p>
</li>
</ul>
<p>由上述公式可以看出我们需要学习 4 个修正参数, 分别是:<br>$$<br>t_x, t_y, t_w, t_h<br>$$</p>
<p>这 4 个参数的 ground truth 值可以通过以下公式算出：</p>
<p>$$\begin{array}{l}<br>t_{x}=\left(G_{x}-A_{x}\right) / A_{w} \<br>t_{y}=\left(G_{y}-A_{y}\right) / A_{h} \<br>t_{w}=\log \left(G_{w} / A_{w}\right) \<br>t_{h}=\log \left(G_{h} / A_{h}\right)<br>\end{array}$$</p>
<p>这样在得到每一个 anchor 的修正参数后就能计算出精准的 anchor, 然后根据 score 值对 anchor 进行排序, 先剔除一些高或者宽小的 anchor (在分类分支判断前景和背景时就剔除了超过边界框的 anchor), 再进行 NMS 处理, 取先 Top-N 的 anchors 作为 proposals 输出到 RoI Pooling 层。</p>
<h3 id="分类和定位"><a href="#分类和定位" class="headerlink" title="分类和定位"></a>分类和定位</h3><p>这里和 Fast R-CNN 原理一样，在 RoI Pooling Layer 之后，就是 aster R-CNN 的分类器和 RoI 边框修正训练。分类器主要是分这个提取的 RoI 具体是哪个类别(人，车，马等)，一共 C+1 类(包含一类背景)。</p>
<p>RoI 边框修正和 RPN 中的 anchor 边框修正原理一样，同样也是 Smooth L1 Loss，值得注意的是，RoI 边框修正也是对于非背景的 RoI 进行修正，对于类别标签为背景的 RoI，则不进行 RoI 边框修正的参数训练。</p>
<h4 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h4><ol>
<li>首先通过 RPN 生成约 20000 个 anchor (40×60×9)</li>
<li>对20000个 anchor 进行第一次边框修正，得到修订边框后的 proposal</li>
<li>对超过图像边界的 proposal 的边进行 clip，使得该 proposal 不超过图像范围</li>
<li>忽略掉长或者宽太小的 proposal</li>
<li>将所有 proposal 按照前景分数从高到低排序，选取 Top-N 个 proposal</li>
<li>使用阈值为 0.7 的 NMS 算法排除掉重叠的 proposal</li>
<li>对于上一步剩下的 proposal，选取 Top-N 个 proposal 进行分类和第二次边框修正</li>
</ol>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>Faster R-CNN 的 loss 分两大块，第一大块是训练 RPN 的 loss (包含一个 Softmax Loss 和 Smooth L1 Loss)，第二大块是训练 Faster R-CNN 中分类器的 loss (包含一个 Softmax Loss 和 Smooth L1 Loss)。</p>
<ol>
<li>RPN 分类损失：anchor是否为前景（二分类）</li>
<li>RPN位置回归损失：anchor位置微调</li>
<li>RoI 分类损失：RoI所属类别（21分类，多了一个类作为背景）</li>
<li>RoI位置回归损失：继续对RoI位置微调</li>
</ol>
<p>四个损失相加作为最后的损失，反向传播，更新参数。</p>
<h4 id="RPN-损失函数"><a href="#RPN-损失函数" class="headerlink" title="RPN 损失函数"></a>RPN 损失函数</h4><p>RPN 的损失函数为分类损失和回归损失之和：</p>
<p>$$<br>\mathrm{L}\left(\left{p_{i}\right},\left{t_{i}\right}\right)=\frac{1}{N_{\mathrm{cls}}} \sum_{i} \mathrm{L}<em>{\mathrm{cls}}\left(p</em>{i}, p_{i}^{<em>}\right)+\lambda \frac{1}{N_{\mathrm{reg}}} \sum_{i} p_{i}^{</em>} \mathrm{L}<em>{\mathrm{reg}}\left(t</em>{i}, t_{i}^{*}\right)<br>$$</p>
<p>注意这里对每一个 anchor 计算完 $\mathrm{L}_{\mathrm{reg}}$ 后还要乘以一个 $p_i^*$，也就是说<strong>只有前景才计算回归损失，背景的 anchor 是不计算回归损失的。</strong></p>
<h5 id="分类损失"><a href="#分类损失" class="headerlink" title="分类损失"></a>分类损失</h5><p>分类损失为交叉熵损失即Softmax Loss：<br>$$<br>L_{c l s}\left(p_{i}, p_{i}^{<em>}\right)=-\log \left[p_{i}^{</em>} p_{i}+\left(1-p_{i}^{*}\right)\left(1-p_{i}\right)\right]<br>$$</p>
<p>其中 $p_i$ 为 anchor 预测为目标的概率，$p_i^*$ 为 Ground Truth 的标签, 即<br>$$<br>p_{i}^{*}=\left{\begin{array}{cl}0 &amp; \text { negative label } \ 1 &amp; \text { positive label }\end{array}\right.<br>$$</p>
<h5 id="回归损失"><a href="#回归损失" class="headerlink" title="回归损失"></a>回归损失</h5><p>回归损失为 Smooth L1 损失：<br>$$\begin{aligned}<br>&amp;\mathrm{L}<em>{\mathrm{reg}}\left(t</em>{i}, t_{i}^{<em>}\right)=\sum_{i \in{x, y, w, h}} \operatorname{smooth}<em>{\mathrm{L} 1}\left(t</em>{i}-t_{i}^{</em>}\right)\<br>&amp;\operatorname{soomth}_{L1}(x)=\left{\begin{array}{ll}<br>0.5 x^{2} &amp; \text { if }|x|&lt;1 \<br>|x|-0.5 &amp; \text { otherwise }<br>\end{array}\right.<br>\end{aligned}$$</p>
<h4 id="Faster-R-CNN-损失函数"><a href="#Faster-R-CNN-损失函数" class="headerlink" title="Faster R-CNN 损失函数"></a>Faster R-CNN 损失函数</h4><h5 id="分类损失-1"><a href="#分类损失-1" class="headerlink" title="分类损失"></a>分类损失</h5><p>RPN的分类损失时二分类的交叉熵损失，而 Fast R-CNN 是多分类的交叉熵损失</p>
<h5 id="回归损失-1"><a href="#回归损失-1" class="headerlink" title="回归损失"></a>回归损失</h5><p>这里和 RPN 中的回归损失一致。</p>
<h3 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h3><p>由于在 RPN 网络中特征提取的过成中也用到了 ZF 和VGG 网络结构，针对这样一个整体的识别任务，文中指出其实这部分网络在提取 proposal 和 目标检测识别两阶段任务中其实时可以共享的，所以文中采取四步交替迭代的策略对整个网络进行训练：</p>
<ol>
<li>在通过image-Net 初始化共享网络结构参数，训练RPN网络。</li>
<li>用训练好的RPN网络生成 proposal，训练 Fast-R-CNN 部分。</li>
<li>在刚才Fast-R-CNN 训练过程中可共享的ZF 或者VGG-16 网络部分训练模型基础上，也就是固定共享网络的参数，对RPN网络没有被共享的层进行fine-tuning。</li>
<li>同样固定共享网络，对 Fast-R-CNN 非共享部分进行 fine-tuning. 这个过程可以迭代执行，但是作者指出，这个过程迭代多次并没有效果，所以只执行一次。</li>
</ol>
<h2 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a>R-FCN</h2><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/rfcn_overview.png"></p>
<h3 id="贡献-4"><a href="#贡献-4" class="headerlink" title="贡献"></a>贡献</h3><p>对 RoI Pooling 进行了改进，利用 Position-sensitive score maps 和 Position-sensitive RoI Pooling 使得目标检测网络应该在保证判别性的基础上具有一定的位置敏感性。并且让网络丢弃全连接层使用全卷积层，不但效果更好而且能适应不同尺度的图片。</p>
<h3 id="思想-2"><a href="#思想-2" class="headerlink" title="思想"></a>思想</h3><p>Fast R-CNN 发明的 RoI Pooling 中间是由全连接层存在，从而将前面的 RoI Pooling 后的 feature map 映射成两个部分（对象分类，坐标回归）；而越来越多的 backbone 网络，如 GoogleNet，ResNet 等全卷积网络证明了不要全连接层，效果不但更好，而且能适应不同尺度的图片，所以为了消灭 Fast R-CNN 的 RoI Pooling 中的全连接层，R-FCN 被提出来了。</p>
<h3 id="Translation-invariance-variance"><a href="#Translation-invariance-variance" class="headerlink" title="Translation invariance/variance"></a>Translation invariance/variance</h3><p>这两个次对应的中文含义可以理解为 “位置不敏感性” 以及 “位置敏感性”，文中意在指出分类任务和目标检测任务的不同，比如在分类问题中，我们并不需要模型对物体的位置敏感，如果图像中存在这个物体，无论这个物体如何移动，我们都可以将该物体分对。 但是在目标检测中，我们希望我们的模型对目标位置是非常敏感的，这样才可以去准确的定位目标。所以从模型上讲分类模型和目标检测模型不应该是完全相同的，目标检测网络应该在保证判别性的基础上具有一定的位置敏感性。也就是说 backbone CNN 模型本身是对图像分类设计的，具有图像移动不敏感性；而对象检测领域却是图像移动敏感的，所以二者之间产生了矛盾，解决的办法就是对 RoI Pooling 进行了改进。</p>
<h3 id="Position-sensitive-score-maps"><a href="#Position-sensitive-score-maps" class="headerlink" title="Position-sensitive score maps"></a>Position-sensitive score maps</h3><p>文中网络的改进基于 Faster R-CNN 模型，基础网络为 ResNet-101，网络的主要改进集中于对目标检测部分中全连接层的简化，称为 Region-specific Fully Convolutional Network (R-FCN)，RPN 网络保持不变。网络结构如下图所示：</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/rfcn.jpg"></p>
<p>这里作者设计一个位置敏感的 RoI Pooling：将 Faster R-CNN 中的 RoI 划分成 $k \times k$ 大小，即图片中本来获取的 RoI 区域，将其分成 $k \times k$ 个 区域（这里 k=3，即分成 9 个部分）。假设该数据集一共由 C 类，那么再加个背景类，一共是 C+1 类。我们希望对每个类别都有各自的位置感应，所以需要设计上图中的 <code>position-sensitive score maps</code> 。就是几个大色块并列的部分：每个色块表示对对象的特定位置进行敏感，而且每个色块大小中有 C+1 个 feature map，所以该区域一共有 $k^{2} \cdot (C+1)$ 个map，其中每个 map 的大小和之前那个 backbone 得到的 feature map 大小一致。</p>
<p>Position-sensitive score maps 的直观含义就是 $C+1$ 个类在 $k^2$ 个位置上的概率。</p>
<h3 id="Position-sensitive-RoI-Pooling"><a href="#Position-sensitive-RoI-Pooling" class="headerlink" title="Position-sensitive RoI Pooling"></a>Position-sensitive RoI Pooling</h3><p>这部分详细解释了如何利用上图中的 <code>Position-sensitive score maps</code> 得到最右边那个 $k \cdot k \cdot (c+1)$ 的特征图。</p>
<p>这里每个小区域都在对应的 $C+1$ 个维度上做 pooling 操作，比如 RoI 左上角的区域就在前 $C+1$ 个维度上 pooling，左中位置的区域就在 $C+2$ 到 $2(C+1)$ 间的维度上做 pooling，以此类推。pooling 后输出的是 $C+1$ 维度的 $k \times k$ 数据，每个维度上的 $k \times k$ 数据相加就得到了一个 $C+1$ 维的向量，这个向量就表示了当前的这个 proposal 属于某一个类的分数，后面接上 softmax 就能得到当前 proposal 属于每一个类的概率。</p>
<p>我们可以发现，在生成 positive-sensitive score map 后的计算都是固定的，没有网络参数的训练过程，这使得目标检测的速度进一步提高。回归部分类似于这个部分，只是在生成 positive-sensitive score maps 时我们需要得到总共 $4k^2(C+1)$ 分别对应于四个回归偏移量。表示９个小区域的 $[dx,dy,dw,dh]$ 4个偏移坐标。</p>
<h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h2><h3 id="贡献-5"><a href="#贡献-5" class="headerlink" title="贡献"></a>贡献</h3><p>提出了特征金字塔结构，在不增加计算量的同时解决了多尺度的问题。</p>
<h3 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h3><h4 id="Image-Pyramid"><a href="#Image-Pyramid" class="headerlink" title="Image Pyramid"></a>Image Pyramid</h4><p>传统的数字图像处理技术中为了解决多尺度问题会使用图像金字塔，就是将图片 resize 到不同的大小，然后对不同 size 的图像提取特征再进行预测，虽然这种方法可以在一定程度上解决多尺度的问题，但是带来的计算量也是非常巨大。如图 a 所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/image_pyramid.jpg"></p>
<h4 id="Single-Feature-Map"><a href="#Single-Feature-Map" class="headerlink" title="Single Feature Map"></a>Single Feature Map</h4><p>很多经典算法是使用单个 feature map 进行检测，这种结构在 2017 年的时候是很多人在使用的结构，比如YOLOv1、YOLOv2、Faster R-CNN 中使用的就是这种架构。直接使用这种架构导致预测层的特征尺度比较单一，对小目标检测效果比较差。ps: YOLOv2 中使用了 multi-scale training 的方式一定程度上缓解了尺度单一的问题，能够让模型适应更多输入尺度。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/single_feature_map.jpg"></p>
<h4 id="Pyramidal-Feature-Hierarchy"><a href="#Pyramidal-Feature-Hierarchy" class="headerlink" title="Pyramidal Feature Hierarchy"></a>Pyramidal Feature Hierarchy</h4><p>理所应当地，有方法在不同大小的 feature map 上分别进行预测，具有了多尺度预测的能力，但是特征与特征之间没有融合，遵从这种架构的经典的目标检测架构就是 SSD, SSD 用了非常多的尺度来进行检测。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/pyramid_feature_hierarchy.jpg"></p>
<h4 id="Feature-Pyramid-Network"><a href="#Feature-Pyramid-Network" class="headerlink" title="Feature Pyramid Network"></a>Feature Pyramid Network</h4><p>然后就是非常经典的FPN架构，FPN可以非常方便地应用到两阶段网络如 Faster R-CNN 等或者一阶段网络 YOLO、SSD 等。FPN 通过构造一种独特的特征金字塔来避免图像金字塔中计算量过高的问题，同时能够较好地处理目标检测中的多尺度变化问题，效果能够达到当时的 STOA。SSD 的一个改进版 DSSD 就是使用了 FPN，取得了比 SSD 更好的效果。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/feature_pyramid.jpg"></p>
<h3 id="FPN-结构"><a href="#FPN-结构" class="headerlink" title="FPN 结构"></a>FPN 结构</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/fpn.jpg"><br>整个结构可以分为左边的 Down-Top 和右边的 Top-Down，其中 Down-Top 就是 backbone 提取特征的过程，随着深度的增大，分辨率在减小，通道数在增加，以常用的 ResNet 为例，选取 conv2、conv3、conv4、conv5 层的最后一个残差 block 层特征作为FPN的特征，因为每个阶段的最深层应该具有最强的特征。，记为 {C2、C3、C4、C5}，也即是FPN网络的 4 个级别。这几个特征层相对于原图的步长分别为 4、8、16、32，就是说这几个层得到的特征图的分辨率分别是原图的 $\frac{1}{4}$、$\frac{1}{8}$、$\frac{1}{16}$ 和 $\frac{1}{32}$，没有把 conv1 考虑在内的一个原因是内存占用。</p>
<p>Top-Down 部分可以看成是特征融合，融合了底层低语义高分辨率的特征以及高层高语义低分辨率的特征，以上图为例的具体过程为：</p>
<p>C5 层先经过 1 x 1 卷积，改变特征图的通道数(文章中设置 d=256，与 Faster R-CNN 中 RPN 层的维数相同便于分类与回归)。M5 通过上采样，再加上(特征图中每一个相同位置元素直接相加) C4 经过 1 x 1 卷积后的特征图，得到 M4，即<strong>这里特征融合的方式为 element-wise add</strong>。这个过程再做两次，分别得到 M3，M2。M 层特征图再经过 3 x 3 卷积(减轻最近邻近插值带来的混叠影响，周围的数都相同)，得到最终的 P2，P3，P4，P5 层特征。和传统的图像金字塔方式一样，所有 M 层的通道数都设计成一样的，文中都用 d=256。</p>
<h3 id="FPN-在-RPN-中的应用"><a href="#FPN-在-RPN-中的应用" class="headerlink" title="FPN 在 RPN 中的应用"></a>FPN 在 RPN 中的应用</h3><p>Faster R-CNN 中的 RPN 是通过最后一层的特征来做的。最后一层的特征经过 3 x 3 卷积，得到 256 个 channel 的特征图，再分别经过两个 1 x 1 卷积得到类别得分和边框回归结果。这里将经过 3 x 3 卷积得到的特征图之后的 RPN 子网络称之为网络头部（network head），在 Faster R-CNN 中有 2 个头部(分支)，一个用于判断每个 anchor 是否含有目标，另一个用于对 anchor 进行坐标修正，对于特征图上的每一个点，作者用 anchor 的方式预设了 9 个框。这些框本身包含不同的尺度和不同的长宽比例。</p>
<p><strong>FPN 针对 RPN 的改进是将网络头部应用到每一个 P 层，可以理解为在 Faster-R-CNN 中只对 P5 添加了 head (分类分支和回归分支)，而这里是对 P2-P6 都添加了相同的 head(因为参数是共享的)。由于每个 P 层相对于原始图片具有不同的尺度信息，因此作者将原始 RPN 中的尺度信息分离，让每个 P 层只处理单一的尺度信息</strong>。具体的，对 {32 x 32、64 x 64、128 x 128、256 x 256、512 x 512} 这五种尺度的 anchor，分别对应到 {P2、P3、P4、P5、P6} 这五个特征层上。每个特征层都处理 1:1、1:2、2:1 三种长宽比例的候选框。P6 是专门为了 RPN 网络而设计的，用来处理 512 x 512 大小的候选框。它由 P5 经过下采样得到。如下图所示：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/fpn_p6.jpg"></p>
<p>P2-P5 将来用于预测物体的 bbox，box-regression，mask 的，而 P2-P6 是用于训练 RPN 的，即 P6 只用于 RPN 网络中。</p>
<p><strong>上述 5 个网络头部的参数是共享的</strong>，作者通过实验发现，网络头部参数共享和不共享两种设置得到的结果几乎没有差别。这说明不同层级之间的特征有相似的语义层次。这和特征金字塔网络的原理一致。</p>
<h3 id="FPN-对-RPN-的影响"><a href="#FPN-对-RPN-的影响" class="headerlink" title="FPN 对 RPN 的影响"></a>FPN 对 RPN 的影响</h3><p>论文还做了 6 个实验来测试 FPN 对 RPN 的影响：<br>(a): 基于 conv4 的 RPN，原始的 RPN<br>(b): 基于 conv5 的 RPN，原始的 RPN<br>(c): 完整FPN<br>(d): 只用了自底向上的多层特征，没有自顶向下的特征。<br>(e): 用了自顶向下的特征，但不用侧向连接。<br>(f): 用了自顶向下的特征，也用了横向特征融合，但只用最后的 P2 做预测。（完整的预测是使用每一个 level 的特征做预测，即 P2-P6 均做预测）</p>
<p>得到的结果如下：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/fpn_results_on_rpn.jpg"><br>由结果可知，自顶向下的特征、横向连接、尺度分离、多个层次的预测是提升 FPN 性能的关键</p>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><p>Mask R-CNN 可以分解为如下的3个模块：Faster-R-CNN、RoI Align 和 Mask。算法框架如下：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/mask_rcnn_overview.jpg"></p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/mask_rcnn.jpg"></p>
<h3 id="贡献-6"><a href="#贡献-6" class="headerlink" title="贡献"></a>贡献</h3><ol>
<li>提出了 RoI Align，解决了 RoI Pooling 中量化操作导致结果出现偏差的问题。</li>
<li>可以添加不同的分支完成不同的任务，模型非常的灵活。</li>
</ol>
<h3 id="算法步骤-4"><a href="#算法步骤-4" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li>将图像进行预处理之后输入到预训练好的神经网络中，获得对应的 feature map</li>
<li>为这个 feature map 上的每一个点设置若干个 anchor，得到多个候选的 RoI</li>
<li>将 RoIs 送入 RPN 中进行分类(前景/背景)和 bounding box 回归，过滤掉一部分 RoIs</li>
<li>对剩下的 RoIs 进行 RoI Align (即先将原图和 feature map 的 pixel 对应起来，然后将 feature map 和固定的 feature 对应起来)</li>
<li>对这些 RoI 进行分类（N类别分类）、BB回归和 Mask 生成（在每一个 ROI 里面进行 FCN 操作）。</li>
</ol>
<h3 id="RoI-Pooling-的问题"><a href="#RoI-Pooling-的问题" class="headerlink" title="RoI Pooling 的问题"></a>RoI Pooling 的问题</h3><p>RoI Pooling 在整个模型过程中使用了 2 次量化(取整)操作，这 2 次量化(取整)操作引入的误差会导致图像中的响度和 feature map 中的像素存在偏差，即将 feature map 上的 RoI 对应到原图上时会出现很大偏差。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_pooling_defect.jpg"></p>
<p>下面举例来具体说明：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_pooling_example.png"><br>如图所示，为了得到大小固定($7 \times 7$)的特征图，RoI Pooling 需要做 2 次量化(取整)操作：</p>
<ol>
<li>图像坐标 $\rightarrow$ feature map 坐标</li>
<li>feature map 坐标 $\rightarrow$ RoI feature 坐标</li>
</ol>
<p>以上图为例，输入的图像为 $800 \times 800$，其中狗的 Bounding Box 大小为 $665 \times 665$，在经过 VGG 的 backbone 后得到对应的 feature map 的大小为 $\frac{800}{32} \times \frac{800}{32} = 25 \times 25$，是个整数。但对于狗的 Bounding Box 而言，在 feature map 的 size 为 $\frac{665}{32} \times \frac{665}{32} = 20.78 \times 20.78$，是个浮点数，由于像素值没有小数所以需要进行量化(取整)操作，将结果变为 $20 \times 20$，这里就引入了第一次量化误差。</p>
<p>由于每个 RoI 的大小是不一致的，但后面的网络需要有固定的输入，所以需要把不同大小的 RoI 转换为固定的 RoI feature，以上图为例，为了把 $20 \times 20$ 的 RoI 映射为 $7 \times 7$ 的 RoI feature，实际上需要在 $20 \times 20$ 的 RoI 上每隔 $\frac{20}{7}$ 就进行一次 pooling，但 $\frac{20}{7}=2.86$ 同样是个浮点数，所以需要进行第二次量化(取整)操作，即上图中所示的每隔 $\frac{20}{7}=2$ 就进行一次 pooling。</p>
<p>在 RoI Pooling 中这 2 次量化(取整)操作大大影响了检测算法的性能，所以作者提出了 RoI Align 来解决这个问题。</p>
<h3 id="RoI-Align"><a href="#RoI-Align" class="headerlink" title="RoI Align"></a>RoI Align</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_align_example.jpg"></p>
<p>上面说到量化(取整)操作会引入误差从而影响检测效果，所以 RoI Align 中没有进行取整操作，比如 $\frac{665}{32} = 20.78$，就使用 20.78 而不是取整后使用 20，比如 $\frac{20.78}{7} = 2.97$，同样直接使用 2.97 而不是 2。对于这些浮点数，使用<code>双线性插值</code>来解决。</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/roi_align.jpg"></p>
<p>蓝色虚线(上图中的 feat.map )就是经过 backbone 得到的特征图，黑色实线表示特征图上的一个 RoI，如果最终需要的固定大小的 RoI feature 是 $2 \times 2$ 大小的，那么需要把 RoI 分成 $2 \times 2$ 个小区域，对每个区域进行 max pooling 或者 average pooling，最终得到 $2 \times 2$ 大小的 RoI feature。</p>
<p>怎么对每个小区域进行 max pooling 或者 average pooling 呢? 这里需要对每个小区域都进行采样，假设采样点为 4 的话，如上图所示，每个小区域就有 4 个蓝色的采样点，每个采样点的像素值是通过<code>双线性插值</code>得到的，每个小区域进行 max pooling 或者 average pooling 实际上就是对这 4 个采样点的像素值求最大值或者平均值。作者在文中说明采样点的个数和位置都不会对结果有比较大的影响，原因在于 RoI Align 本质上解决了 2 次量化取整操作所带来的像素区域不匹配问题，在目标检测任务中，小目标更容易受到这种问题的影响。</p>
<h3 id="线性插值和双线性插值"><a href="#线性插值和双线性插值" class="headerlink" title="线性插值和双线性插值"></a>线性插值和双线性插值</h3><h4 id="线性插值"><a href="#线性插值" class="headerlink" title="线性插值"></a>线性插值</h4><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/linear_interpolation.jpg"></p>
<p>这个很直观了，[x, y] 点的值可以通过 [x1, y1] 和 [x2, y2] 这 2 个点的值计算得到。</p>
<h4 id="双线性插值"><a href="#双线性插值" class="headerlink" title="双线性插值"></a>双线性插值</h4><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/bilinear_interpolation.png"></p>
<p>和线性插值同理，$R_2$ 的值可以通过 $Q_{12}$ 和 $Q_{22}$ 得到，$R_1$ 的值可以通过 $Q_{11}$ 和 $Q_{21}$ 得到，最终 $P$ 点的值就能由 $R_2$ 和 $R_1$ 得到。</p>
<h3 id="Mask-分支"><a href="#Mask-分支" class="headerlink" title="Mask 分支"></a>Mask 分支</h3><p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/mask_rcnn_mask.jpg"><br>可以看到作者在原来 Faster R-CNN 的基础上加了一个 mask 分支用于分割任务，作者没有采用 FCN 式的SoftmaxLoss，反而是输出了 K 个 Mask 预测图(为每一个类都输出一张)，并采用 average binary cross-entropy loss 训练，当然在训练 Mask branch 的时候，输出的 K 个特征图中，也只是对应 ground truth 类别的那一个特征图对 Mask Loss 有贡献。</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/mask_rcnn_mask_branch.jpg"></p>
<p>作者实验了 2 个 mask 分支，对于每一个 RoI 的 mask 都有 80 类，因为 coco 上的数据集是 80 个类别，并且这样做的话是为了减弱类别间的竞争，从而得到更加好的结果。</p>
<h3 id="Classifier-分支"><a href="#Classifier-分支" class="headerlink" title="Classifier 分支"></a>Classifier 分支</h3><p>这里实际上由 2 个分支组成，就是 Faster-R-CNN 中的分类分支和回归分支。<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/mask_rcnn_classifier.jpg"></p>
<h3 id="Mask-R-CNN-训练和预测"><a href="#Mask-R-CNN-训练和预测" class="headerlink" title="Mask R-CNN 训练和预测"></a>Mask R-CNN 训练和预测</h3><p>训练和预测是分开的，不是套用同一个流程。在训练的时候，classifier 和 mask 都是同时进行的；在预测的时候，先得到 classifier 的结果，然后再把此结果传入到 mask 预测中得到 mask，有一定的先后顺序。</p>
<h2 id="Cascade-R-CNN"><a href="#Cascade-R-CNN" class="headerlink" title="Cascade R-CNN"></a>Cascade R-CNN</h2><h3 id="贡献-7"><a href="#贡献-7" class="headerlink" title="贡献"></a>贡献</h3><p>提出了 cascade 结构，解决了单纯提高 IoU 阈值反而导致检测精度下降的问题，通过设置不同 IoU 阈值的 Head 来逐渐提高 proposals 和 GT 的 IoU，从而逐步提升 proposals 的质量，最终解决 Training 过程中的过拟合问题(负例过多正例过少)以及 Inference 阶段的 proposals 存在的 mismatch 问题。</p>
<p>在 Training 阶段通过设置级联的 head 能得到更多的正例，一定程度上避免了过拟合(其实 Focal Loss 也能解决这个问题)，在 Inference 阶段每个 proposal 通过 multi-stage 的 head，其实也是得到了更多正例(有更多的 proposals 和 GT 的 IoU 更大)，这样就保证了 training 和 inference 阶段的 proposals 分布大致相同，解决 mismatch 问题。</p>
<h3 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h3><p>在目标检测中，IoU 阈值用于定义正例和反例，一个使用低阈值的检测器通常会产生不太好的结果(误检过多)，但是增加 IoU 阈值也会使得检测精度下降，作者认为主要有 2 个原因：① 在训练过程中由于增大 IoU 阈值导致的正例数量呈指数形式减少，会导致对负例过拟合。② 推理阶段的 proposals 存在 mismatch 的问题(分布不一致)。</p>
<p>下图给出了随着 IoU 阈值的增大导致的检测结果改变以及回归精度和检测精度的变化曲线：<br><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/results_with_diff_iou.png"></p>
<ul>
<li>Figure1(a) 展示了当 IoU 阈值等于 0.5 时的检测结果，可以看到结果图中存在较多误检，因为的 0.5 阈值会使得正样本中有较多的背景，这是产生误检的主要原因。</li>
<li>Figure1(b) 展示了当 IoU 阈值等于 0.7 时的检测结果，可以看到误检变少了，但是并不一定误检变少了结果就一定更好，因为 IoU 阈值更大导致正样本越少，那么过拟合的风险就越大。</li>
<li>Figure1(c) 展示了模型定位表现，其中横坐标表示 RPN 得到的 proposals 和 GT 框的 IoU 值，纵坐标表示经过 bbox reg 之后的 proposals 和 GT 框的 IoU 值。然后，红，绿，蓝三条曲线分别代表训练检测模型时用的正负样本标签的阈值分别是0.5，0.6，0.7。可以<strong>从图中看到当 proposal 和 GT 的 IoU 比较小的时候，更小 IoU 阈值能得到更好的结果，当 proposal 和 GT 的 IoU 比较大的时候，更大的 IoU 阈值能得到更好的结果，也就是说单一的 IoU 阈值训练的结果非常有限，不能对所有的 proposals 都有很好的效果。所以后面的 cascade 结构会把 IoU 阈值设置的越来越大，因为每经过一次级联结构，proposal 和 GT 的 IoU 也会越大，所以需要更大的 IoU 阈值。</strong></li>
<li>Figure1(d) 横轴表示在 Training 阶段判定一个 box 是否为 TP 的 IoU 阈值，纵轴为 Inference 阶段的 mAP，可以看出当 IoU 阈值从 0.5 上升到 0.7 的时候效果下降比较明显，原因是 IoU 阈值过高导致正样本数量更少，这样样本会更加不平衡并且容易导致过拟合。</li>
</ul>
<h3 id="Mismatch-问题"><a href="#Mismatch-问题" class="headerlink" title="Mismatch 问题"></a>Mismatch 问题</h3><p>这里详细说下第二点，即训练阶段和推理阶段的 proposals 存在不匹配的问题，先看下 Faster R-CNN 的结构。</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn_two_stage.jpg"></p>
<p>如上图所示，整个网络其实可以分为 2 个阶段，训练阶段和推理阶段。</p>
<ol>
<li><p>Training 阶段<br>RPN 网络提出了 2000 左右的 proposals，这些 proposals 被送入到 Faster R-CNN 结构中，在 Faster R-CNN 结构中，首先计算每个 proposal 和 gt 之间的 IoU，通过人为的设定一个 IoU 阈值（通常为0.5），把这些 Proposals 分为正样本（前景）和负样本（背景），并对这些正负样本采样，使得他们之间的比例尽量满足（1:3，二者总数量通常为128），之后这些 proposals（128个）被送入到 Roi Pooling，最后进行类别分类和 box 回归。</p>
</li>
<li><p>Inference 阶段<br>RPN 网络提出了 300 左右的 proposals，这些 proposals 被送入到 Faster R-CNN 结构中，<strong>和 training 阶段不同的是，inference 阶段没有办法对这些 proposals 采样（inference 阶段肯定不知道 GT 的，也就没法计算 IoU）</strong>，所以他们直接进入 Roi Pooling，之后进行类别分类和 bbox 回归。</p>
</li>
</ol>
<p><strong>可以明显的看到 training 阶段和 inference 阶段，bbox 回归器的输入分布是不一样的，这就是论文中提到 mismatch 问题</strong>，training 阶段的输入 proposals 质量更高(被采样过，IoU &gt; threshold )，inference 阶段的输入 proposals 质量相对较差（没有被采样过，可能包括很多 IoU &lt; threshold 的），这个问题是固有存在的，通常 threshold 取 0.5 时，mismatch 问题还不会很严重。</p>
<h3 id="Cascade-结构"><a href="#Cascade-结构" class="headerlink" title="Cascade 结构"></a>Cascade 结构</h3><p>前面已经说明了<code>单一的 IoU 阈值训练的结果非常有限，不能对所有的 proposals 都有很好的效果</code>，所以作者提出了 multi-stage 的结构，每个 stage 都有不同的 IoU 阈值。</p>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/cascade_structure.png"></p>
<p>可以看到不止作者一个人想到了这种结构，作者讨论了 cascade 结构和另外 2 种结构的不同之处以及 cascade 结构为什么更优秀的原因。</p>
<p>为什么 Cascade R-CNN 要这样来设计呢？这和上面的出发点密切相关，从Figure1(c)中我们看出使用不同的 IoU 阈值训练得到的检测模型对有不同 IoU 阈值的输入候选框的结果差别较大，因此我们希望训练每个检测模型用的 IoU 阈值要尽可能和输入候选框的 IoU 接近。并且从 Figure1(c) 中可以看出三条彩色曲线都在灰度曲线上方，这说明对于这几个阈值来说，输出的 IoU 阈值都大于输入的 IoU 阈值。根据这一特点，我们就可以拿上一个阶段的输出作为下一个阶段的输入，这样就可以得到越来越高的 IoU。</p>
<p>总结就是我们很难让一个在指定 IoU 阈值界定的训练集上训练得到的检测模型对 IoU 跨度较大的输入候选框都达到最佳，因此采取级联的方式能够让每一个阶段的检测器都专注于检测 IoU 在某一范围内的候选框，因为输出 IoU 普遍大于输入 IoU，因此检测效果会越来越好。</p>
<h4 id="和-Iterative-BBox-比较"><a href="#和-Iterative-BBox-比较" class="headerlink" title="和 Iterative BBox 比较"></a>和 Iterative BBox 比较</h4><p>Iterative BBox 的 H 位置都是共享的，而且 3 个分支的 IoU 阈值都取 0.5。Iterative BBox 存在的问题：</p>
<ul>
<li>三个分支的 IoU 阈值都是 0.5，单一阈值无法对所有 proposals 取得良好效果</li>
<li>detector 会改变样本的分布，这时候每个分支使用一个共享的头 H(head) 对最终检测结果有影响，作者做了下面的实验证明样本分布在各个 stage 的变化。第一行是 x, y 的变化图，第二行是 w, h 的变化图。</li>
</ul>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/sample_distribution_in_diff_stage.jpg"></p>
<p>可以看到每经过一次回归，样本点都会更加靠近 GT 一点，质量也就更高一点，样本的分布也在逐渐发生变化，如果仍然使用 0.5 的阈值，在后面 2 个 stage 就会有较多的离群点，</p>
<h4 id="和-Integral-Loss-比较"><a href="#和-Integral-Loss-比较" class="headerlink" title="和 Integral Loss 比较"></a>和 Integral Loss 比较</h4><p>Integral Loss 共用 pooling，只有一个 stage，但有 3 个不共享的 H，每个 H 处都对应不同的 IoU 阈值。Integral Loss 存在的问题：</p>
<ul>
<li>第一个 stage 的输入 IoU 的分布很不均匀，高阈值 proposals 数量很少，导致负责高阈值的 detector 很容易过拟合。</li>
<li>在 inference 时，3 个 detector 的结果要进行 ensemble，但是它们的输入的 IoU 大部分都比较低，这时高阈值的 detector 也需要处理低 IoU 的 proposals，它就存在较严重的 mismatch 问题，它的 detector 效果就很差了。</li>
</ul>
<p><img src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/iou_histogram_of_training_sample.png"></p>
<p>由上图可以看到每经过一个 stage，即每经过一个检测器进行坐标回归，候选框就越准确，候选框和 GT 的 IoU 也就如图所示的越来越大。这就说明了 Cascade R-CNN 是可行的，因为它不会因为后面的检测器提高阈值导致正样本过少而过拟合。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>RPN 提出的 proposals 大部分质量不高，导致没办法直接使用高阈值的 detector，Cascade R-CNN 使用 cascade 回归作为一种重采样的机制，逐 stage 提高 proposal 和 GT 的 IoU 值，从而使得前一个 stage 重新采样过的 proposals 能够适应下一个有更高阈值的 stage。</p>
<ul>
<li>每一个 stage 的 detector 都不会过拟合，都有足够满足阈值条件的样本。</li>
<li>更深层的 detector 也就可以优化更大阈值的 proposals。</li>
<li>每个 stage 的 H 不相同，意味着可以适应多级的分布。</li>
<li>在 inference 时，虽然最开始 RPN 提出的 proposals 质量依然不高，但在每经过一个 stage 后质量都会提高，从而和有更高 IoU 阈值的 detector 之间不会有很严重的 mismatch。</li>
</ul>
<!-- ## TridentNet
TODO --></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ming Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://coderming.cn/2020/06/20/Two-Stage-Object-Detection/">https://coderming.cn/2020/06/20/Two-Stage-Object-Detection/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Two-Stage/">Two-Stage</a></div><div class="post_share"><div class="social-share" data-image="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/wechat.png" target="_blank"><img class="post-qr-code-img" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/wechat.png" alt="WeChat"/></a><div class="post-qr-code-desc">WeChat</div></li><li class="reward-item"><a href="https://qr.alipay.com/fkx12431y6k0soy7vokzi19" target="_blank"><img class="post-qr-code-img" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/reward/alipay.jpg" alt="AliPay"/></a><div class="post-qr-code-desc">AliPay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/06/20/Anchor-Free-Object-Detection/"><img class="prev-cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Anchor-Free-Object-Detection/cover.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Anchor Free Object Detection</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/20/One-Stage-Object-Detection/"><img class="next-cover" src="https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/One-Stage-Object-Detection/cover.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">One-Stage Object Detection</div></div></a></div></nav></article></main><footer id="footer" style="background-image: url(https://limingcv.coding.net/p/images/d/images/git/raw/master/posts/2020/06/Two-Stage-Object-Detection/faster_rcnn.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2020 By Ming Li</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div></div></body></html>